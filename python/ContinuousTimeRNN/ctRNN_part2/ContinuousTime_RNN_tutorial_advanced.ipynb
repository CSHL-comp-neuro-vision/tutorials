{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO_2jJncJEV9"
   },
   "source": [
    "# Continuous time recurrent neural network - more advanced topics\n",
    "\n",
    "john serences (jserences@ucsd.edu) 06.2024\n",
    "\n",
    "* In the `basics` tutorial we covered how to create and train a continuous time RNN. While some biological 'realism' was built in (e.g. activation function, pre/post activation noise), we did not place any constraints on the weight matrices, the degree to which units are connected, or the sign of the connections (i.e. excitatory vs inhibitory synapses). For example, in visual cortex we having excitatory neurons (pyramidal cells) and we have inhibitory interneurons. Generally speaking these neurons serve a fixed function (Dale's principle), they exist in more-or-less known ratios, and they have different temporal properties. Implementing some of these constraints can dramaticallly change how an RNN solves a given problem, and also allows us to ask more targeted questions about the role of different cell types in information processing.\n",
    "  \n",
    "* Dale's principle: unit j is excitatory if all of its projections on other units are zero or excitatory, i.e.,\n",
    "if $W^{hid}_{ij} > 0$ for all $i$ and inhibitory if $W^{hid}_{ij} < 0$ for all $i$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLyXB18kJEV_"
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wl4pad7bJEV_"
   },
   "outputs": [],
   "source": [
    "# minimal imports - can import sub modules to make code cleaner, but\n",
    "# keeping this simple for now\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# import class to make model inputs (defines different tasks, and can add other tasks easily using this framework)\n",
    "from tasks import ctTASKS\n",
    "\n",
    "# import class to make model, make hidden layer, and loss function\n",
    "from ct_rnn import *\n",
    "from h_layer import *\n",
    "\n",
    "# load the autoreload extension - don't need for tutorial but good if you ever modify the classes used in the tutorial \n",
    "# autoreload will update if you make changes to the ctTASKS class\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# check for available devices \n",
    "# (mps works for M1/M2 macs)\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = torch.device(\"cuda\")\n",
    "    # set the device to default \n",
    "    torch.set_default_device(cuda_device)\n",
    "    print(f'Using device: {torch.get_default_device()}')\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    # set the device to default \n",
    "    torch.set_default_device(mps_device)\n",
    "    print(f'Using device: {torch.get_default_device()}')\n",
    "\n",
    "else:\n",
    "   print (\"CUDA/MPS device not found. Using cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network params, task params, and build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# task params\n",
    "#--------------------------------\n",
    "task_type = 'go-nogo' # task type (go-nogo, mante, xor)\n",
    "T = 100          # timesteps in each trial\n",
    "stim_on = 50     # timestep of stimulus onset\n",
    "stim_dur = 25    # stim duration\n",
    "n_trials = 64    # number of trials in each training batch\n",
    "acc_amp_thresh =[0.8,0.2]  # to determine acc of model output: > acc_amp_thresh[0] is classified as a 'go' trial, < acc_amp_thresh[1] is 'nogo'\n",
    "\n",
    "# init dict of task related params \n",
    "settings = {'task' : task_type, 'T' : T, 'stim_on' : stim_on, 'stim_dur' : stim_dur, 'n_trials' : n_trials,\n",
    "           'acc_amp_thresh' : acc_amp_thresh}\n",
    "\n",
    "# create the task object\n",
    "task = ctTASKS(settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delayed match to sample..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# task params\n",
    "#--------------------------------\n",
    "task_type = 'dmts'    # task type (go-nogo, mante, dmts)\n",
    "T = 300          # timesteps in each trial\n",
    "stim_on = 50     # timestep of stimulus onset\n",
    "stim_dur = 50    # stim duration\n",
    "delay = 10       # delay between stim1 and stim2\n",
    "n_trials = 64    # number of trials in each training batch\n",
    "acc_amp_thresh = 0.8  # to determine acc of model output: > acc_amp_thresh during target window is correct\n",
    "\n",
    "# init dict of task related params \n",
    "settings = {'task' : task_type, 'T' : T, 'stim_on' : stim_on, 'stim_dur' : stim_dur, 'delay' : delay, 'n_trials' : n_trials,\n",
    "           'acc_amp_thresh' : acc_amp_thresh}\n",
    "\n",
    "# create the task object\n",
    "task = ctTASKS(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mante 2013 task (response to sensory input depends on context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# task params\n",
    "#--------------------------------\n",
    "task_type = 'mante'   # task type (go-nogo, mante, dmts)\n",
    "T = 500          # timesteps in each trial\n",
    "stim_on = 50     # timestep of stimulus onset\n",
    "stim_dur = 200   # stim duration\n",
    "n_trials = 64    # number of trials in each training batch\n",
    "acc_amp_thresh =[0.8,0.2]  # to determine acc of model output: > acc_amp_thresh[0] is classified as a 'go' trial, < acc_amp_thresh[1] is 'nogo'\n",
    "\n",
    "# init dict of task related params \n",
    "settings = {'task' : task_type, 'T' : T, 'stim_on' : stim_on, 'stim_dur' : stim_dur, 'n_trials' : n_trials,\n",
    "           'acc_amp_thresh' : acc_amp_thresh}\n",
    "\n",
    "# create the task object\n",
    "task = ctTASKS(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the network and train model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIPwV2x2JEWC",
    "outputId": "dca5a527-db37-484e-c373-411167c5f21b"
   },
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# for debugging\n",
    "# torch.manual_seed(0)\n",
    "#--------------------------------\n",
    "# RNN Params\n",
    "#--------------------------------\n",
    "h_size = 200          # number of units in hidden layer\n",
    "inp_size = 4          # number of input streams (i.e. '1' means one stimulus input over time on each trial)[1 for go-ng0, 2 for dmts, 4 for mante]\n",
    "out_size = 1          # number of output streams\n",
    "dt = 1                # timestep\n",
    "tau = 20              # dt/tau determines decay time (how long prior state of a unit impacts current state)\n",
    "act_func = 'sigmoid'  # activation function linking x and r for each unit (currently restricted to relu or sigmoid, but easy to add custom funcs)\n",
    "p_rec = 0.2           # probability of two hidden layer units forming a synapse\n",
    "p_inh = 0.2           # probability that a hidden layer connection, if formed, will be inhibitory\n",
    "apply_dale = True     # apply Dale's principle (i.e. exc and inh connections cannot change signs during training)\n",
    "w_dist = 'gaus'       # hidden layer weight distribution (gauss or gamma)\n",
    "w_gain = 1.5          # gain on weights in hidden layer if w_dist == gauss\n",
    "preact_n = 0.01       # noise applied before passing x through activation function to get r\n",
    "postact_n = 0.01      # noise applied after passing x through activation function to get r\n",
    "loss_crit = 0.01      # stop training if loss < loss_crit \n",
    "acc_crit = 0.98       # or stop training if prediction acc is > acc_crit\n",
    "\n",
    "# Each layer does y = xw + b so these \n",
    "# flags for determining which weights/biases\n",
    "# are trainable. \n",
    "W_in_trainable = False\n",
    "bias_in_trainable = False\n",
    "W_hid_trainable = True\n",
    "bias_hid_trainable = False\n",
    "W_out_trainable = True\n",
    "bias_out_trainable = True\n",
    "\n",
    "# Initial values of weights/biases for input and output layers and the bias \n",
    "# for the hidden layer...\n",
    "# We will setup the hidden unit weights in the RLayer class based on \n",
    "# the probability of excitatory and inhibitory connections and Dale's \n",
    "# principle (set above)\n",
    "W_in = torch.randn((h_size,inp_size),dtype=torch.float32)\n",
    "bias_in = torch.zeros(h_size,dtype=torch.float32)\n",
    "bias_hid = torch.zeros(h_size,dtype=torch.float32)\n",
    "W_out = torch.randn((out_size, h_size),dtype=torch.float32)/100\n",
    "bias_out = torch.zeros(out_size,dtype=torch.float32)\n",
    "\n",
    "# dict of params to init the network\n",
    "rnn_settings = {'h_size' : h_size, 'inp_size' : inp_size, 'out_size' : out_size,'dt' : dt, 'tau' : tau,  \n",
    "                'act_func' : act_func, 'p_rec' : p_rec, 'p_inh' : p_inh, \n",
    "                'apply_dale' : apply_dale, 'w_dist' : w_dist, 'w_gain' : w_gain, 'preact_n' : preact_n, 'postact_n' : postact_n, \n",
    "                'W_in_trainable' : W_in_trainable, 'bias_in_trainable' : bias_in_trainable, \n",
    "                'W_hid_trainable' : W_hid_trainable, 'bias_hid_trainable' : bias_hid_trainable,\n",
    "                'W_out_trainable' : W_out_trainable, 'bias_out_trainable' : bias_out_trainable, \n",
    "                'W_in' : W_in, 'bias_in' : bias_in, 'W_out' : W_out, 'bias_out' : bias_out}\n",
    "\n",
    "# Init the network object\n",
    "net = ctRNN(rnn_settings)\n",
    "\n",
    "#--------------------------------\n",
    "# Model training params\n",
    "#--------------------------------\n",
    "iters = 10000              # number of training iterations\n",
    "loss_update_step = 100  # output the current loss/acuracy every loss_update_step training iterations\n",
    "\n",
    "# learning rate of optimizer function - step size during gradient descent\n",
    "# if this is too large you might jump over minima and never converge, but \n",
    "# if this is too small then it can take a long time to find the minima\n",
    "learning_rate = 0.001   \n",
    "\n",
    "#--------------------------------\n",
    "# Train the model!\n",
    "#--------------------------------\n",
    "\n",
    "# Use Adam optimizer: Adaptive Moment Estimation (ADAM) combines\n",
    "# gradient descent with momentum (accelerates as approaching minima) \n",
    "# and RMSprop which uses a moving average of the squares of gradients\n",
    "# to adjust the learning rate for each weight in the model\n",
    "# to discourage exploration in directions with steep gradients\n",
    "# and to promote faster exploration of flatter regions. \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# for storing running average of loss and accuracy computed \n",
    "# over the last batch of trials every loss_update_step trials\n",
    "running_loss = 0\n",
    "running_acc = 0 \n",
    "\n",
    "# loop over number of model training iterations\n",
    "for i in range(iters):\n",
    "    \n",
    "    # get a batch of inputs and targets\n",
    "    if task_type == 'go-nogo':\n",
    "       inputs,tri_type = task.stim_go_nogo()  \n",
    "       targets = task.target_go_nogo(tri_type)\n",
    "    elif task_type == 'dmts':\n",
    "        inputs,tri_type = task.stim_dmts()  \n",
    "        targets = task.target_dmts(tri_type)   \n",
    "    elif task_type == 'mante':\n",
    "        inputs,tri_type = task.stim_mante()  \n",
    "        targets = task.target_mante(tri_type) \n",
    "    \n",
    "    # zero out the gradient buffers before updating model params (e.g. Weights/biases)\n",
    "    # because gradients accumulate so the new gradient will be \n",
    "    # combined with the old gradient which has already been used to update the model \n",
    "    # parameters and the combined gradient may point in the wrong direction (i.e. not towards the minima)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # pass inputs...get outputs\n",
    "    outputs, _ = net(inputs)\n",
    "\n",
    "    # compute loss given current output and target \n",
    "    # output on each trial in this batch\n",
    "    loss = net.mse_loss(outputs, targets)\n",
    "\n",
    "    # backprop the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # single optimization step to update parameters\n",
    "    optimizer.step()        \n",
    "\n",
    "    # update running loss (just to keep track and to primt out)\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # Compute prediction accuracy (defined by the thresholds specified in settings dict)\n",
    "    \n",
    "    if (task_type == 'go-nogo') | (task_type == 'dmts'):\n",
    "        running_acc += task.compute_acc(settings,outputs,targets)    \n",
    "    else:\n",
    "        running_acc = 0\n",
    "        \n",
    "    # update about current loss and acc rate of model \n",
    "    # every loss_update_step steps\n",
    "    if i % loss_update_step == loss_update_step-1:\n",
    "\n",
    "        # compute avg loss and avg acc over last loss_update_step iterations\n",
    "        running_loss /= loss_update_step\n",
    "        running_acc /= loss_update_step\n",
    "        \n",
    "        # print out to monitor training\n",
    "        print(f'Step {i+1}, Loss {running_loss:0.4f}, Acc {running_acc:0.4f}')\n",
    "\n",
    "        # see if we've reached criteria to stop training\n",
    "        if (running_loss < loss_crit) | (running_acc > acc_crit):\n",
    "            print('Training finished')\n",
    "            break\n",
    "\n",
    "        # reset to zero before evaluating the loss and acc\n",
    "        # of the next loss_update_step iterations...\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot output and targets of last batch to see how well the model is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple plot the output of last batch (last group of trials)\n",
    "# to visualize model performance\n",
    "plt.plot(np.squeeze(outputs.cpu().detach()))\n",
    "plt.plot(np.squeeze(targets.cpu().detach()),'.')\n",
    "plt.title('Model output (solid), Targets (dots)')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at named model parameters to see current values of weights and biases and to see if they are trainable (requires_grad = True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.named_parameters():\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
