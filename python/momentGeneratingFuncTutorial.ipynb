{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment generating functions tutorial\n",
    "\n",
    "- Written by Michael Shadlen, 2003 (Revised 2004)\n",
    "- Translated to Python by Michael Waskom, 2020\n",
    "\n",
    "This tutorial introduces the moment generating function. It complements (and repeats to some extent) the section in the Mathematica tutorial on Wald's identity (Wald_identity.nb).\n",
    "\n",
    "The goal is to develop a few basic intuitions that are essential for understanding Wald's identity and its connection to the psychometric function. Here are the topics you need to understand:\n",
    "\n",
    "- The basic definition of the MGF\n",
    "- The use of the MGF to calculate moments\n",
    "- The MFG for a random variable, $Z$, that is the sum of two independent random variables, $X + Y$: the product of the MGF's associated with these two random variables.\n",
    "\n",
    "One of the more important goals is to gain an intuition for how changes in the properties of a random variable, its mean and variance, affect a special point on the moment generating function where it reaches 1: the special root, $\\theta_1$. The reason we care about this is spelled out in the mathematica tutorial. It is the connection between Wald's Identity and the psychometric function.\n",
    "\n",
    "This short tutorial accomplishes two things.\n",
    "- In part 1, we define the moment generating function (mgf) and use it to compute moments (hence the name). \n",
    "- In the second part, we look for the existence of a special root to the mgf. This is the nonzero solution of the equation $mgf(Z) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. The Moment Generating Function: Definition and properties\n",
    "\n",
    "The MGF is a transform of a probability function or probability density function. The PDF, $f(x)$, is the likelihood (or probability) of observing a random value, $x$. The MFG, $M(\\theta)$ is a function of a new variable. Think of it the same way you would think of a fourier or laplace transform.\n",
    "\n",
    "Let's begin with an example of something familiar, the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = .01\n",
    "x = np.arange(-10, 10 + dx, dx)\n",
    "\n",
    "# Mean and standard deviation\n",
    "u, s = 1, 1.2\n",
    "pdf = stats.norm(u, s).pdf(x)\n",
    "\n",
    "# Plot the probability density function (PDF)\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(x, pdf)\n",
    "ax.set(\n",
    "    xlabel=\"$x$\",\n",
    "    ylabel=\"$f(x)$\",\n",
    "    ylim=(0, None),\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moment generating function is a function of a new variable, theta. It is the expected value of $e^{\\theta x}$, where $x$ is our random variable. Notice that for any value of $\\theta$, the expectation of $e^{\\theta x}$ is a number.\n",
    "\n",
    "Discretely, the mgf at each value of theta is the expectation of `exp(theta[i] * x)`\n",
    "\n",
    "The expectation is just the sum of `exp(theta[i] * x)` with each term weighted by the probability of of observing x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mgf(x, theta, pdf):\n",
    "    H = np.exp(np.outer(theta, x))\n",
    "    return integrate.trapz(H * pdf, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtheta = .01\n",
    "theta = np.arange(-2.5, 1 + dtheta, dtheta)\n",
    "y = mgf(x, theta, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(theta, y)\n",
    "ax.axhline(1, color=\".6\", ls=\"--\")\n",
    "ax.set(\n",
    "    xlim=(theta.min(), theta.max()),\n",
    "    xlabel=r\"$\\theta$\",\n",
    "    ylabel=r\"$M(\\theta)$\",\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the next section is to get a feel for why the MGF looks the way it does. Let's just notice afew things about this function.\n",
    "\n",
    "First, $M(0) = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgf(x, 0, pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be obvious that all values must be greater than or equal to 0. Remember, it's just a weighted average of exp(...). Notice that on one side of 0, the function keeps getting bigger. On the other side of 0, it dips below 1 and then rises again, crossing 1 at another point. That crossing point turns out to be very important for psychophysics. We'll spend time on it later. For the moment (no pun intended) let's develop an intuition for these properties.\n",
    "\n",
    "The moment generating function has its name because if you take its derivative at theta=0, you get the 1st moment of x. In other words the mean. If you take the second derivative, again evaluated at theta=0, you obtain the 2nd moment: the average of the square of x. And so forth for higher derivatives and moments. \n",
    "\n",
    "This is easy to see in the math. Look at section 1.2 of Wald_identity.nb.  Here, let's convince ourselves of this using numerical approximations.\n",
    "\n",
    "The first moment is the expectation of x (i.e., the mean). We know what to expect, because we set the mean `u`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get it from the random variable object\n",
    "assert np.isclose(u, stats.norm(u, s).moment(1))\n",
    "assert np.isclose(u, stats.norm(u, s).stats(\"m\"))\n",
    "\n",
    "# Calculate the expectation\n",
    "assert np.isclose(u, integrate.trapz(x * pdf, x))\n",
    "\n",
    "# Differentiate the moment generating function\n",
    "d = np.diff(y) / dtheta\n",
    "\n",
    "# The numerical differntiation is right-sided; interpolate to find the derivative\n",
    "idx = np.argmin(np.abs(theta))\n",
    "d_0 = d[idx - [0, 1]].mean()\n",
    "assert np.isclose(u, d_0, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also provides some intution for why the MGF's appearance. At `theta = 0`, we know that the tangent to the curve should have `slope = u`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.abs(theta) < .35\n",
    "tangent = d_0 * theta + 1\n",
    "ax.plot(theta[L], tangent[L], color=\".15\")\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second moment is the expectation of $x^2$. We also know what this should be, because we set `u` and `s`, above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = s ** 2 + u ** 2\n",
    "\n",
    "# Get from the scipy object\n",
    "assert np.isclose(m2, stats.norm(u, s).moment(2))\n",
    "\n",
    "# Calculate it directly\n",
    "assert np.isclose(m2, integrate.trapz(x ** 2 * pdf, x))\n",
    "\n",
    "# Or use the moments\n",
    "d2 = np.diff(y, 2) / (dtheta ** 2)\n",
    "d2_0 = d2[idx - 1]\n",
    "assert np.isclose(d2_0, m2, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the curve, we're not surprised that it is convex up at `theta = 0`. The second moment has to be positive. If the variance were larger, then the degree of convexity would increase, making for a tighter U-shaped curve.\n",
    "\n",
    "Let's pay attention to the point on the left portion of the curve where it crosses the dashed line. The \n",
    "value of theta where this occurs is $\\theta_1$. This value is going to turn out to be very important to us.\n",
    "\n",
    "Let's define a function that plots the PDF and MGF for arbitrary distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf_mgf(x, theta, *distributions):\n",
    "\n",
    "    f, (ax_pdf, ax_mgf) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "\n",
    "    for d in distributions:\n",
    "        ax_pdf.plot(x, d.pdf(x))\n",
    "        ax_mgf.plot(theta, mgf(x, theta, d.pdf(x)))\n",
    "    ax_mgf.axhline(1, color=\".6\", dashes=(3, 1.5), lw=1)\n",
    "    \n",
    "    ax_pdf.set(\n",
    "        xlabel=\"$x$\",\n",
    "        ylabel=\"$f(x)$\",\n",
    "        xlim=(x.min(), x.max()),\n",
    "        ylim=(0, None),\n",
    "    )\n",
    "\n",
    "    ax_mgf.set(\n",
    "        xlabel=r\"$\\theta$\",\n",
    "        ylabel=r\"$M(\\theta)$\",\n",
    "        xlim=(theta.min(), theta.max()),\n",
    "        ylim=(0, 5),\n",
    "    )\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's change the standard deviation and hold the mean constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s = 1, 1.2\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(u, 1.5 * s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important intuition to hang on to. If the convexity were greater, the second point where the curve crosses the horizontal line would move closer to $\\theta=0$. In other words $\\theta_1$ would be smaller in absolute magnitude.\n",
    "\n",
    "If the variance were smaller, the convexity would be lower and $\\theta_1$ would move off to the left, further from $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s = 1, 1.2\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(u, .8 * s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mean were larger, but the convexity did not change, then the slope at 0 would increase, and that would push $\\theta_1$ away from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s = 1, 1.2\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(1.5 * u, s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mean were smaller, but the convexity did not change, then the slope at 0 would decrease, and that would pull $\\theta_1$ closer to the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s = 1, 1.2\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(.8 * u, s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check this out. We can pit these two tendencies against each other. It turns out that their effects cancel if we change the mean and variance by the same amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s = 1, 1.2\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(1.5 * u, np.sqrt(1.5) * s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an important observation. For the gaussian distribution this special crossing point, $\\theta_1$, is a function of the ratio of variance and mean. If they are kept the same, $\\theta_1$ does not change. That's not to say that the MGF doesn't change. You can see from Figure 10 that it does. But that value, $\\theta_1$, is going to turn out to be pivotal to the performance.\n",
    "\n",
    "Already, you should be scratching your head skeptically. Shouldn't performance have something to do with signal to noise ratio, hence mean and standard deviation? Yet the $\\theta_1$ depends on mean and *variance*.  Store the thought. We have yet to connect $\\theta_1$ to the psychometric function.\n",
    "\n",
    "Let's expand our intuitions by looking at MGFs associated with mean = 0 or mean < 0.  Let's start with the former.\n",
    "\n",
    "When the mean is zero, we know that the slope of the MGF at theta=0 is flat. Moreover, the function is convex up. So there's no 2nd crossing of the horizontal line. $\\theta_1$ is effectively 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s = 0, 1\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(u, 1.5 * s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the mean is less than 0, everything simply flips around the y-axis. I think this only seems obvious, but if you think about it (or do the math) you'll see that if pdf2(x) = pdf1(-x), then the weighted averages of $e^{\\theta x}$ with weights given by pdf2 have to correspond to the opposite signed theta. In any case we can see this graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.arange(-2.5, 2.5 + dtheta, dtheta)\n",
    "u, s = 1, 1.2\n",
    "d1 = stats.norm(u, s)\n",
    "d2 = stats.norm(-u, s)\n",
    "plot_pdf_mgf(x, theta, d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to think about the negative version of a random variable will be important in a moment when we consider sums of RVs.\n",
    "\n",
    "## The MGF of a sum of RVs\n",
    "\n",
    "I may fill this in with more intuition later. But here's the bottom line. Suppose $x_1$ has distribution $f_1(x)$ and moment generating function $M_1(\\theta)$, and suppose $x_2$ has distribution $f_2(x)$ and moment generating function $M_2(\\theta)$.  Consider the random variable, $Z$. It has a pdf that is the convolution of $f_1(x)$ and $f_2(x)$. It has a moment generating function that is the *product* $M1(\\theta)$ and $M2(\\theta)$.\n",
    "\n",
    "It may seem surprising that the pdf $f(z)$ is a convolution, but think about it. Suppose we know the value, $x_1$. Then the conditional probability of observing any $Z$ as the sum $x_1 + x_2$  is the probability of choosing $x2 = z - x_1$. That's $f_2(z-x_1)$. To get the probability of getting $Z$ from the any sum, it's just a matter of integrating this conditional probability across all possible values of $x_1$:\n",
    "\n",
    "$$f(z) = \\int f_1(x_1)f_2(z - x_1)\\, dx_1$$\n",
    "   \n",
    "\n",
    "In other words, `f_z = conv(f_1, f_2)`. (I'm not writing real code here. We would have to be careful about axes.)\n",
    "\n",
    "I'm not going to go through the math, but instead I'm appealing to an intution that I hope we share. If we were dealing with functions of time or space, we would know that the fourier transform of `f_z` would be the product of the fourier transforms of `f_2` and `f_2`. The moment generating function is a lot like a fourier transform.\n",
    "\n",
    "I may flesh this out one day.\n",
    "\n",
    "There are two points relevant to our topic. First, if we were to make a new RV from the difference of two RVs, we need simply multiply the $M_1(\\theta)$ by $M_2(-\\theta)$. That's because we're adding $x_1$ and $-x_2$. Second, when we look at Wald's identity, we should not be surprised the the moment generating function for a sum of RVs is equal to the product of MGFs for the increments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MGF of differences of random variables\n",
    "\n",
    "The MGF of a variety of RVs formed by taking the difference of two RVs has a nonzero $\\theta_1$. Recall that if $Z$ is the sum of two RVs, $X + Y$, each with MGF $M_X(\\theta)$ and $M_Y(\\theta)$, the MGF associated with $z$ is $M_Z = M_X(\\theta)M_Y(\\theta)$. Subtraction is like adding one of the RVs with its sign switched. This is $M_X(\\theta)M_Y(-\\theta)$.\n",
    "\n",
    " et's look at the dfference between two RVs described by two Weibull distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 20 + dx, dx)\n",
    "theta = np.arange(-1, 1 + dtheta, dtheta)\n",
    "d1 = stats.weibull_min(1.2, 0, 3)\n",
    "d2 = stats.weibull_min(2.4, 0, 3)\n",
    "plot_pdf_mgf(x, theta, d1, d2)\n",
    "f = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add in the MGF for the difference of the second Weibull minus the first. Notice the  $-\\theta$ on the first MGF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mgf(x, -theta, d1.pdf(x)) * mgf(x, theta, d2.pdf(x))\n",
    "f.axes[1].plot(theta, y)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the MGF to get the mean difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(y) / np.diff(theta)\n",
    "idx = np.argmin(np.abs(theta))\n",
    "d_0 = d[idx - [0, 1]].mean()\n",
    "\n",
    "L = np.abs(theta) < .25\n",
    "tangent = d_0 * theta + 1\n",
    "f.axes[1].plot(theta[L], tangent[L], color=\".2\")\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the second moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = np.diff(y, 2) / dtheta ** 2\n",
    "dd_0 = dd[idx - 1]\n",
    "var_est = dd_0 - d_0 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare our estimations of the mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(d_0, d2.mean() - d1.mean(), rtol=1e-2)\n",
    "assert np.isclose(var_est, d2.var() + d1.var(), rtol=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cshl-tutorials (py38)",
   "language": "python",
   "name": "cshl-tutorials"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
