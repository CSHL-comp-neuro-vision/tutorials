{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization Attention Tutorial\n",
    "\n",
    "This tutorial works through the 'normalization model of attention' based\n",
    "on the 2009 paper published in Neuron by John Reynolds and David Heeger.\n",
    "The model is worth going through because it hits on a range of topics\n",
    "associated with this course - linear filtering, normalization, population\n",
    "responses, spatial attention and feature-based attention.  It's powerful\n",
    "because it can predict a range of physiological, behavioral and fMRI\n",
    "results from attention studies.\n",
    "\n",
    "Written by G.M. Boynton in June 2010 based heavily on code provided by the authors.\n",
    "\n",
    "Translated to Python by Michael Waskom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import pi, sqrt\n",
    "from numpy.fft import fft, ifft, fftshift\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import convolve2d\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 'neural image'\n",
    "\n",
    "The backbone of the model is the way the population of neuronal responses\n",
    "are represented.  The 'neural image' is a 2-D matrix (or image) in which\n",
    "the two matrix dimensions represent two physical dimensions - space and\n",
    "orientation in this model - with the intensity of each pixel representing\n",
    "the size of the response to a neuron with the corresponding space and\n",
    "feature selectivity.   \n",
    "\n",
    "Showing this matrix as a gray-scale image is a convenient way of\n",
    "representing the population response for neurons tuned to specific\n",
    "locations (receptive fields) and orientations.\n",
    "\n",
    "## Model/neural parameters\n",
    "\n",
    "First we'll define our 2-d stimulus space for the neural images.  Space\n",
    "(matrix rows) occupies a single dimension which may seem weird, but\n",
    "without loss of any generality we can think of the space dimesion as 2-d\n",
    "space unwrapped into a single vector.  Or, we can just think of it as 1-d\n",
    "space, since all computations will generalize to 2-d space.  If you\n",
    "really wanted, you could define a cube, or a 'neural volume' instead of a\n",
    "neural image with 2 dimension for space and the third dimension as the\n",
    "feature. (and so on).\n",
    "\n",
    "The dictionary `p` will eventually contain all parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict(\n",
    "    x=np.arange(-200, 204, 4),\n",
    "    theta=np.arange(-180, 181),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimulus parameters\n",
    "\n",
    "Stimuli will be defined as having centers and Gassian widths in both\n",
    "space and orientation.  These are basically like Gabors if the\n",
    "orientation width is narrow and if we assume that the stimulus is\n",
    "narrowly defined in spatial frequency.\n",
    "\n",
    "We'll define the stimulus parameters inside the dictionary 'stim'.  Each\n",
    "of the fields contain vectors of equal length, with each component\n",
    "corresponding to a different stimulus.  We'll start with a single\n",
    "stimulus - a high-contrast Gabor at position `x = -100` with an orientation\n",
    "of `theta = 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = dict(\n",
    "    x_center=[-100],  # stimulus center_positions\n",
    "    x_width = [3],  # stimulus widths (deg)\n",
    "    th_center=[0],  # stimulus orientation centers\n",
    "    th_width=[1],  # stimulus orientation widths\n",
    "    contrast=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stimulus can be represented as a 'stimulus image' in the same\n",
    "coordinates as the neural image.  Each pixel in the stimulus image\n",
    "represents the amount of contrast at that pixel's corresponding position\n",
    "and orientation.\n",
    "\n",
    "We can translate these stimulus parameters into a stimulus image with the\n",
    "following function. It works by generating Gaussians in space and orientation\n",
    "for each stimulus component and computing the outer product of the two to generate\n",
    "the stimulus image matrix.  So the contrast energy of the stimulus is defined to\n",
    "be a separable matrix with Gaussian orientation and spatial tuning on the marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neural_image(p, stim):\n",
    "    \"\"\"Make a 2D array representing the population response.\"\"\"\n",
    "    stim = stim.copy()\n",
    "    stim.setdefault(\"contrast\", [1] * len(stim[\"x_center\"]))\n",
    "    stim.setdefault(\"rng\", (0, 1))\n",
    "    stim.setdefault(\"method\", \"multiply\")\n",
    "    \n",
    "    img = np.zeros((p[\"theta\"].size, p[\"x\"].size))\n",
    "    for i, _ in enumerate(stim[\"x_center\"]):\n",
    "\n",
    "        x_w_i, x_c_i = stim[\"x_width\"][i], stim[\"x_center\"][i]\n",
    "        th_w_i, th_c_i = stim[\"th_width\"][i], stim[\"th_center\"][i]\n",
    "        \n",
    "        x_g = x_w_i * sqrt(2 * pi) * norm.pdf(p[\"x\"], x_c_i, x_w_i)\n",
    "        th_g = th_w_i * sqrt(2 * pi) * norm.pdf(p[\"theta\"], th_c_i, th_w_i)\n",
    "\n",
    "        if stim[\"method\"] == \"multiply\":\n",
    "            sub_img = np.outer(th_g, x_g)\n",
    "        elif stim[\"method\"] == \"add\":\n",
    "            sub_img = np.average(np.meshgrid(x_g, th_g), axis=0)\n",
    "\n",
    "        img += stim[\"contrast\"][i] * sub_img\n",
    "\n",
    "    rng = stim[\"rng\"]\n",
    "    img = img * (rng[1] - rng[0]) + rng[0]\n",
    "    return img\n",
    "\n",
    "def show_neural_image(p, img, **kws):\n",
    "    \"\"\"Plot a 2D population response image as a heatmap.\"\"\"\n",
    "    f, ax = plt.subplots(figsize=(7, 6))\n",
    "    ax.pcolormesh(p[\"x\"], p[\"theta\"], img, cmap=\"gray\", **kws)\n",
    "    ax.set(xlabel=\"Position\", ylabel=\"Orientation\",\n",
    "           yticks=[-180, -90, 0, 90, 180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_neural_image(p, make_neural_image(p, stim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some time to think about what different stimuli look like in this\n",
    "stimulus space. What does a full-field grating look like?  It's a\n",
    "horizontal stripe.  A spatially localized patch of noise?  A vertical\n",
    "stripe.  (Note that a 'stimulus image' here is different from an actual\n",
    "image of the stimulus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excitatory response parameters (neuronal tuning widths)\n",
    "\n",
    "The first stage of the model is the linear excitatory response to the\n",
    "stimulus.  This is simply the response of a linear receptive field tuned\n",
    "to space and feature.  The center of the tuning for each neuron is\n",
    "determined by where it lives in the neural image.  The width of tuning\n",
    "for space (receptive field size) and feature (orientation tuning) are\n",
    "defined here.  These are in terms of standard deviations of Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[\"e\"] = dict(x_width=5, th_width=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Excitatory response\n",
    "\n",
    "The neural response for the excitatory component is a linear filtering of\n",
    "the stimulus with the excitatory receptive fields. For any given pixel in\n",
    "the neural image, the response is a Gaussian centered at that position\n",
    "with widths determined by p.e above, multiplied by the stimulus image,\n",
    "and added up.  The entire neural image is therefore calculated with a\n",
    "convolution of the stimulus image by the Gaussians determined by the\n",
    "excitatory parameters.  \n",
    "\n",
    "We'll define a function `convolve_image` that produces this neural image\n",
    "through convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_image(p, img, x_width, theta_width):\n",
    "\n",
    "    dx = p[\"x\"][1] - p[\"x\"][0]\n",
    "    dtheta = p[\"theta\"][1] - p[\"theta\"][0]\n",
    "\n",
    "    # Standard convolution for space (x) dimension\n",
    "    x_c = p[\"x\"][p[\"x\"].size // 2]\n",
    "    x_filt = norm.pdf(p[\"x\"], x_c, x_width)\n",
    "    out = dx * convolve2d(img, x_filt[np.newaxis, :], \"same\")\n",
    "\n",
    "    # Circular convolution for orientation (theta) dimension\n",
    "    theta_c = p[\"theta\"][p[\"theta\"].size // 2]\n",
    "    theta_filt = norm.pdf(p[\"theta\"], theta_c, theta_width)\n",
    "    theta_filt = np.tile(theta_filt[:, np.newaxis], len(p[\"x\"]))\n",
    "\n",
    "    conv_fft = fft(out, axis=0) * fft(theta_filt, axis=0)\n",
    "    out = dtheta * fftshift(ifft(conv_fft, axis=0).real, axes=0)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the excitatory neural image looks like (the authors call it 'excitatory drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = make_neural_image(p, stim)\n",
    "E = convolve_image(p, img, p[\"e\"][\"x_width\"], p[\"e\"][\"th_width\"])\n",
    "show_neural_image(p, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make sense?  Remember, the intensity of the image at each point\n",
    "is the response of a neuron with tuning centered at the corresponding\n",
    "location and orientation.  The orientation tuning with is pretty broad\n",
    "(60 deg), so neurons tuned fairly far away from the stimulus still\n",
    "respond somewhat to the stimulus.  But the spatial receptive field is\n",
    "narrow (5 deg), so the neural image drops off rapidly in the spatial (x)\n",
    "dimension.\n",
    "\n",
    "## Inhibitory response parameters\n",
    "\n",
    "In the standard normalization model, the excitatory signal is divided by\n",
    "the pooled response across a range of neurons tuned across space and\n",
    "features.  We can define the range of spatial pooling the same way we\n",
    "defined the stimulus image and the excitatory response parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[\"i\"] = dict(x_width=20, th_width=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a constant in the denominator to keep the ratio from blowing up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[\"sigma\"] = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These specific parameters mean that each neuron is suppressed by neurons\n",
    "tuned away across a fairly narrow range in space, but across neurons\n",
    "tuned across all orientations.  \n",
    "\n",
    "The neural image describing the summed response in the pool of neurons is\n",
    "therefore simply the convolution of the excitatory neural image (E) with\n",
    "Gaussians determined by the inhibitory pooling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = convolve_image(p, E, p[\"i\"][\"x_width\"], p[\"i\"][\"th_width\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a picture of the inhibitory pooling neural image which the authors call 'inhibitory drive':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_neural_image(p, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brightness of each pixel in this image represents the strength of\n",
    "divisive inhibition for the corresponding neuron.\n",
    "\n",
    "Although these inhibitory parameters are defined the same way as the\n",
    "excitatory parameters, they have a very different meaning.  The\n",
    "excitatory parameters determine the receptive field properties of the\n",
    "neurons.  These inhibitory parameters determine the range of neurons that\n",
    "we're polling across for normalization. It's similiar in that the\n",
    "excitatory parameters determine the range of stimuli excite a given\n",
    "neuron, the inhibitory parameters determine the range of neurons that\n",
    "inhibit a given neuron.\n",
    "\n",
    "The neural image for the normalized response is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = E / (I + p[\"sigma\"])\n",
    "show_neural_image(p, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the population response for the normalization model.  As\n",
    "expected, there are the largest responses for neurons tuned for the\n",
    "stimulus properties, and the neural responses fall of for neurons tuned\n",
    "away for both space and orientation.  \n",
    "\n",
    "You should play with the model parameters to see how they affect is\n",
    "neural image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def response_tutorial(e_x_width=(0, 10), i_x_width=(10, 30),\n",
    "                      e_theta_width=(30, 90), i_theta_width=(90, 360)):\n",
    "\n",
    "    S = make_neural_image(p, stim)\n",
    "    E = convolve_image(p, img, e_x_width, e_theta_width)\n",
    "    I = convolve_image(p, E, i_x_width, i_theta_width)\n",
    "    R = E / (I + p[\"sigma\"])\n",
    "    show_neural_image(p, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get in to attention, we can play around a bit with this basic\n",
    "normalization model to see how it predicts some basic response properties\n",
    "of V1 neurons.\n",
    "\n",
    "\n",
    "## Contrast response\n",
    "\n",
    "Let's measure the response of the normalization model to a range of\n",
    "contrasts.  This means generating a neural image for each contrast. We'll\n",
    "define a function `normalization_model` that calculates the neural image, `R`,\n",
    "shown above (and returns the other images for free)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_model(p, stim, attend=None, return_all=False):\n",
    "\n",
    "    S = make_neural_image(p, stim)\n",
    "    E = convolve_image(p, S, p[\"e\"][\"x_width\"], p[\"e\"][\"th_width\"])\n",
    "\n",
    "    if attend is not None:\n",
    "        A = make_neural_image(p, attend)\n",
    "    else:\n",
    "        A = np.ones_like(E)\n",
    "\n",
    "    G = E * A\n",
    "    I = convolve_image(p, G, p[\"i\"][\"x_width\"], p[\"i\"][\"th_width\"])\n",
    "    R = G / (I + p[\"sigma\"])\n",
    "\n",
    "    if return_all:\n",
    "        return R, S, E, A, G, I\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = np.logspace(-12, 0, 21, base=np.e)\n",
    "R = np.zeros((contrasts.size, p[\"theta\"].size, p[\"x\"].size))\n",
    "for i, c in enumerate(contrasts):\n",
    "    stim[\"contrast\"] = [c]\n",
    "    R[i] = normalization_model(p, stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the neurons in these neural images are not responding because\n",
    "they are tuned away from the stimulus in space or orientation. Let's find\n",
    "the neuron that is most closely tuned to the center of the stimulus and\n",
    "plot its contrast response function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idx = np.argmin((p[\"x\"] - stim[\"x_center\"]) ** 2)\n",
    "th_idx = np.argmin((p[\"theta\"] - stim[\"th_center\"]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.log(contrasts), R[:, th_idx, x_idx], label=\"Grating\")\n",
    "ax.set(xlabel=\"log(contrast)\", ylabel=\"Response\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of sigma in the denominator of the normalization equation controls the contrast gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def contrast_gain(sigma_exp=(-8, -4, .5)):\n",
    "\n",
    "    contrasts = np.logspace(-12, 0, 21, base=np.e)\n",
    "    p[\"sigma\"] = 10 ** sigma_exp\n",
    "\n",
    "    R = np.zeros((contrasts.size, p[\"theta\"].size, p[\"x\"].size))\n",
    "    for i, c in enumerate(contrasts):\n",
    "        stim[\"contrast\"] = [c]\n",
    "        R[i] = normalization_model(p, stim)\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.plot(np.log(contrasts), R[:, th_idx, x_idx], label=\"Grating\")\n",
    "    ax.set(xlabel=\"log(contrast)\", ylabel=\"Response\", ylim=(-1, 23))\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-orientation inhibition\n",
    "\n",
    "Consider the contrast response for the same neuron, but in the presence\n",
    "of a high contrast grating tuned to the orthognal dimension.  This is a\n",
    "sequence of plaids where the off-orientation is high contrast and the\n",
    "preferred orientation varies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim.update(\n",
    "    x_center=[-100, -100],\n",
    "    x_width=[3, 3],\n",
    "    th_center=[0, 90],\n",
    "    th_width=[1, 1],\n",
    ")\n",
    "\n",
    "p[\"sigma\"] = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = np.logspace(-12, 0, 21, base=np.e)\n",
    "R = np.zeros((contrasts.size, p[\"theta\"].size, p[\"x\"].size))\n",
    "for i, c in enumerate(contrasts):\n",
    "    stim[\"contrast\"] = [c, 1]\n",
    "    R[i] = normalization_model(p, stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(np.log(contrasts), R[:, th_idx, x_idx], label=\"Plaid\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the presence of a high contrast orthogonal stimulus (orange)\n",
    "suppresses the responses to the stimulus compared to the grating alone\n",
    "(blue).  This is exactly what's found in the physiology literature and\n",
    "has been modeled by normalization models such as Heeger's work in the\n",
    "90's. Technically, Heeger's original model squares the excitatory input\n",
    "before feeding into the normalization process, but the general idea is\n",
    "the same.\n",
    "\n",
    "The normalization model predicts these curves because the normalization\n",
    "pool is orientation-independent, so the orthogonal grating adds a strong\n",
    "divisive signal.\n",
    "\n",
    "You probably noticed that the response to the plaid is greater than the\n",
    "response to the grating at low grating contrasts.  This is because the\n",
    "excitatory orientation tuning is broad (60 deg), so the orthogonal\n",
    "stimulus feeds some signal into the excitatory input.\n",
    "\n",
    "## Stimulus parameters for an attention experiment\n",
    "\n",
    "Finally we're ready to talk about attention.  First we'll set up a new\n",
    "stimulus condition.  This will be a classic spatial attention condition\n",
    "where two intermediate contrast Gabors are presented, one on the left and\n",
    "one on the right side of the visual field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = dict(\n",
    "    x_center=[-100, 100],\n",
    "    x_width=[3, 3],\n",
    "    th_center=[0, 0],\n",
    "    th_width=[1, 1],\n",
    "    contrast=[.25, .25],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the stimulus image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = make_neural_image(p, stim)\n",
    "show_neural_image(p, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention parameters\n",
    "\n",
    "The normalization model of attention defines the spatial and featural\n",
    "spread of attention using the same sort of structure as the stimulus,\n",
    "excitatory and inhibitory parameters.  \n",
    "\n",
    "Spatial attention is a Gaussian 'spotlight' centered at some location\n",
    "with some standard deviation that may vary with the task. We'll have \n",
    "attention directed to the left stimulus (`x = 100`) and focused down to a\n",
    "size that matches the size of the stimulus (`width = 3`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend = dict(\n",
    "    x_center=[-100],\n",
    "    x_width=[3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature-based attention is defined similarly with a center and a width.\n",
    "\n",
    "Here's an example of feature-based attention being spread across all\n",
    "orientations using a very large `th_width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend.update(\n",
    "    th_center=[0],\n",
    "    th_width=[1e3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final attention parameters determine the minimum and maximum gain changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend.update(rng=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'spotlight' of attention can be represented as an 'Attention Field' in the neural image space and can be generated with 'makeNeuralImage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = make_neural_image(p, attend)\n",
    "show_neural_image(p, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling attention by modulating the excitatory input\n",
    "\n",
    "The effects of attention are implemented by simply multiplying the\n",
    "excitatory drive by the attentional spotlight. \n",
    "\n",
    "Here's the excitatory input, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = convolve_image(p, S, p[\"e\"][\"x_width\"], p[\"e\"][\"th_width\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the gain change due to attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = A * E\n",
    "show_neural_image(p, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the excitatory response to the left stimulus is greater (brighter)\n",
    "than the right, because attention was directed there.\n",
    "\n",
    "## Inhibitory image with attention\n",
    "\n",
    "Remakably, the rest of the model is exactly as before.  We calculate the\n",
    "inhibitory neural image by convolving the (now modulated) excitatory\n",
    "input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = convolve_image(p, G, p[\"i\"][\"x_width\"], p[\"i\"][\"th_width\"])\n",
    "show_neural_image(p, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for this example, the divisive inhibitory input is also\n",
    "greater for the attended stimulus. We'll see soon how the relative\n",
    "contributions of attention to the excitatory and inhibitory inputs allows\n",
    "for a range of ways that attention can influence the neuronal response.\n",
    "\n",
    "## Normalization model with attention\n",
    "\n",
    "The last step is also like before, we divide the response by the\n",
    "inhibitory input (plus a small constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = G / (I + p[\"sigma\"])\n",
    "show_neural_image(p, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast response for attended and unattended stimuli\n",
    "\n",
    "The way attention influences neuronal responses as a function of stimulus\n",
    "contrast is a useful way to characterize the overall effects of\n",
    "attention.\n",
    "\n",
    "Next we'll plot contrast response functions for neurons that were\n",
    "presented identical physical stimuli, but with attention directed within\n",
    "only one of the two receptive fields.  The responses of these two neurons\n",
    "is equivalent to the response of a single neuron with attention shifted\n",
    "within and away from its receptive field.  \n",
    "\n",
    "To plot the contrast response functions, we need to find the indices for\n",
    "the two neurons that are most closely tuned to the two stimuli:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idxs = np.argmin((p[\"x\"][:, np.newaxis] - stim[\"x_center\"]) ** 2, axis=0)\n",
    "th_idxs = np.argmin((p[\"theta\"][:, np.newaxis] - stim[\"th_center\"]) ** 2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the neural images and plot the two responses just\n",
    "like before.  The function 'normalizationModel' can take in a third\n",
    "argument 'attend' that contains the attention parameters.  The\n",
    "calculations within 'normalizationModel' are identical to those in this\n",
    "script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((contrasts.size, p[\"theta\"].size, p[\"x\"].size))\n",
    "for i, c in enumerate(contrasts):\n",
    "    stim[\"contrast\"] = [c, c]\n",
    "    R[i] = normalization_model(p, stim, attend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out the two contrast response functions to each stimulus from the neural images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = R[:, th_idxs, x_idxs]\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(np.log(contrasts), y[:, 0], label=\"Attend in\")\n",
    "ax.plot(np.log(contrasts), y[:, 1], label=\"Attend out\")\n",
    "ax.set(xlabel=\"log(contrast)\", ylabel=\"Response\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is, a higher response to an attended stimulus than an unattended\n",
    "stimulus.  Note that for these parameters, it looks like attention is\n",
    "acting as a 'response gain', which is a vertical scaling between the\n",
    "attended and unattended responses across contrast.  \n",
    "\n",
    "## Response gain vs. Contrast gain.\n",
    "\n",
    "Why does a narrow focus of attention lead to response gain?  It helps to\n",
    "look at the cross-section of the neural images to get some insight.\n",
    "Consider a high-contrast stimulus with a narrow focus of attention. We'll\n",
    "make the conditions more extreme (larger stimulus, smaller focus of\n",
    "attention) for better illustration.  Here is a complete set of stimulus\n",
    "and attentional parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = dict(\n",
    "    x_center=[-100, 100],\n",
    "    x_width=[20, 20],\n",
    "    th_center=[0, 0],\n",
    "    th_width=[1, 1],\n",
    "    contrast=[1, 1],\n",
    ")\n",
    "\n",
    "attend = dict(\n",
    "    x_center=[-100],\n",
    "    x_width=[1],\n",
    "    th_center=[0],\n",
    "    th_width=[1e3],\n",
    "    rng=(1, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, S, E, A, G, I = normalization_model(p, stim, attend, return_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of the spatial profile of the excitatory drive and inhibitory drives after it is modulated by attention (G)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = p[\"theta\"].size // 2\n",
    "f, (ax_g, ax_i) = plt.subplots(2, 1, sharex=True)\n",
    "ax_g.plot(p[\"x\"], G[idx])\n",
    "ax_g.set(title=\"Excitatory drive\")\n",
    "ax_i.plot(p[\"x\"], I[idx])\n",
    "ax_i.set(xlabel=\"Space\", title=\"Inhibitory drive\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the effect of attention on the excitatory drive is to\n",
    "multiply the original excitatory input by a narrow Gaussian (ranging\n",
    "between 1 and 2), which increases the excitatory drive only within cells\n",
    "with RF's near the attended location (like the one we're plotted from\n",
    "above).\n",
    "\n",
    "The inhibitory drive is the convolution of the excitatory drive by a\n",
    "pooling filter, which simply spatially blurrs the excitatory drive. Since\n",
    "the spike is so narrow, the spatial blurring by the pooling process\n",
    "leaves the inhibitory drive very similar for the attended and unattened\n",
    "regions of space (left vs. right).\n",
    "\n",
    "The output of the model is basically the ratio of the excitatory and the\n",
    "inhibitory drives.  Since convolution is linear, changing the contrast\n",
    "simply scales these curves up and down (try running this section again\n",
    "with a different contrast.  Only the y-axis scales). So you can see how\n",
    "for the neurons with RF's centered at the focus of attention, the effect\n",
    "of attention is all in the numerator.  So scaling by contrast simply\n",
    "scales the response.  Contrast gain!\n",
    "\n",
    "Now, consider the same stimulus but with a broad focus of spatial attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend[\"x_width\"] = [30]\n",
    "R, S, E, A, G, I = normalization_model(p, stim, attend, return_all=True)\n",
    "\n",
    "ax_g.plot(p[\"x\"], G[idx], ls=\"--\")\n",
    "ax_i.plot(p[\"x\"], I[idx, ], ls=\"--\")\n",
    "ax_g.legend([\"Narrow focus\", \"Broad focus\"])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of a broad focus of spatial attention is to boost a broader \n",
    "neuronal population since we multiplied by a broader Gaussian.  Note,\n",
    "however, that the peak excitatory drive is the same as for the narrow\n",
    "focus. \n",
    "\n",
    "This time, the inhibitory drive is strongly affected by attention.  This\n",
    "is because the blurring of the new excitatory drive by the attention\n",
    "filter is summing the response over a lot of active neurons. \n",
    "\n",
    "As before, contrast simply scales both the excitatory and inhibitory\n",
    "drives up and down.  But now both the numerator AND denominator for the\n",
    "model are growing faster with contrast for the 'attended' neuron.  This\n",
    "is just like changing the contrast of the stimulus with attention.\n",
    "Contrast gain!  \n",
    "\n",
    "Reynolds and Heeger go and predict a range of published papers showing\n",
    "both contrast gain and response gain effects of attention just by\n",
    "changing the spatial focus of attention relative to the size of the\n",
    "stimulus. This helps to settle a lot of discrepancies (and debates) in\n",
    "the literature.  This simple explanation also leads to some testable\n",
    "hypotheses about how the spatial focus of attention should affect\n",
    "neronal, behavioral and fMRI measurements.\n",
    "\n",
    "## Attention and orientation tuning.\n",
    "\n",
    "McAdams and Maunsell (1999) measured how the orientation tuning of V4\n",
    "neurons are affected by spatial attention by measuring orientation tuning\n",
    "while monkeys attended either inside or outside the receptive field of\n",
    "each neuron. \n",
    "\n",
    "The model naturally predicts how spatial and feature-based attention\n",
    "influences orientation tuning curves.  We can see this by simply slicing\n",
    "through the neural images along the orientation dimension.  For\n",
    "simplicity, we'll assume that as before, while feature-based attention is\n",
    "localized, attention is directed to all orientations.\n",
    "\n",
    "Let's set up some reasonable stimulus and attention conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = dict(\n",
    "    x_center=[-100, 100],\n",
    "    x_width=[10, 10],\n",
    "    th_center=[0, 0],\n",
    "    th_width=[1, 1],\n",
    "    contrast=[1, 1],\n",
    ")\n",
    "\n",
    "attend = dict(\n",
    "    x_center=[-100],\n",
    "    x_width=[10],\n",
    "    th_center=[0],\n",
    "    th_width=[1e3],\n",
    "    rng=(1, 4),\n",
    ")\n",
    "\n",
    "R = normalization_model(p, stim, attend)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(p[\"theta\"], R[:, x_idxs[0]])\n",
    "ax.plot(p[\"theta\"], R[:, x_idxs[1]])\n",
    "ax.set(xlabel=\"Orientation (deg)\")\n",
    "ax.legend([\"Attended\", \"Unattended\"], loc=\"upper left\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks just like the scaling of the tuning functions seen by McAdams and Maunsell\n",
    "(1999).  \n",
    "\n",
    "You might think it's weird that we're plotting slices of the neural image\n",
    "which is really the model's prediction of the response of a bunch of\n",
    "different neurons to the same stimulus.  But if you think about it, this\n",
    "is just the same as plotting the response of the same neuron to a range\n",
    "of stimuli.  \n",
    "\n",
    "The model (as I've implemented it) used this 'or' rule all along but it\n",
    "wasn't noticeable because `attend[\"th_width\"]` is very large , so 'and'\n",
    "and 'or' predict the same thing.\n",
    "\n",
    "But with `attend[\"th_width\"] = 30`, we can now look at the effects of\n",
    "feature based attention.\n",
    "\n",
    "## Moran and Desimone\n",
    "\n",
    "The study by Moran and Desimone (1985) was the first to show attentional\n",
    "effects in monkey cortex.  V4 neurons were measured with two oriented bars \n",
    "in the receptive field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = dict(\n",
    "    x_center=[-100, -100],\n",
    "    x_width=[10, 10],\n",
    "    th_center=[0, 90],\n",
    "    th_width=[1, 1],\n",
    "    contrast=[1, 1],\n",
    ")\n",
    "\n",
    "attend = dict(\n",
    "    x_center=[-100],\n",
    "    x_width=[2],\n",
    "    th_center=[0],\n",
    "    th_width=[30],\n",
    "    rng=(1, 2),\n",
    "    method=\"add\",\n",
    ")\n",
    "\n",
    "Rpref = normalization_model(p, stim, attend)\n",
    "Rpref, Spref, Epref, Apref, Gpref, Ipref = normalization_model(p, stim, attend, return_all=True)\n",
    "\n",
    "attend[\"th_center\"] = [90]\n",
    "Rnonpref = normalization_model(p, stim, attend)\n",
    "\n",
    "y1 = Rpref[th_idxs[0], x_idxs[0]]\n",
    "y2 = Rnonpref[th_idxs[0], x_idxs[0]]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.bar([\"Attend\\nPref\", \"Attend\\nNon-pref\"], [y1, y2])\n",
    "ax.set(ylabel=\"Model response\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-similarity gain\n",
    "\n",
    "This 'cross' profile of attentional gain means that feature-based\n",
    "attention should influence responses for neurons with receptive fields\n",
    "outside the focus of spatial attention.  This turns out to be true.\n",
    "Treue and Martinez-Trujillo (1999) originally discovered this by\n",
    "measuring the response to an unattended moving stimulus in MT when\n",
    "feature-based attention was altered by having the monkey perform a\n",
    "task on a stimulus in the opposite hemfield.  \n",
    "\n",
    "We can demonstrate this with the model by presenting stimuli that simulate\n",
    "the conditions for an fMRI experiment by Saenz et al. (2002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = dict(\n",
    "    x_center=[-100, -100, 100],\n",
    "    x_width=[10, 10, 10],\n",
    "    th_center=[0, 90, 0],\n",
    "    th_width=[2, 2, 2],\n",
    "    contrast=[1, 1, 1],\n",
    ")\n",
    "\n",
    "attend = dict(\n",
    "    x_center=[-100],\n",
    "    x_width=[2],\n",
    "    th_center=[0],\n",
    "    th_width=[30],\n",
    "    rng=(1, 2),\n",
    "    method=\"add\",\n",
    ")\n",
    "\n",
    "Rpref, S, E, Apref, G, I = normalization_model(p, stim, attend, return_all=True)\n",
    "\n",
    "show_neural_image(p, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real experiment, the stimuli were moving dots instead of oriented\n",
    "stimuli but it doesn't matter here.  On the attended (left) side, two\n",
    "stimuli were presented overlapping in space - one vertical and one\n",
    "horizontal.  The unattended side contained a single oriented stimulus.\n",
    "\n",
    "Save the response to the unattended stimulus for the neuron that both\n",
    "prefers that stimulus and the attended orientation (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yP = Rpref[th_idxs[1], x_idxs[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll shift feature-based attention to the non-preferred component of the attended stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend[\"th_center\"] = [90]\n",
    "Rnonpref = normalization_model(p, stim, attend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out the same neuron's response and plot both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yNP = Rnonpref[th_idxs[1], x_idxs[1]]\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.bar([\"Attend out\\npreferred\", \"Attend out\\nnon-preferred\"], [yP, yNP])\n",
    "ax.set(ylabel=\"Model response\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
