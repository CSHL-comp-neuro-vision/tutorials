{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b073f5-2d60-48a3-91f9-d5df5fd456d5",
   "metadata": {},
   "source": [
    "## Adapted (mostly comments added) from [pytorch.org](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "js(jserences@ucsd.edu) for CSHL 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bbc9a8-d2ae-4fa8-8d4c-a6222648d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14109205-8ecd-4367-bc1c-f50014f40be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6168df53-d53d-4e22-b966-f5966728c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# number of 'samples' or 'trials' in each training batch\n",
    "# send in >1 before backprop - usually leads to more effecient \n",
    "# training\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# y is the image label\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fe0b8-a5aa-448e-b9e7-15c22df49e81",
   "metadata": {},
   "source": [
    "#### Have a look at stuff in dataloader (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce0af42-0c30-4114-800e-bf8f558da586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
      "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
      "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdUElEQVR4nO3db2yV9f3/8ddpKYd/7altaU+P/LH8EYxAl6F0HcpUGkq3GBFuqPMGGqLBFTNk6sIyQbdlnSxxxoXpbiwwM1FnMmCaSILVlmwrGFBCzEZDSZUibZlozymtbbH9/G7ws98d+fu5OO27Lc9H8knoOde717tXr/bFOefq+4Scc04AAAyyNOsGAABXJwIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJkZZN/BNfX19OnHihDIzMxUKhazbAQB4cs6pvb1dsVhMaWkXfpwz5ALoxIkTmjx5snUbAIAr1NTUpEmTJl3w/iH3FFxmZqZ1CwCAFLjU7/MBC6DNmzfruuuu05gxY1RSUqL333//sup42g0ARoZL/T4fkAB6/fXXtW7dOm3cuFEffPCBiouLVV5erpMnTw7E7gAAw5EbAAsWLHCVlZX9H/f29rpYLOaqqqouWRuPx50kFovFYg3zFY/HL/r7PuWPgHp6enTgwAGVlZX135aWlqaysjLV1dWds313d7cSiUTSAgCMfCkPoM8++0y9vb0qKChIur2goEAtLS3nbF9VVaVIJNK/uAIOAK4O5lfBrV+/XvF4vH81NTVZtwQAGAQp/zugvLw8paenq7W1Nen21tZWRaPRc7YPh8MKh8OpbgMAMMSl/BHQ6NGjNX/+fFVXV/ff1tfXp+rqapWWlqZ6dwCAYWpAJiGsW7dOK1eu1E033aQFCxbo+eefV0dHhx588MGB2B0AYBgakAC655579N///lcbNmxQS0uLvvWtb2nXrl3nXJgAALh6hZxzzrqJ/5VIJBSJRKzbAABcoXg8rqysrAveb34VHADg6kQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATKQ+gp59+WqFQKGnNnj071bsBAAxzowbik95444165513/m8nowZkNwCAYWxAkmHUqFGKRqMD8akBACPEgLwGdOTIEcViMU2bNk3333+/jh07dsFtu7u7lUgkkhYAYORLeQCVlJRo69at2rVrl1588UU1Njbq1ltvVXt7+3m3r6qqUiQS6V+TJ09OdUsAgCEo5JxzA7mDtrY2TZ06Vc8995xWrVp1zv3d3d3q7u7u/ziRSBBCADACxONxZWVlXfD+Ab86IDs7W9dff70aGhrOe384HFY4HB7oNgAAQ8yA/x3Q6dOndfToURUWFg70rgAAw0jKA+jxxx9XbW2tPv74Y/3rX//S3XffrfT0dN13332p3hUAYBhL+VNwx48f13333adTp05p4sSJuuWWW7R3715NnDgx1bsCAAxjA34Rgq9EIqFIJGLdBgDgCl3qIgRmwQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADAx4G9IBwAXkp6e7l3T19fnXTOYM5eDvMHm/74r9OWaMWOGd42kC745qAUeAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDANG7hCoVBoUGqCTIG+9tprvWskqbS01Lvm7bff9q7p6Ojwrhnqgky2DmLFihWB6p599tkUdxIcj4AAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYYBgpYCDIYNEgbr311kB1JSUl3jWxWMy75oUXXvCuGery8/O9a8rLy71rEomEd81QwyMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJhhGClyh9PR075qvvvrKu+amm27yrrnhhhu8aySptbXVu2bmzJneNdu3b/eu+fzzz71rxo4d610jSZ988ol3TW5urndNVlaWd83x48e9a4YaHgEBAEwQQAAAE94BtGfPHt15552KxWIKhULasWNH0v3OOW3YsEGFhYUaO3asysrKdOTIkVT1CwAYIbwDqKOjQ8XFxdq8efN579+0aZNeeOEFvfTSS9q3b5/Gjx+v8vJydXV1XXGzAICRw/sihIqKClVUVJz3Puecnn/+ef385z/XXXfdJUl6+eWXVVBQoB07dujee++9sm4BACNGSl8DamxsVEtLi8rKyvpvi0QiKikpUV1d3Xlruru7lUgkkhYAYORLaQC1tLRIkgoKCpJuLygo6L/vm6qqqhSJRPrX5MmTU9kSAGCIMr8Kbv369YrH4/2rqanJuiUAwCBIaQBFo1FJ5/4RW2tra/993xQOh5WVlZW0AAAjX0oDqKioSNFoVNXV1f23JRIJ7du3T6WlpancFQBgmPO+Cu706dNqaGjo/7ixsVEHDx5UTk6OpkyZorVr1+pXv/qVZs6cqaKiIj311FOKxWJatmxZKvsGAAxz3gG0f/9+3X777f0fr1u3TpK0cuVKbd26VU8++aQ6Ojr08MMPq62tTbfccot27dqlMWPGpK5rAMCwF3LOOesm/lcikVAkErFuA1eptDT/Z6X7+vq8a8aPH+9ds2HDBu+a7u5u7xop2Nd03XXXeddkZ2d713zxxRfeNUH/Axzk+xTkQqog513Q7+3atWsD1QURj8cv+rq++VVwAICrEwEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAhPfbMWBoC4VC3jVBB6IHmeAbZF9BatLT071rJKm3tzdQna/Vq1d717S0tHjXdHV1eddIwSZbB5k4/c13T74cQb63QaZ7S1JHR4d3TU9Pj3dNkHeCDofD3jVSsAnfQY7D5eAREADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMMIx0kgzUkNOhg0SCCDnj0FWT45GANFZWk++67z7smGo1613zwwQfeNRkZGd41kpSdne1dc+rUKe+azz//3LsmLy/PuyYzM9O7Rgo+1NZXkMG+48aNC7SvmTNnetccPHgw0L4uhUdAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDCMdJAM1pDQIEMNg9RIwQZ+BjkOgzlY9MEHH/SumTVrlndNU1OTd02QIZxBhuBK0tixY71rPv30U++aIENCgwzB7ezs9K6RpDFjxnjXDNbg4aDKy8u9axhGCgAYUQggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJi4qoeRBh3CGUSQYYNBhhoGGdQYpGYwxWIx75rly5cH2leQIZxHjhzxrpkwYYJ3TTgc9q7Jzc31rpGknp4e75og5/i4ceO8a4IIOtC2u7t7UPbV0dHhXRP053bhwoWB6gYCj4AAACYIIACACe8A2rNnj+68807FYjGFQiHt2LEj6f4HHnhAoVAoaS1dujRV/QIARgjvAOro6FBxcbE2b958wW2WLl2q5ubm/vXqq69eUZMAgJHH+yKEiooKVVRUXHSbcDisaDQauCkAwMg3IK8B1dTUKD8/X7NmzdIjjzyiU6dOXXDb7u5uJRKJpAUAGPlSHkBLly7Vyy+/rOrqaj377LOqra1VRUXFBS9NrKqqUiQS6V+TJ09OdUsAgCEo5X8HdO+99/b/e+7cuZo3b56mT5+umpoaLV68+Jzt169fr3Xr1vV/nEgkCCEAuAoM+GXY06ZNU15enhoaGs57fzgcVlZWVtICAIx8Ax5Ax48f16lTp1RYWDjQuwIADCPeT8GdPn066dFMY2OjDh48qJycHOXk5OiZZ57RihUrFI1GdfToUT355JOaMWOGysvLU9o4AGB48w6g/fv36/bbb+//+OvXb1auXKkXX3xRhw4d0p///Ge1tbUpFotpyZIl+uUvfxlojhUAYOQKuSATBAdQIpFQJBJRWlqa1zDOoMMGIU2cODFQ3dSpU71rZs+e7V0T5OnbIMM0Jamrq8u7Jshg0SCvdWZkZHjXBBmuKknjx48flJogX1NbW5t3TdDfD+np6d41QQaLnjlzxrsmyHknSZFIxLvm17/+tdf2vb29Onz4sOLx+EXPdWbBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMpPwtuVOlr69vwPdRUFAQqC7IFOjBmi4cZPpxUVGRd40kjRs3zrsmyNTf06dPe9ekpQX7v1WQScFBjvlXX33lXRPkeHd2dnrXSFJ3d7d3zejRo71rmpubvWuCfI+CHDtJ+uKLL7xrgkypvuaaa7xrgkzdlqRoNOpdk5ub67X95Z7fPAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgYsgOI/VVVlbmXROLxQLtK8hAzfz8fO+aIAM1gwxxDfL1SFJ7e7t3TZBBjUGGJ4ZCIe8aSQqHw941QQZWBvneBjl26enp3jVSsEGXQc6HeDzuXRPkZ2kwBTkfgvzcBhmCKwUbGus7PJdhpACAIY0AAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJITuM9I477tCoUZff3qpVq7z3cfjwYe8aSWpubvauSSQS3jVBBkn29PQMyn6CCjKwMsjwxN7eXu8aScrKyvKuCTL4NMggySADKzMyMrxrpGADYAsKCrxrbrzxRu+aIF/TYJ7jQQa5jhs3zrumq6vLu0YK1t/Jkye9tr/cc5VHQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwM2WGkBw4c8Bry+J3vfMd7H3PnzvWukaSFCxcGqvP11VdfedcEGfb5+eefe9cErYvH4941QYaRBhkQKkm5ubneNbNmzfKuCTJ8MsigVOecd40kFRcXe9ccOnTIu+bjjz/2rikrK/OuCYfD3jVS8OPnK8jP+qeffhpoX0EGI0+YMMFr+8sdBswjIACACQIIAGDCK4Cqqqp08803KzMzU/n5+Vq2bJnq6+uTtunq6lJlZaVyc3M1YcIErVixQq2trSltGgAw/HkFUG1trSorK7V3717t3r1bZ86c0ZIlS5Le4Oixxx7Tm2++qTfeeEO1tbU6ceKEli9fnvLGAQDDm9dFCLt27Ur6eOvWrcrPz9eBAwe0aNEixeNx/elPf9K2bdt0xx13SJK2bNmiG264QXv37g10oQAAYGS6oteAvr6iKScnR9LZK9fOnDmTdJXK7NmzNWXKFNXV1Z33c3R3dyuRSCQtAMDIFziA+vr6tHbtWi1cuFBz5syRJLW0tGj06NHKzs5O2ragoEAtLS3n/TxVVVWKRCL9a/LkyUFbAgAMI4EDqLKyUh999JFee+21K2pg/fr1isfj/aupqemKPh8AYHgI9Ieoa9as0VtvvaU9e/Zo0qRJ/bdHo1H19PSora0t6VFQa2urotHoeT9XOBwO/EdiAIDhy+sRkHNOa9as0fbt2/Xuu++qqKgo6f758+crIyND1dXV/bfV19fr2LFjKi0tTU3HAIARwesRUGVlpbZt26adO3cqMzOz/3WdSCSisWPHKhKJaNWqVVq3bp1ycnKUlZWlRx99VKWlpVwBBwBI4hVAL774oiTptttuS7p9y5YteuCBByRJv/vd75SWlqYVK1aou7tb5eXl+sMf/pCSZgEAI0fIDda0vcuUSCQUiUSs27go38F8klRSUuJdc/3113vXfPe73/Wuyc/P966Rgg3HHD9+vHdNkMGiQU/rvr4+75ogQ1kPHz7sXbN7927vmrffftu7Rjo70WSo+vvf/+5dM2XKlED7+uyzz7xrggwEDlITZICpdPZPX3w9/vjjXts759TZ2al4PH7R3xPMggMAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmGAaNgBgQDANGwAwJBFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEx4BVBVVZVuvvlmZWZmKj8/X8uWLVN9fX3SNrfddptCoVDSWr16dUqbBgAMf14BVFtbq8rKSu3du1e7d+/WmTNntGTJEnV0dCRt99BDD6m5ubl/bdq0KaVNAwCGv1E+G+/atSvp461btyo/P18HDhzQokWL+m8fN26cotFoajoEAIxIV/QaUDwelyTl5OQk3f7KK68oLy9Pc+bM0fr169XZ2XnBz9Hd3a1EIpG0AABXARdQb2+v+8EPfuAWLlyYdPsf//hHt2vXLnfo0CH3l7/8xV177bXu7rvvvuDn2bhxo5PEYrFYrBG24vH4RXMkcACtXr3aTZ061TU1NV10u+rqaifJNTQ0nPf+rq4uF4/H+1dTU5P5QWOxWCzWla9LBZDXa0BfW7Nmjd566y3t2bNHkyZNuui2JSUlkqSGhgZNnz79nPvD4bDC4XCQNgAAw5hXADnn9Oijj2r79u2qqalRUVHRJWsOHjwoSSosLAzUIABgZPIKoMrKSm3btk07d+5UZmamWlpaJEmRSERjx47V0aNHtW3bNn3/+99Xbm6uDh06pMcee0yLFi3SvHnzBuQLAAAMUz6v++gCz/Nt2bLFOefcsWPH3KJFi1xOTo4Lh8NuxowZ7oknnrjk84D/Kx6Pmz9vyWKxWKwrX5f63R/6/8EyZCQSCUUiEes2AABXKB6PKysr64L3MwsOAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGBiyAWQc866BQBAClzq9/mQC6D29nbrFgAAKXCp3+chN8QecvT19enEiRPKzMxUKBRKui+RSGjy5MlqampSVlaWUYf2OA5ncRzO4jicxXE4aygcB+ec2tvbFYvFlJZ24cc5owaxp8uSlpamSZMmXXSbrKysq/oE+xrH4SyOw1kch7M4DmdZH4dIJHLJbYbcU3AAgKsDAQQAMDGsAigcDmvjxo0Kh8PWrZjiOJzFcTiL43AWx+Gs4XQchtxFCACAq8OwegQEABg5CCAAgAkCCABgggACAJgYNgG0efNmXXfddRozZoxKSkr0/vvvW7c06J5++mmFQqGkNXv2bOu2BtyePXt05513KhaLKRQKaceOHUn3O+e0YcMGFRYWauzYsSorK9ORI0dsmh1AlzoODzzwwDnnx9KlS22aHSBVVVW6+eablZmZqfz8fC1btkz19fVJ23R1damyslK5ubmaMGGCVqxYodbWVqOOB8blHIfbbrvtnPNh9erVRh2f37AIoNdff13r1q3Txo0b9cEHH6i4uFjl5eU6efKkdWuD7sYbb1Rzc3P/+sc//mHd0oDr6OhQcXGxNm/efN77N23apBdeeEEvvfSS9u3bp/Hjx6u8vFxdXV2D3OnAutRxkKSlS5cmnR+vvvrqIHY48Gpra1VZWam9e/dq9+7dOnPmjJYsWaKOjo7+bR577DG9+eabeuONN1RbW6sTJ05o+fLlhl2n3uUcB0l66KGHks6HTZs2GXV8AW4YWLBggausrOz/uLe318ViMVdVVWXY1eDbuHGjKy4utm7DlCS3ffv2/o/7+vpcNBp1v/3tb/tva2trc+Fw2L366qsGHQ6Obx4H55xbuXKlu+uuu0z6sXLy5EknydXW1jrnzn7vMzIy3BtvvNG/zX/+8x8nydXV1Vm1OeC+eRycc+573/ue+/GPf2zX1GUY8o+Aenp6dODAAZWVlfXflpaWprKyMtXV1Rl2ZuPIkSOKxWKaNm2a7r//fh07dsy6JVONjY1qaWlJOj8ikYhKSkquyvOjpqZG+fn5mjVrlh555BGdOnXKuqUBFY/HJUk5OTmSpAMHDujMmTNJ58Ps2bM1ZcqUEX0+fPM4fO2VV15RXl6e5syZo/Xr16uzs9OivQsacsNIv+mzzz5Tb2+vCgoKkm4vKCjQ4cOHjbqyUVJSoq1bt2rWrFlqbm7WM888o1tvvVUfffSRMjMzrdsz0dLSIknnPT++vu9qsXTpUi1fvlxFRUU6evSofvazn6miokJ1dXVKT0+3bi/l+vr6tHbtWi1cuFBz5syRdPZ8GD16tLKzs5O2Hcnnw/mOgyT98Ic/1NSpUxWLxXTo0CH99Kc/VX19vf72t78ZdptsyAcQ/k9FRUX/v+fNm6eSkhJNnTpVf/3rX7Vq1SrDzjAU3Hvvvf3/njt3rubNm6fp06erpqZGixcvNuxsYFRWVuqjjz66Kl4HvZgLHYeHH364/99z585VYWGhFi9erKNHj2r69OmD3eZ5Dfmn4PLy8pSenn7OVSytra2KRqNGXQ0N2dnZuv7669XQ0GDdipmvzwHOj3NNmzZNeXl5I/L8WLNmjd566y299957SW/fEo1G1dPTo7a2tqTtR+r5cKHjcD4lJSWSNKTOhyEfQKNHj9b8+fNVXV3df1tfX5+qq6tVWlpq2Jm906dP6+jRoyosLLRuxUxRUZGi0WjS+ZFIJLRv376r/vw4fvy4Tp06NaLOD+ec1qxZo+3bt+vdd99VUVFR0v3z589XRkZG0vlQX1+vY8eOjajz4VLH4XwOHjwoSUPrfLC+CuJyvPbaay4cDrutW7e6f//73+7hhx922dnZrqWlxbq1QfWTn/zE1dTUuMbGRvfPf/7TlZWVuby8PHfy5Enr1gZUe3u7+/DDD92HH37oJLnnnnvOffjhh+6TTz5xzjn3m9/8xmVnZ7udO3e6Q4cOubvuussVFRW5L7/80rjz1LrYcWhvb3ePP/64q6urc42Nje6dd95x3/72t93MmTNdV1eXdesp88gjj7hIJOJqampcc3Nz/+rs7OzfZvXq1W7KlCnu3Xffdfv373elpaWutLTUsOvUu9RxaGhocL/4xS/c/v37XWNjo9u5c6ebNm2aW7RokXHnyYZFADnn3O9//3s3ZcoUN3r0aLdgwQK3d+9e65YG3T333OMKCwvd6NGj3bXXXuvuuece19DQYN3WgHvvvfecpHPWypUrnXNnL8V+6qmnXEFBgQuHw27x4sWuvr7etukBcLHj0NnZ6ZYsWeImTpzoMjIy3NSpU91DDz004v6Tdr6vX5LbsmVL/zZffvml+9GPfuSuueYaN27cOHf33Xe75uZmu6YHwKWOw7Fjx9yiRYtcTk6OC4fDbsaMGe6JJ55w8XjctvFv4O0YAAAmhvxrQACAkYkAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJ/wfS3ncBjBZLmwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image label\n",
    "print(y)\n",
    "\n",
    "# show a training image from this batch\n",
    "# image is 1x24x24 - greyscale so only 1 channel\n",
    "# and 24x24 pixels\n",
    "img = 0\n",
    "plt.imshow(X[img,0,:,:],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2396cd-f46b-4b14-9df8-bcf92186fab1",
   "metadata": {},
   "source": [
    "#### And other relevant properties of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a125b81-99e0-4974-b0c2-343229fb0d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a11120-5062-4af0-8daa-8dbe92eed570",
   "metadata": {},
   "source": [
    "### Get cpu, gpu or mps (apple M1,M2,MX) device for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6da9ca4-6bfa-4b80-bd26-b24e85c7b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# query available devices \n",
    "# and save name - do in order: cuda/mps/cpu\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    # init the model - main part is to make a 'Sequential' container\n",
    "    # accepts an input and sends it to the first module in the container. \n",
    "    # Outputs are sequentially passed into subsequent modules (“chaining”) \n",
    "    # and an output is returned by the last module.\n",
    "    # In this case we have a series of 'Linear' modules, with ReLU (rectified\n",
    "    # linear unit) activation functions in between hidden layers. \n",
    "    def __init__(self):\n",
    "\n",
    "        # init from nn.Module super class to inherit all methods\n",
    "        super().__init__()\n",
    "        \n",
    "        # give flatten to self to shape the inputs to the sequential \n",
    "        # processing chain\n",
    "        self.flatten = nn.Flatten() \n",
    "\n",
    "        # build the sequential container \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    # define what to do on each forward pass of the model\n",
    "    # 1) flatten input to convert 28x28 image into a vector \n",
    "    #    with 784 pixel values \n",
    "    # 2) pass input to sequential relu stack\n",
    "    # 3) \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# send the network to the desired device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ccae5-65e1-4402-8e36-547bcc1d1c3b",
   "metadata": {},
   "source": [
    "#### define a loss function and an optimizer\n",
    "* loss function: function you are trying to minimize during training, in this case the cross entropy (difference between two probability distributions) between input logits (log-probabilities) and target...useful when training a classification problem with C classes. \n",
    "* optimizer object to store the current state and will then update the parameters based on the computed gradients (lots of different kinds...often use an Adam optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3d477b-2327-4141-950e-c9c41d975c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4d1ff-24f8-4c24-96e2-bd89779a2bb9",
   "metadata": {},
   "source": [
    "#### Define what happens in each training loop\n",
    "* Train model (compute current weights)\n",
    "* Makes predictions on the current batch of training images\n",
    "* backprop the prediction error to adjust model parameters using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7331a20-3b9b-4b8e-8ea2-7c175d8250bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    # get the number of batches \n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # train the model object \n",
    "    # (note that there is no output because the weights, etc\n",
    "    # are stored internally in this instance of the object so\n",
    "    # they are modified in the object)...below you will send in \n",
    "    # new training data, compute loss, do backprop...\n",
    "    model.train()\n",
    "\n",
    "    # pass in the batch \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = model(X)\n",
    "\n",
    "        # compute difference between pred \n",
    "        # and actual label (in y)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single parameter update\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero out the gradient buffers before updating model params (e.g. Weights/biases)\n",
    "        # because gradients accumulate so the new gradient will be \n",
    "        # combined with the old gradient which has already been used to update the model \n",
    "        # parameters and the combined gradient may point in the wrong direction\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print out progress...\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6b25e-15b1-4b91-a2ab-d16c5d4f20a0",
   "metadata": {},
   "source": [
    "#### Functon to evaluate (test) the model with held out images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711c6ffc-62b9-4e42-9c9e-a546e83e5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # get the model ready for evaluation (testing)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # this means \"do this stuff with the gradients turned off\" \n",
    "    # i.e. fix the gradients during testing\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # loop over images in batch\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # move the images to the current device (e.g. gpu, mps, or cpu)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # make a prediction for each image in the current batch\n",
    "            pred = model(X)\n",
    "\n",
    "            # pass predictions and real labels into our loss function\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # keep track of how well we're doing by comparing each prediction\n",
    "            # with the actual label \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # print it out\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d215f43-408b-4753-bdf0-2dbf954d4501",
   "metadata": {},
   "source": [
    "#### Train the model in 10 epochs, with each epoch containing 60000 training iterations. Then test the model after each epoch to see how training is going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6f9ef9-11a6-4c98-81f7-f3555d6649c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304897  [   64/60000]\n",
      "loss: 2.285691  [ 6464/60000]\n",
      "loss: 2.265212  [12864/60000]\n",
      "loss: 2.259078  [19264/60000]\n",
      "loss: 2.232242  [25664/60000]\n",
      "loss: 2.210690  [32064/60000]\n",
      "loss: 2.209539  [38464/60000]\n",
      "loss: 2.172639  [44864/60000]\n",
      "loss: 2.177558  [51264/60000]\n",
      "loss: 2.136769  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.132194 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.153867  [   64/60000]\n",
      "loss: 2.139712  [ 6464/60000]\n",
      "loss: 2.077411  [12864/60000]\n",
      "loss: 2.087494  [19264/60000]\n",
      "loss: 2.023602  [25664/60000]\n",
      "loss: 1.977502  [32064/60000]\n",
      "loss: 1.995322  [38464/60000]\n",
      "loss: 1.915332  [44864/60000]\n",
      "loss: 1.927176  [51264/60000]\n",
      "loss: 1.841603  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.839641 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.892559  [   64/60000]\n",
      "loss: 1.855560  [ 6464/60000]\n",
      "loss: 1.730505  [12864/60000]\n",
      "loss: 1.760389  [19264/60000]\n",
      "loss: 1.643277  [25664/60000]\n",
      "loss: 1.618183  [32064/60000]\n",
      "loss: 1.627528  [38464/60000]\n",
      "loss: 1.534569  [44864/60000]\n",
      "loss: 1.564135  [51264/60000]\n",
      "loss: 1.454360  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 1.472053 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.554820  [   64/60000]\n",
      "loss: 1.517772  [ 6464/60000]\n",
      "loss: 1.362466  [12864/60000]\n",
      "loss: 1.427711  [19264/60000]\n",
      "loss: 1.307897  [25664/60000]\n",
      "loss: 1.321471  [32064/60000]\n",
      "loss: 1.329491  [38464/60000]\n",
      "loss: 1.257857  [44864/60000]\n",
      "loss: 1.295869  [51264/60000]\n",
      "loss: 1.199783  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.224137 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.310794  [   64/60000]\n",
      "loss: 1.292580  [ 6464/60000]\n",
      "loss: 1.122488  [12864/60000]\n",
      "loss: 1.225618  [19264/60000]\n",
      "loss: 1.099413  [25664/60000]\n",
      "loss: 1.134541  [32064/60000]\n",
      "loss: 1.156311  [38464/60000]\n",
      "loss: 1.092009  [44864/60000]\n",
      "loss: 1.133574  [51264/60000]\n",
      "loss: 1.057150  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.075184 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.152064  [   64/60000]\n",
      "loss: 1.156007  [ 6464/60000]\n",
      "loss: 0.968176  [12864/60000]\n",
      "loss: 1.103586  [19264/60000]\n",
      "loss: 0.974589  [25664/60000]\n",
      "loss: 1.010029  [32064/60000]\n",
      "loss: 1.051321  [38464/60000]\n",
      "loss: 0.987477  [44864/60000]\n",
      "loss: 1.027040  [51264/60000]\n",
      "loss: 0.968943  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.979464 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.041392  [   64/60000]\n",
      "loss: 1.068094  [ 6464/60000]\n",
      "loss: 0.861938  [12864/60000]\n",
      "loss: 1.022321  [19264/60000]\n",
      "loss: 0.895885  [25664/60000]\n",
      "loss: 0.920374  [32064/60000]\n",
      "loss: 0.981729  [38464/60000]\n",
      "loss: 0.918388  [44864/60000]\n",
      "loss: 0.951127  [51264/60000]\n",
      "loss: 0.908413  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.912775 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.958756  [   64/60000]\n",
      "loss: 1.005713  [ 6464/60000]\n",
      "loss: 0.784107  [12864/60000]\n",
      "loss: 0.962847  [19264/60000]\n",
      "loss: 0.842266  [25664/60000]\n",
      "loss: 0.852531  [32064/60000]\n",
      "loss: 0.931289  [38464/60000]\n",
      "loss: 0.870521  [44864/60000]\n",
      "loss: 0.894659  [51264/60000]\n",
      "loss: 0.863358  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.863305 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.893899  [   64/60000]\n",
      "loss: 0.957448  [ 6464/60000]\n",
      "loss: 0.724273  [12864/60000]\n",
      "loss: 0.916753  [19264/60000]\n",
      "loss: 0.802823  [25664/60000]\n",
      "loss: 0.800082  [32064/60000]\n",
      "loss: 0.892141  [38464/60000]\n",
      "loss: 0.836105  [44864/60000]\n",
      "loss: 0.851622  [51264/60000]\n",
      "loss: 0.827621  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.824847 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.841214  [   64/60000]\n",
      "loss: 0.917417  [ 6464/60000]\n",
      "loss: 0.676911  [12864/60000]\n",
      "loss: 0.880008  [19264/60000]\n",
      "loss: 0.772144  [25664/60000]\n",
      "loss: 0.759061  [32064/60000]\n",
      "loss: 0.860163  [38464/60000]\n",
      "loss: 0.810130  [44864/60000]\n",
      "loss: 0.817971  [51264/60000]\n",
      "loss: 0.798018  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.793710 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433ef91-52de-4ac1-b914-d3ccd7d637c2",
   "metadata": {},
   "source": [
    "#### Save the model (structure + weights and other parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6da5b57-fe35-49b5-9960-9987322cb6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdd455-7aba-4534-837f-90f245863e40",
   "metadata": {},
   "source": [
    "#### Load the model structure and the state dictionary (i.e. state of all layer weights, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a85ef3-9849-4f3e-85bb-edd77c1adc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23164c9-d995-4e26-a8ea-123492c431e7",
   "metadata": {},
   "source": [
    "#### General approach for accessing wieghts via the `state_dictionary` (dict with all params for all layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce4aeb72-d42c-4896-893c-433a367482c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_relu_stack.0.weight\n",
      "linear_relu_stack.0.bias\n",
      "linear_relu_stack.2.weight\n",
      "linear_relu_stack.2.bias\n",
      "linear_relu_stack.4.weight\n",
      "linear_relu_stack.4.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0090, -0.0086,  0.0087,  ...,  0.0033, -0.0352, -0.0137],\n",
       "        [-0.0147, -0.0360,  0.0345,  ..., -0.0132, -0.0312, -0.0161],\n",
       "        [-0.0293,  0.0251, -0.0069,  ..., -0.0293,  0.0340,  0.0213],\n",
       "        ...,\n",
       "        [ 0.0072, -0.0389,  0.0280,  ..., -0.0282, -0.0104,  0.0107],\n",
       "        [-0.0132, -0.0167,  0.0272,  ...,  0.0435, -0.0412, -0.0356],\n",
       "        [ 0.0238,  0.0243,  0.0061,  ..., -0.0242, -0.0357,  0.0052]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = model.state_dict()\n",
    "\n",
    "# print out dictionary keys\n",
    "for k in sd.keys():\n",
    "    print(k)\n",
    "\n",
    "# Then can access values in the dictionary...\n",
    "# e.g. weights from hidden 2\n",
    "sd['linear_relu_stack.2.weight']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
