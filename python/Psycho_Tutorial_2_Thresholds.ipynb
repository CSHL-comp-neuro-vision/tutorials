{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psychophysics Tutorial II: Threshold Estimation\n",
    "\n",
    "Written by G.M. Boynton for Psych 448 at the University of Washington\n",
    "\n",
    "Adapted for CSHL in June 2010, and revised for 2012 and 2014.\n",
    "\n",
    "Translated to Python by Michael Waskom in 2018.\n",
    "\n",
    "The goal of this tutorial is to define the detection threshold. We'll\n",
    "simulate psychophysical data based on signal detection theory.  Then\n",
    "we'll fit the simulated data with a parametric psychometric function\n",
    "using a maximum likelihood procedure. Then we'll then pick off the\n",
    "stimulus intensity level that predicts 80% correct performance.\n",
    "\n",
    "It is suggested you look at the Psychophysics I tutorial first, which\n",
    "covers signal detection theory, ROC curves and the 2AFC paradigm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import sqrt, log, exp\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The psychometric function\n",
    "\n",
    "Real psychophysicists don't report proportion correct as a dependent\n",
    "measure.  They often don't report d-prime either.  Instead, they report\n",
    "the 'threshold', which is the stimulus intensity required to produce a\n",
    "desired level of performance.  A big advantage to the threshold is that\n",
    "it is in the physical units of the stimulus, which can be more easily\n",
    "compared across paradigms and labs.  Another advantage is that it can\n",
    "produce a family of stimuli that all presumably produce the same internal\n",
    "response.\n",
    "\n",
    "There are various ways to find the intensity that yields a criterion\n",
    "performance level.  We'll implement the a 3-down 1-up staircase method.\n",
    "In this tutorial we'll simulate a subject's performance by assuming a\n",
    "specific psychometric function that predicts performance in a 2AFC task.\n",
    "This function will be a Weibull function.\n",
    "\n",
    "## A SDT model to generate psychometric functions.\n",
    "\n",
    "If you've gone through the Psycho_Tutorial_I tutorial you've learned that\n",
    "for a 2AFC experiment under signal detection theory (SDT), d-prime can\n",
    "be calculated from percent correct by:\n",
    "\n",
    "\n",
    "intensityList = linspace(0,4,101);\n",
    "dPrime = intensityList;\n",
    "PC = normcdf(dPrime,0,sqrt(2));\n",
    "figure(1)\n",
    "clf\n",
    "plot(intensityList,PC,'b-');\n",
    "ylabel('Proportion correct');\n",
    "xlabel('Stimulus intensity (d-prime)');\n",
    "title('This is a cumulative normal');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c = .8\n",
    "d_prime = sqrt(2) * norm.ppf(p_c)\n",
    "\n",
    "# We could also go the other way\n",
    "assert norm.cdf(d_prime / sqrt(2)) == p_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if the physical stimulus strength is equivalent to\n",
    "d-prime, then the expected psychometric function should be a cumulative\n",
    "normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_prime = x = np.linspace(0, 4, 101)\n",
    "p_c = norm.cdf(d_prime, 0, sqrt(2))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(x, p_c)\n",
    "ax.set(xlabel=\"Stimulus intensity or $d'$\",\n",
    "       ylabel=\"Proportion correct\",\n",
    "       title=\"Cumulative normal\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if d-prime represents the signal-to-noise ratio for the internal\n",
    "response, it does not typically increase linearly with physical stimulus\n",
    "strength.  In most cases, like for luminance or contrast, it seems that\n",
    "d-prime increases in a compressive fashion with stimulus strength.  A\n",
    "simple compressive function is a power function with an exponent less\n",
    "than 1.\n",
    "\n",
    "If d-prime increases as a power function then then percent correct will\n",
    "no longer be a cumulative normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 30, 101)\n",
    "d_prime = x ** .5\n",
    "p_c = norm.cdf(d_prime / sqrt(2))\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(x, p_c)\n",
    "ax.set(xlabel=\"Stimulus intensity or $d'$\",\n",
    "       ylabel=\"Proportion correct\",\n",
    "       title=\"Not a cumulative normal\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can plot on a log axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(x[1:], p_c[1:])\n",
    "ax.set(xscale=\"log\",\n",
    "       xlabel=\"Stimulus intensity or $d'$\",\n",
    "       ylabel=\"Proportion correct\",\n",
    "       title=\"This looks like a Weibull function\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Weibull function\n",
    "\n",
    "The Weibull cumulative distribution function is one of a variety of\n",
    "functions that predict a psychometric function.  It has the general form:\n",
    "\n",
    "$$y = 1 - e^{-(x/\\lambda)^k},$$\n",
    "\n",
    "where $x$ is the stimulus intensity and $y$ is the proportion correct.\n",
    "$\\lambda$ and $k$ are free parameters.  A reparameterized version for our purposes\n",
    "is:\n",
    "\n",
    "$$y = 1 - (1-g)\\,e^{-(kx/t)^b},$$\n",
    "\n",
    "where\n",
    "\n",
    "$$k = -\\log \\bigg(\\frac{1-a}{1-g}\\bigg)^{1/b}.$$\n",
    "\n",
    "With this parameterization, $g$ is the performance expected at chance\n",
    "(0.5 in our case for 2AFC), $t$ is the threshold, and $a$ is the performance level\n",
    "that defines the threshold.  That is, when $x=t$ then $y=a$.  The parameter $b$\n",
    "determines the slope of the function.\n",
    "\n",
    "Here's our version of our function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull(x, b, t, g=.5):\n",
    "    x = np.asarray(x)\n",
    "    a = .5 ** (1 / 3)\n",
    "    k = (-log((1 - a) / (1 - g))) ** (1 / b)\n",
    "    y = 1 - (1 - g) * exp(-(k * x / t) ** b)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an interactive widget to let you play with the slope and threshold (actually the exponents) and see their effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def weibull_tutorial(b_exp=(-4, 4, .5), t_exp=(-4, 4, .5)):\n",
    "    x = np.logspace(-3, 3, 201, base=4)\n",
    "    y = weibull(x, 4 ** b_exp, 4 ** t_exp)\n",
    "    f, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.plot(x, y, lw=2)\n",
    "    ax.set_xscale(\"log\", basex=4)\n",
    "    ax.set(\n",
    "        ylim=(.5, 1),\n",
    "        xlabel=\"Intensity\",\n",
    "        ylabel=\"P(correct)\",\n",
    "    )\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the two parameters, we can generate a whole family of curves\n",
    "that are good at describing psychometric functions in a 2AFC experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax_b, ax_t) = plt.subplots(1, 2, figsize=(7, 4),\n",
    "                               sharex=True, sharey=True)\n",
    "\n",
    "xs = ts = .0625, .25, 1, 4, 16, 64\n",
    "bs = .5, 1, 2, 4\n",
    "x = np.logspace(-2.5, 3.5, 101, base=4)\n",
    "\n",
    "for b in bs:\n",
    "    y = weibull(x, b=b, t=1)\n",
    "    ax_b.plot(x, y, label=b)\n",
    "    \n",
    "for t in ts:\n",
    "    y = weibull(x, b=2, t=t)\n",
    "    ax_t.plot(x, y, label=t)\n",
    "\n",
    "ax_b.legend(loc=\"lower right\", title=\"$b$\")\n",
    "ax_t.legend(loc=\"lower right\", title=\"$t$\")\n",
    "ax_b.set_xscale(\"log\")\n",
    "ax_b.set(\n",
    "    xticks=xs,\n",
    "    xticklabels=xs,\n",
    "    xlabel=\"Intensity\",\n",
    "    ylabel=\"P(correct)\",\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how for each curve, when the intensity is equal to the threshold,\n",
    "then the proportion correct is ~80%. So the variable $t$ determines\n",
    "where on the x-axis the curve reaches 80%.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## The 3-down-1-up staircase\n",
    "\n",
    "The method of constant stimuli is simple but has the disadvantage that\n",
    "many of the trials are either too hard or too easy. These trials\n",
    "contribute relatively little to our estimate of the subject's threshold.\n",
    "\n",
    "Instead, we'll adjust the stimulus on each trial based on previous\n",
    "responses using a 3-down 1-up 'staircase' procedure. Whenever the subject\n",
    "makes an incorrect response, the next trial has an easier (larger)\n",
    "intensity.  If the subject gets three trials in a row correct, the next\n",
    "trial gets harder.\n",
    "\n",
    "Staircase procedures deserves a tutorial on its own.  A 3-down 1-up is\n",
    "the simplest.  There are many others, with names like Quest and Pest.\n",
    "\n",
    "First, we need to define the exponent for the compressive nonlinearity\n",
    "that transforms stimulus strength to d-prime. 0.3 is a good value to\n",
    "simulate the dimension of stimulus contrast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_mean = 0\n",
    "sd = 1\n",
    "p = .3\n",
    "n_trials = 200\n",
    "correct_in_a_row = 0\n",
    "\n",
    "# Control the randomness of the simulation\n",
    "random_state = np.random.RandomState(seed=100)\n",
    "\n",
    "# Initialize the results\n",
    "trials = np.arange(n_trials) + 1\n",
    "intensities = np.zeros(n_trials)\n",
    "responses = np.zeros(n_trials)\n",
    "\n",
    "# Start with the strongest stimulus\n",
    "x_idx = len(xs) - 1\n",
    "\n",
    "for i in range(n_trials):\n",
    "\n",
    "    intensities[i] = x_i = xs[x_idx]\n",
    "    signal_mean = x_i ** p\n",
    "\n",
    "    noise = norm.rvs(noise_mean, sd, random_state=random_state)\n",
    "    signal = norm.rvs(signal_mean, sd, random_state=random_state)\n",
    "    responses[i] = response = signal > noise\n",
    "\n",
    "    if response:\n",
    "        correct_in_a_row += 1\n",
    "        if correct_in_a_row == 3:\n",
    "            x_idx = max(0, x_idx - 1)\n",
    "            correct_in_a_row = 0\n",
    "    else:\n",
    "        correct_in_a_row = 0\n",
    "        x_idx = min(x_idx + 1, len(xs) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the staircase showing which trials were answered correctly (white) or incorrectly (red) and how the intensity level was adjusted by these responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.set_yscale(\"log\", basey=4)\n",
    "ax.step(trials, intensities, c=\".5\")\n",
    "ax.scatter(trials, intensities, c=responses,\n",
    "           cmap=\"Reds_r\", vmin=-.2, edgecolor=\".5\", zorder=3)\n",
    "ax.set(\n",
    "    xlim=(0, n_trials + 1),\n",
    "    xlabel=\"Trial\",\n",
    "    ylabel=\"Intensity\",\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the staircase, we can compile the results to generate a psychometric function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf_x = np.unique(intensities)\n",
    "pmf_y = [np.mean(responses[intensities == x_i]) for x_i in pmf_x]\n",
    "pmf_e = [stats.sem(responses[intensities == x_i]) for x_i in pmf_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax_pmf = plt.subplots(figsize=(5, 5))\n",
    "ax_pmf.errorbar(pmf_x, pmf_y, pmf_e,\n",
    "                marker=\"o\", ls=\"\")\n",
    "ax_pmf.set_xscale(\"log\", basex=4)\n",
    "ax_pmf.set(\n",
    "    xlabel=\"Intensity\",\n",
    "    ylabel=\"P(correct)\"\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have used seaborn to calculate the aggregate statistics\n",
    "and error bars for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.lineplot(x=intensities, y=responses, ci=68,\n",
    "             err_style=\"bars\", marker=\"o\", lw=0)\n",
    "ax.set_xscale(\"log\", basex=4)\n",
    "ax.set(\n",
    "    xlabel=\"Intensity\",\n",
    "    ylabel=\"P(correct)\"\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold\n",
    "\n",
    "The goal is to measure the minimum stimulus intensity that is 'reliably'\n",
    "detected. But what does this mean? We could mean a stimulus intensity\n",
    "that corresponds to above-chance performance.  More commonly, we choose\n",
    "fix performance level, say 80% correct and estimate the intensity that\n",
    "produces this level of performance. This is important.  Beware of people\n",
    "who get excited about the ability to reliably 'see' stimuli below\n",
    "'threshold'!\n",
    "\n",
    "With a 3-down 1-up staircase in the limit, the intensity level should\n",
    "gravitate toward a value that has an equal probability of getting easier\n",
    "and getting harder. Thus, the probability of getting 3 in a row correct\n",
    "is equal to 1/2.  This probability is the cube root of 1/2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 / 2) ** (1 / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a 3-down 1-up staircase method will naturally adjust the intensity\n",
    "level to show trials that lead to about 80% correct.  Cool?  I think so.\n",
    "\n",
    "How do we estimate the stimulus intensity that produces 80% correct\n",
    "performance? Looking at the graph, if we interpolate, we can guess that\n",
    "it's about 0.08.  But linear interpolation is a bad idea.  First, it\n",
    "throws out information from all data except for two intensity values, and\n",
    "second it requires the measured psychometric function data to be\n",
    "monotonic.\n",
    "\n",
    "Instead, we'll get to the heart of this tutorial and fit a smooth curve\n",
    "to the psychometric function data and use this best-fitting curve to pick\n",
    "off the threshold.\n",
    "\n",
    "### A first guess at the parameters\n",
    "\n",
    "Let's choose some initial parameters for a slope and threshold. We can\n",
    "choose the values that we used to generate our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_guess = dict(b=1, t=.25)\n",
    "y_guess = weibull(x, **p_guess)\n",
    "ax_pmf.plot(x, y_guess, c=\".2\")\n",
    "display(ax_pmf.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good fit? To answer this we need a measure of how well the\n",
    "curve fits the data. A common measure for curve fitting is a\n",
    "sums-of-squared error (SSE), which is the sum of the squared deviations\n",
    "between the curves and the data points.  However, for proportion-correct\n",
    "data like this, SSE is not appropriate because deviations along the\n",
    "proportion-correct do not have equal weights.  A 10% deviation for\n",
    "performance around 50% is less meaningful than a 10% deviation around\n",
    "90%.  You can see this by looking at the error bars.  They're always\n",
    "small near 100%.\n",
    "\n",
    "## Likelihood\n",
    "\n",
    "For proportion-correct data (or any data generated through a binary\n",
    "process), the appropriate measure is 'likelihood'. Here's how it's\n",
    "calculated;\n",
    "\n",
    "For a given trial, $i$, at a given stimulus intensity, $x_i$, the Weibull\n",
    "function predicts the probability that the subject will get the answer\n",
    "correct. So if the subject did get respond correctly, then the\n",
    "probability of that happening is:\n",
    "\n",
    "$$p_i = W(x_i)$$\n",
    "\n",
    "Where $x_i$ is the intensity of the stimulus at trial $i$, and $W(x_i)$ is the\n",
    "Weibull function evaluated at that stimulus intensity.  The probability\n",
    "of an incorrect answer is, of course:\n",
    "\n",
    "$$q_i = 1-W(x_i)$$\n",
    "\n",
    "Assuming that all trials in the staircase are independent, then for a\n",
    "given Weibull function, the probability of observing the entire sequence\n",
    "of subject responses is:\n",
    "\n",
    "$$\\prod p_i = \\prod W(x_i)$$\n",
    "\n",
    "for correct trials, and\n",
    "\n",
    "$$\\prod q_i = \\prod 1 - W(x_i)$$\n",
    "\n",
    "for incorrect trials.\n",
    "\n",
    "Here's how to do it in Python with our data set. First we evaluate the\n",
    "Weibull function for the stimulus intensities used in the staircase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = weibull(intensities, **p_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the likelihood of observing our data set given this\n",
    "particular choice of parameters for the Weibull:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = np.prod(np.where(responses, y, 1 - y))\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to choose values of p.t and p.b to make this number as large as\n",
    "possible. This is a really small number (in fact, someimtes it only gets displayed as 0).\n",
    "This is sort of surprising since we thought that our choice of parameters for the\n",
    "Weibull was reasonably good. The reason for this small number is that the product\n",
    "of a bunch of numbers less than 1 gets really small. So small that it gets into the\n",
    "tolerance for numpy to represent small numbers. To avoid this, we can take the\n",
    "logarithm of the equation above - this expands the range of small numbers so tha\n",
    "we're not going to run into machine tolerance problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.errstate(all=\"ignore\"):\n",
    "    log_likelihood = np.sum(np.log(np.where(responses, y, 1 - y)))\n",
    "log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this `nan` (or sometimes `inf`)? Turns out that this calculation can fail if the values of y reach 1\n",
    "because the log of `1 - y` can't be computed.  The way around this is to pull\n",
    "the values of y away from zero and 1 like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "y = np.clip(y, eps, 1 - eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a 'correction for guessing' and it deals with the fact\n",
    "that subjects will never truly perform at 100% for any stimulus intensity\n",
    "because human subjects will always make non-perceptual errors, like motor\n",
    "errors, or have lapses in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = np.sum(np.log(np.where(responses, y, 1 - y)))\n",
    "log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more reasonable magnitude to deal with. It's negative because\n",
    "the log of a number less than 1 is negative. Larger numbers (less\n",
    "negative) still correspond to 'good' fits.\n",
    "\n",
    "Let's calculate the log likelihood for a different set of Weibull\n",
    "parameters.\n",
    "\n",
    "Here's a function that does a very similar calculation as above, using a vector of the parameters, but reverses the sign to that better fits are associated with smaller positive numbers. We do this because we will use optimization search algorithms that minimize the result of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_cost(p, intensities, responses, eps=1e-5):\n",
    "    b, t = p\n",
    "    y = np.clip(weibull(intensities, b, t), eps, 1 - eps)\n",
    "    return -1 * np.sum(np.log(np.where(responses, y, 1 - y)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the same parameters as before, we should get exactly the negative of the previous calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_cost([p_guess[\"b\"], p_guess[\"t\"]], intensities, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_better = dict(b=1, t=1)\n",
    "weibull_cost([p_better[\"b\"], p_better[\"t\"]], intensities, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this by plotting this new prediction in red on the old graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_pmf.plot(x, weibull(x, 1, 1), c=\"r\")\n",
    "display(ax_pmf.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The log-likelihood surface\n",
    "\n",
    "How do we find the parameters that maximize the log likelihood?  `scipy`\n",
    "has a function that does this in a sophisticated way.  But for fun, let's\n",
    "just look at what the log likelihood is for a range of parameter values.\n",
    "Since there are two parameters for the Weibull function, we can think of\n",
    "the log likelihood as being a 'surface' with `b` and `y` being the x and\n",
    "y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = np.linspace(.05, 2, 201)\n",
    "ts = np.logspace(-3, 3, 202, base=4)\n",
    "\n",
    "bb, tt = np.meshgrid(bs, ts, indexing=\"ij\")\n",
    "pp = bb.ravel(), tt.ravel()\n",
    "ll = weibull_cost(pp, intensities[:, np.newaxis], responses[:, np.newaxis])\n",
    "loglike_surf= ll.reshape(bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax_ll = plt.subplots(figsize=(6, 5))\n",
    "ax_ll.set_xscale(\"log\", basex=4)\n",
    "ax_ll.set(xlabel=\"t\", ylabel=\"b\")\n",
    "cset = ax_ll.contour(tt, bb, loglike_surf, 50)\n",
    "f.colorbar(cset)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the most recent set of parameters as a symbol on this contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_ll.scatter(\n",
    "    [p_guess[\"t\"], p_better[\"t\"]],\n",
    "    [p_guess[\"b\"], p_better[\"b\"]],\n",
    "    c=[\".2\", \"r\"],\n",
    "     )\n",
    "ax_ll.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure is like a topographical map. Each line represents the values\n",
    "of b and t that produce an equal value for log likelihood.\n",
    "\n",
    "The figure shows a series of concentric shapes with the smallest circling\n",
    "around `t=0.12` and `b = 1.6`.  This is the 'bottom' of our surface and is\n",
    "close to the best choice of Weibull parameters.\n",
    "\n",
    "## scipy's 'minimize' routine\n",
    "\n",
    "Searching through the entire grid of possible parameters is clearly an\n",
    "inefficient strategy (especially if there are even more parameters to\n",
    "deal with).  Fortunately there is a whole science behind finding the best\n",
    "parameters to minimize a function, and `scipy` has incorporated some of\n",
    "the best in their function called 'minimize'.\n",
    "\n",
    "TODO: The original tutorial includes a wrapper function that gives a slightly\n",
    "nicer interface to the optimization. This would not be too hard to mirror,\n",
    "but I'm punting for now.\n",
    "\n",
    "To use the `minimize` function, we need to provide an objective function\n",
    "(a function that takes parameters and returns the score we want to minimize),\n",
    "an initial guess for the parameters (`x0`), and other arguments that should\n",
    "be passed to the objective function each time it's called (`args`). The\n",
    "`minimize` function is actually an interface to a large number of optimizers,\n",
    "so we also need to pick the one we need to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(weibull_cost, x0=[1, .25], args=(intensities, responses), method=\"nelder-mead\")\n",
    "p_best = dict(zip([\"b\", \"t\"], res.x))\n",
    "log_likelihood_best = res.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: b={b:.2f}, t={t:.2f}\".format(**p_best))\n",
    "print(f\"Best score {log_likelihood_best:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameters changed from the initial values, and the\n",
    "resulting log likelihood is better than the two used earlier (using\n",
    "`p_guess` and `p_better`.  Let's plot the best-fitting parameters on the\n",
    "contour plot in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_ll.scatter(p_best[\"t\"], p_best[\"b\"], c=\"b\")\n",
    "display(ax_ll.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this best-fitting pair of parameters falls in the middle of the\n",
    "circular-shaped contour.  This is the lowest point on the surface.\n",
    "\n",
    "We now have the best fitting parameters.  Let's draw the best predictions\n",
    "in blue on top of the psychometric function data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_pmf.plot(x, weibull(x, **p_best), c=\"C0\")\n",
    "display(ax_pmf.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = (1 / 2) ** (1 / 3)\n",
    "thresh_x = [ax_pmf.get_xlim()[0], p_best[\"t\"], p_best[\"t\"]]\n",
    "thresh_y = [t_c, t_c, ax_pmf.get_ylim()[0]]\n",
    "ax_pmf.plot(thresh_x, thresh_y, ls=\":\", c=\".6\", scalex=False, scaley=False)\n",
    "display(ax_pmf.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fixed and free parameters\n",
    "\n",
    "The MATLAB tutorial shows how to hold one of the parameters fixed using the custom interface.\n",
    "Doing so without the interface is kind of a drag, so I'm going to skip. I'll revisit if\n",
    "I decide to implement a similar interface function in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increment thresholds\n",
    "\n",
    "So far we've only talked about detecting a signal against noise.  It's\n",
    "easy to use SDT to predict the ability to discriminate two stimuli that\n",
    "differ in strength.  An example of this is the increment threshold, which\n",
    "is the smallest increment in response that leads to some criterion\n",
    "performance.\n",
    "\n",
    "We can calculate the increment threshold function from the nonlinear\n",
    "transducer function using our SDT model.  Recall that in a 2AFC task, the\n",
    "threshold is the stimulus intensity (or increment) that produces a\n",
    "criterion level of performance.  In our definition, the level of\n",
    "performance is $(1/2)^(1/3) \\approx .7937$ proportion correct.  Ths corresponds\n",
    "to a d-prime value of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = .3\n",
    "d_prime = sqrt(2) * norm.ppf((1 / 2) ** (1 / 3))\n",
    "print(f\"d' = {d_prime:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following baseline intensity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_intensities = np.arange(11, dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate the increments in stimulus intensity that produce a\n",
    "d-prime increment in internal response.  We'll find that increment using\n",
    "a binary search algorithm in which we refine our estimates within\n",
    "narrowing brackets of high and low estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting intensities that are too low\n",
    "lo = np.zeros_like(base_intensities)\n",
    "\n",
    "# Starting intensities that are too high\n",
    "hi = np.full_like(base_intensities, 40)\n",
    "\n",
    "base_response = base_intensities ** p\n",
    "\n",
    "n_iter = 20\n",
    "for _ in range(n_iter):\n",
    "    mid = (hi + lo) / 2\n",
    "    increment_response = (base_intensities + mid) ** p - base_response\n",
    "    too_high = increment_response > d_prime\n",
    "    hi[too_high] = mid[too_high]\n",
    "    lo[~too_high] = mid[~too_high]\n",
    "\n",
    "pred_increment_thresh = (hi + lo) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax_r, ax_t) = plt.subplots(1, 2, figsize=(7, 5))\n",
    "ax_r.plot(base_intensities, base_response, \"o-\")\n",
    "ax_t.plot(base_intensities, pred_increment_thresh, \"o-\")\n",
    "ax_r.set(\n",
    "    title=\"Nonlinear transducer function\",\n",
    "    xlabel=\"Baseline intensity\",\n",
    "    ylabel=\"Internal response\",\n",
    "    ylim=(0, 5),\n",
    ")\n",
    "ax_t.set(\n",
    "    title=\"Increment response function\",\n",
    "    xlabel=\"Baseline intensity\",\n",
    "    ylabel=\"Increment threshold\",\n",
    "    ylim=(0, 40),\n",
    ")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Weber's law!  Increment thresholds increase with baseline\n",
    "contrast because of the compressive transducer function (power function)\n",
    "between stimulus intensity and internal response.  As the baseline\n",
    "intensity increases, the slope of the transducer function gets shallower\n",
    "so you need a larger and larger stimulus increment to get the signal mean\n",
    "to increase enough to obtain the d-prime needed for the threshold.\n",
    "\n",
    "Mess with the exponent `p` to see the relationship between the two\n",
    "curves.\n",
    "\n",
    "Let's simulate the increment thresholds for the same range of baseline\n",
    "stimulus strengths with our power-function model.  This will involve\n",
    "repeated 3-down 1-up staircases for varying baseline intensities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_levels = .0156, .0625, .25, 1, 4, 16, 32, 64, 128\n",
    "increment_thresholds = np.zeros_like(base_intensities)\n",
    "n_trials = 1000\n",
    "sd = 1\n",
    "\n",
    "# Control the randomness of the simulation\n",
    "random_state = np.random.RandomState(seed=100)\n",
    "\n",
    "for j, base in enumerate(base_intensities):\n",
    "\n",
    "    noise_mean = base ** p\n",
    "\n",
    "    # Set up the staircase for this baseline\n",
    "    idx = len(intensity_levels) - 1\n",
    "    correct_in_a_row = 0\n",
    "    intensities = np.zeros(n_trials)\n",
    "    responses = np.zeros(n_trials)\n",
    "\n",
    "    for i in range(n_trials):\n",
    "\n",
    "        intensity = intensities[i] = intensity_levels[idx]\n",
    "\n",
    "        signal_mean = (base + intensity) ** p\n",
    "        signal = norm.rvs(signal_mean, sd, random_state=random_state)\n",
    "        noise = norm.rvs(noise_mean, sd, random_state=random_state)\n",
    "\n",
    "        response = responses[i] = signal > noise\n",
    "\n",
    "        if response:\n",
    "            correct_in_a_row += 1\n",
    "            if correct_in_a_row == 3:\n",
    "                idx = max(0, idx - 1)\n",
    "                correct_in_a_row = 0\n",
    "        else:\n",
    "            correct_in_a_row = 0\n",
    "            idx = min(idx + 1, len(intensity_levels) - 1)        \n",
    "\n",
    "    res = minimize(weibull_cost, x0=[1, .25], args=(intensities, responses), method=\"nelder-mead\")\n",
    "    increment_thresholds[j] = res.x[1]\n",
    "\n",
    "ax_t.plot(base_intensities, increment_thresholds, \"o\",\n",
    "          ms=10, mew=2, mfc=\"none\")\n",
    "display(ax_t.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be a reasonable match between our simulated data and the\n",
    "expected increment response function.  The fit should improve with\n",
    "increasing number of trials per staircase.\n",
    "\n",
    "What good is this? The goal of the psychophysicist is to take behavioral\n",
    "results (psycho) to learn something about the transformation between the\n",
    "physical stimulus (physics) and the internal response.  This simulation\n",
    "shows how you can predict in increment response function for a given\n",
    "nonlinear transducer function.  But we can go the other way too.  For a\n",
    "given psychophysical increment response function we can find the best\n",
    "nonlinear transducer function that predicts our data.  \n",
    "\n",
    "It turns out that for contrast increments, the psychophysical contrast\n",
    "increment threshold function (sometimes called threshold vs. contrast or\n",
    "TvC function) is surprisingly consistent with contrast response functions\n",
    "measured in V1 with fMRI.  Moreover, the effects of surround stimuli and\n",
    "attention affect psychophysical and fMRI responses in  a consistent way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Find your own stimulus intensity threshold by fitting the Weibull\n",
    "function to your own psychophysical data generated in Lesson 4.\n",
    "\n",
    "2. Run 50 trials with a stimulus intensity level set at your own\n",
    "threshold and see how close the proportion correct is to 80%\n",
    "\n",
    "3. What happens when you use different initial parameters when calling\n",
    "`minimize`?  Try some wild and crazy starting values.  You might find some\n",
    "interesting 'fits'.  This illustrates the need to choose sensible initial\n",
    "values.  If your function surface is convoluted then a bad set of initial\n",
    "parameters may settle into a 'local minimum'.\n",
    "\n",
    "4. How would you obtain an estimate of the reliability of the threshold\n",
    "measure?  After all, $t$ is just a statistic based on the values in\n",
    "'results'.\n",
    "\n",
    "5. Run the increment threshold simulation for an exponent of 1.  What\n",
    "happens and why?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
