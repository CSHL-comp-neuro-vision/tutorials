{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psychophysics Tutorial I: Signal Detection Theory and 2AFC\n",
    "\n",
    "This is the first Psychophysics tutorial, covering Signal Detection\n",
    "Theory, ROC curves and the 2AFC paradigm.  See also the sdtTutorial which\n",
    "covers some of the same material.\n",
    "\n",
    "Written by G.M. Boynton for CSHL 2012\n",
    "\n",
    "Python translation M.L. Waskom at CSHL 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Signal Detection Theory\n",
    "\n",
    "Suppose you want to determine if a subject can reliably detect a weak\n",
    "stimulus.  The simplest experiment would be to present this stimulus over\n",
    "multiple trials and ask if the subject saw it.  But this won't work\n",
    "because, for example, the subject could simply say 'yes'on each trial.\n",
    "To alleviate this catch trials can be included to keep the subject from\n",
    "cheating.\n",
    "\n",
    "Now suppose you introduce catch trials (no stimulus trials) randomly on\n",
    "half of the trials.  The subject's task is to determine if the signal was\n",
    "present on any given trial.  Stimulus present trials are called 'signal'\n",
    "trials, and stimulus absent trials are called 'noise' trials. A subject\n",
    "that guesses, or says 'yes' or 'no' on every trial will be performing at\n",
    "50%, or chance level.  No more cheating.\n",
    "\n",
    "There is a range of stimulus intensities where a subject will perform\n",
    "somewhere between chance and 100% correct performance.  The presence of\n",
    "such a 'soft' threshold is most commonly explained in terms of Signal\n",
    "Detection Theory (SDT).\n",
    "\n",
    "SDT assumes that subjects base their decision on an internal response to\n",
    "a stimulus that varies trom trial to trial.  If this internal response\n",
    "exceeds some criterion, the subject reports to have perceived the\n",
    "stimulus.\n",
    "\n",
    "This trial-to-trial variability of the internal response could be due to\n",
    "variability in the stimulus itself (as in the case of Poisson noise for\n",
    "very dim lights), or to random neuronal noise at the sensory\n",
    "representation of the stimulus, or due to higher level variability in the\n",
    "attentional or motivational state of the subject.  \n",
    "\n",
    "Most commonly, this variability is modeled as a normal distribution\n",
    "centered around some mean. The simplest implementation has the mean\n",
    "response for the noise trials be zero and signal trials some larger\n",
    "value, with the standard deviations of the signal and noise responses the\n",
    "same.  \n",
    "\n",
    "Here are some example parameters all in a single structure \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict(noise_mean=0, signal_mean=1, sd=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a graph of the probability distribution for the internal responses to signal and noise trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-4, 6, 200)\n",
    "f, ax_z = plt.subplots()\n",
    "noise_dist = norm(p[\"noise_mean\"], p[\"sd\"])\n",
    "signal_dist = norm(p[\"signal_mean\"], p[\"sd\"])\n",
    "ax_z.plot(z, noise_dist.pdf(z), label=\"noise\")\n",
    "ax_z.plot(z, signal_dist.pdf(z), label=\"signal\")\n",
    "ax_z.set(xlabel=\"Internal response\")\n",
    "ax_z.legend(loc=\"best\")\n",
    "ax_z.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to set a criterion value for determining what internal reponses lead to 'Yes' responses.  We'll show it in the figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[\"criterion\"] = 1\n",
    "ax_z.axvline(p[\"criterion\"], ls=\"--\", color=\".6\", label=\"criterion\")\n",
    "ax_z.legend()\n",
    "display(ax_z.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On any trial, one of four things will happen. Either the signal is\n",
    "present or absent crossed with the subject reporting 'yes' or 'no'.\n",
    "Trial types are labeled this way:\n",
    "\n",
    "|         |  \"Yes\"    |  \"No\"    |\n",
    "|---------|-----------|----------|\n",
    "|Present  |    Hit    |  Miss    |\n",
    "|Absent   |  False alarm   | Correct rejection  |\n",
    "\n",
    "It's easy to see that SDT predicts the probability of each of these four\n",
    "trial types by areas under the normal curve.  The probability of a hit is\n",
    "the probability of drawing a value above the criterion, given that it\n",
    "came from the signal distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hit = signal_dist.sf(p[\"criterion\"])\n",
    "print(f\"P(Hit) = {p_hit:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fa = noise_dist.sf(p[\"criterion\"])\n",
    "print(f\"P(FA) = {p_fa:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole table looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\n",
    "f\"\"\"\n",
    "||\"Yes\"|\"No\"|\n",
    "|-|-|-|\n",
    "|Present|{p_hit:.1%}|{1-p_hit:.1%}|\n",
    "|Absent|{p_fa:.1%}|{1-p_fa:.1%}|\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since half the trials are signal trials, the overall performance will be the average of the hit and correction rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_c = .5 * p_hit + .5 * (1 - p_fa)\n",
    "print(f\"P(Correct) = {p_c:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Play around with the parameters. See how:\n",
    "\n",
    "1. If you shift your criterion very low or high performance will be at chance.\n",
    "2. Performance is maximized when it's halfway between the signal and noise means.  This is the criterion an 'ideal observer' should choose. \n",
    "3. Performance increases as either the standard deviations decrease or the difference between signal and noise mean increases.\n",
    "4. You can offset an increase between signal and noise means by increasing the standard deviation by the same amount. The model is over-parameterized.\n",
    "\n",
    "---\n",
    "\n",
    "## Estimating d-prime from Hits and False Alarms\n",
    "\n",
    "You should see that simply reporting percent correct in a yes/no\n",
    "experiment is a problem because performance varies with criterion: you\n",
    "cannot estimate d-prime from percent correct alone.\n",
    "\n",
    "Fortunately we can estimate d-prime by finding the difference in the\n",
    "corresponding z-values from the hit and false alarm rates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hit = norm.ppf(p_hit)\n",
    "z_fa = norm.ppf(p_fa)\n",
    "dprime_est = z_hit - z_fa\n",
    "print(f\"Z_hit: {z_hit:.1f} | Z_fa: {z_fa:.1f} | d': {dprime_est:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 'criterion' free estimate of d-prime, and is what is often reported instead of percent correct for a Yes/No experiment.\n",
    "\n",
    "Use this interactive widget to play with the parameters.  See how $d^\\prime$ stays constant for different criterion values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def dprime_tutorial(signal_mean=(0, 2, .1), sd=(0, 2, .1), criterion=(0, 2, .1)):\n",
    "\n",
    "    signal_dist = norm(signal_mean, sd)    \n",
    "    noise_dist = norm(0, sd)\n",
    "\n",
    "    p_hit = signal_dist.sf(criterion)\n",
    "    p_fa = noise_dist.sf(criterion)\n",
    "    p_c = .5 * p_hit + .5 * (1 - p_fa)\n",
    "    z_hit = norm.ppf(p_hit)\n",
    "    z_fa = norm.ppf(p_fa)\n",
    "    dprime_est = z_hit - z_fa\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    z = np.linspace(-4, 6, 200)\n",
    "    ax.plot(z, noise_dist.pdf(z), label=\"noise\")\n",
    "    ax.plot(z, signal_dist.pdf(z), label=\"signal\")\n",
    "    ax.axvline(criterion, ls=\"--\", color=\".6\", label=\"criterion\")\n",
    "\n",
    "    text = f\"P(correct) = {p_c:.1%}\\n$d^\\prime$ = {dprime_est:.1f}\"\n",
    "    ax.text(.05, .85, text, size=12, transform=ax.transAxes)\n",
    "\n",
    "    ax.set(xlabel=\"Internal response\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The ROC curve\n",
    "\n",
    "The criterion determines the trade-off between hits and false alarms.  A\n",
    "low (liberal) criterion is sure to get a hit but will lead to lots of\n",
    "false alarms. A high (conservative) criterion will miss a lot of signals,\n",
    "but will also minimize false alarms.  This trade-off is typically\n",
    "visualized in the form of a 'Reciever Operating Characteristic' or ROC\n",
    "curve.  An ROC curve is a plot of hits against false alarms for a range\n",
    "of criterion values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hits = norm.sf(z, p[\"signal_mean\"], p[\"sd\"])\n",
    "p_fas = norm.sf(z, p[\"noise_mean\"], p[\"sd\"])\n",
    "f, ax_roc = plt.subplots()\n",
    "ax_roc.plot([0, 1], [0, 1], c=\".5\", ls=\":\")\n",
    "ax_roc.plot(p_fas, p_hits)\n",
    "ax_roc.set(xlim=(0, 1), ylim=(0, 1), aspect=\"equal\",\n",
    "       xlabel=\"p(FA)\", ylabel=\"p(Hit)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot out example hit rate against our example FA rate too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_roc.scatter(p_fa, p_hit)\n",
    "display(ax_roc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around again.  See how:\n",
    "\n",
    "1. The point in the ROC curve moves around as you vary the criterion.  \n",
    "\n",
    "2. The 'bow' of the ROC curve varies with d-prime (either by increasing\n",
    "the signal mean or reducing the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def roc_tutorial(signal_mean=(0, 2, .1), sd=(0, 2, .1), criterion=(0, 2, .1)):\n",
    "    \n",
    "    signal_dist = norm(signal_mean, sd)\n",
    "    noise_dist = norm(0, sd)\n",
    "\n",
    "    p_hit = signal_dist.sf(criterion)\n",
    "    p_fa = noise_dist.sf(criterion)\n",
    "\n",
    "    p_c = .5 * p_hit + .5 * (1 - p_fa)\n",
    "    z_hit = norm.ppf(p_hit)\n",
    "    z_fa = norm.ppf(p_fa)\n",
    "    dprime_est = z_hit - z_fa\n",
    "\n",
    "    p_hits = signal_dist.sf(z)\n",
    "    p_fas = noise_dist.sf(z)\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot([0, 1], [0, 1], c=\".5\", ls=\":\")\n",
    "    ax.plot(p_fas, p_hits)\n",
    "    ax.scatter(p_fa, p_hit)\n",
    "    ax.set(xlim=(0, 1), ylim=(0, 1), aspect=\"equal\",\n",
    "           xlabel=\"p(FA)\", ylabel=\"p(Hit)\",)\n",
    "\n",
    "    text = f\"P(correct) = {p_c:.1%}\\n$d^\\prime$ = {dprime_est:.1f}\"\n",
    "    ax.text(.45, .05, text, size=12, transform=ax.transAxes)\n",
    "    ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area under the ROC curve\n",
    "\n",
    "You hopefully saw that increasing d-prime increases the bow of the ROC\n",
    "curve away from the diagonal.  A measure of this bowing is the area under\n",
    "the ROC curve.  This can be estimated by numerically integrating the\n",
    "sampled curve.  We'll use scipy's `trapz` function. (The negative sign\n",
    "is to undo the fact that the ROC curve traces from left-to-right for\n",
    "increasing criterion values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import trapz\n",
    "roc_area = -trapz(p_hits, p_fas)\n",
    "print(f\"Area under ROC curve: {roc_area:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see later that this area has a special meaning - it's the percent\n",
    "correct that is expected in a two-alternative forced choice (2AFC)\n",
    "experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The relationship between d-prime and the area under the ROC curve\n",
    "\n",
    "d-prime can be calculated from the area under the ROC curve by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprime_from_area = np.sqrt(2) * norm.ppf(roc_area)\n",
    "print(f\"d' from area under ROC curve: {dprime_from_area:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculus behind this is interesting but we'll pass on it.\n",
    "\n",
    "## Simulating a Yes/No experiment\n",
    "\n",
    "Next we'll use SDT to simulate a subject's response to a series of trials\n",
    "in a Yes/No experiment and estimate the d-prime value that was used in\n",
    "the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "signal = np.random.rand(n_trials) > .5\n",
    "\n",
    "# Generate the internal response for each trial:\n",
    "x = np.where(signal, signal_dist.rvs(n_trials), noise_dist.rvs(n_trials))\n",
    "\n",
    "# Simulate responses and behavioral metrics\n",
    "response = x > p[\"criterion\"]\n",
    "p_hit_sim = response[signal].mean()\n",
    "p_fa_sim = response[~signal].mean()\n",
    "\n",
    "display(Markdown(\n",
    "f\"\"\"\n",
    "||\"Yes\"|\"No\"|\n",
    "|-|-|-|\n",
    "|Present|{p_hit_sim:.1%}|{1-p_hit_sim:.1%}|\n",
    "|Absent|{p_fa_sim:.1%}|{1-p_fa_sim:.1%}|\n",
    "\"\"\"))\n",
    "\n",
    "# Plot it on the ROC curve from above:\n",
    "ax_roc.scatter(p_fa_sim, p_hit_sim, c=\"C1\")\n",
    "display(ax_roc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate d prime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hit_sim = norm.ppf(p_hit_sim)\n",
    "z_fa_sim = norm.ppf(p_fa_sim)\n",
    "dprime_sim = z_hit_sim - z_fa_sim\n",
    "print(f\"d' from simulation: {dprime_sim:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Compare the simulated values to the expected values from the STD model.\n",
    "You can run this section over and over to look at the variability of the\n",
    "d-prime estimate. You can see that:\n",
    "\n",
    "1) The estimates of d-prime become more accurate with increasing number\n",
    "of trials\n",
    "\n",
    "2) The estimates of d-prime become less accurate with criterion values\n",
    "that deviate from the ideal value.  This is important. we typically don't\n",
    "have control over the criterion.  If a lame subject says 'yes' or 'no'\n",
    "almost all the time then there is very litle information for estimating\n",
    "d-prime.\n",
    "\n",
    "3) If you're motivated, add a loop to simulate a bunch of simulations to\n",
    "estimate the variability in the estimate for a range of model parameters.\n",
    "\n",
    "Simulations like this illustrate an often neglected fact:  A 'perfect'\n",
    "subject that makes decisions according to Signal Detection Theory will\n",
    "still have variability in performance from experimental run to\n",
    "experimental run.  That is, ideal observers will still generate data\n",
    "with finite-sized error bars.  Simulations can give you a feel for how\n",
    "small the erorr bars should be under ideal conditions.\n",
    "\n",
    "----\n",
    "\n",
    "## Yes/No with rating scales\n",
    "\n",
    "One way to get around the criterion problem is to allow subjects more\n",
    "options in their response.  For example, rather than having two buttons,\n",
    "let them have four to indicate their confidence that a signal was\n",
    "present:\n",
    "\n",
    "1. Definately no\n",
    "2. Probably no\n",
    "3. Probably yes\n",
    "4. Definately yes\n",
    "\n",
    "This effectively allows the subject to have more than one criterion.  To\n",
    "model this with SDT, three criterion values will divide the internal\n",
    "response range into the four response categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[\"criterion\"] = [-.5, .5, 1.5]\n",
    "for c in p[\"criterion\"]:\n",
    "    ax_z.axvline(c, ls=\":\", c=\".5\")\n",
    "display(ax_z.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating scales and the ROC curve\n",
    "\n",
    "We can visualize these criterion values on the ROC curve like we did for\n",
    "the single criterion earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hit = signal_dist.sf(p[\"criterion\"])\n",
    "p_fa = noise_dist.sf(p[\"criterion\"])\n",
    "\n",
    "# Clean up the ROC plot\n",
    "plt.setp(ax_roc.collections, visible=False)\n",
    "\n",
    "# Plot the new expected points\n",
    "ax_roc.scatter(p_fa, p_hit, c=\"C0\")\n",
    "display(ax_roc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, `p_hit` for the lowest point on the ROC curve corresponds to\n",
    "the probability that subject will report a 2, 3 or 4 on a signal trial.\n",
    "The next one up is for 3 or 4, and the highest is the probability of\n",
    "a hit if a subject just reponds 4.\n",
    "\n",
    "## Simulating a Yes/No experiment with rating scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "signal = np.random.rand(n_trials) > .5\n",
    "x = np.where(signal, signal_dist.rvs(n_trials), noise_dist.rvs(n_trials))\n",
    "response = np.digitize(x, p[\"criterion\"])\n",
    "\n",
    "crit_idxs =  np.unique(response)\n",
    "p_hit = [(response > i)[signal].mean() for i in crit_idxs]\n",
    "p_fa = [(response > i)[~signal].mean() for i in crit_idxs]\n",
    "\n",
    "ax_roc.scatter(p_fa, p_hit, c=\"C1\")\n",
    "display(ax_roc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this section over and over to get an idea of the variability of the simulation with respect to the expected values on the ROC curve.\n",
    "\n",
    "---\n",
    "\n",
    "## Maximum likelihood fit of ROC curve\n",
    "\n",
    "You probably have the (correct) intuition that the rating scale\n",
    "information adds reliability to the estimate of d-prime. But now we have\n",
    "the problem of translating these three points on the ROC curve to a\n",
    "single estimate of d-prime.  \n",
    "\n",
    "There are a variety of ways of doing this.  One would be to use the\n",
    "cumulative normals like we did before for each of the three points on the\n",
    "ROC curve and average them.  But this doesn't seem right - different\n",
    "points should have different weights due to variability in their\n",
    "reliablity.  \n",
    "\n",
    "We'll implement a model fitting method.  You should appreciate that every\n",
    "point in the 2D ROC space corresponds to a unique pair of d-prime and\n",
    "criterion values.  That's why there is a direct translation between a\n",
    "single point on the ROC curve and d-prime.  But now we have three points\n",
    "in ROC space that don't necessarily fall on the same ROC curve.  Our goal\n",
    "is to find the single ROC curve that passes closest to the three points.\n",
    "Specifically, we need a four-parmeter fit: what d-prime and three\n",
    "criterion values best fits our observed values?\n",
    "\n",
    "To do this we need a cost function that takes in a set of model\n",
    "parameters and data points and returns a value that represents goodness\n",
    "of fit. Then we'll use scipy's optimization routine to find the model\n",
    "parameters that minmizes this cost function.\n",
    "\n",
    "When dealing with proportions, the cost function is always in terms of\n",
    "likelihood (You should never use a least-squares criterion for\n",
    "proportional data!) Our cost function will be the probability of our\n",
    "observed data for a given set of model parameters.\n",
    "\n",
    "Suppose the first trial was a noise trial and the subject reported '1'\n",
    "(Definately no).  Looking at the ROC figure, the probability of this happening\n",
    "is the area under the blue curve to the left of the 1st criterion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_noise_resp = noise_dist.cdf(p[\"criterion\"][0])\n",
    "print(f\"P(respond '1' | noise) = {p_noise_resp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of responding '2' is the area between the 1st and second\n",
    "criteria and so on.  The whole table of probabilities can be computed\n",
    "like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.r_[-np.inf, p[\"criterion\"], np.inf]\n",
    "noise_ps = np.diff(noise_dist.cdf(bounds))\n",
    "signal_ps = np.diff(signal_dist.cdf(bounds))\n",
    "ps = np.vstack([noise_ps, signal_ps]).T\n",
    "print(ps.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is for noise trials, the second is for signal trials,\n",
    "and each row corresponds to each response ('1'-'4').  Verify that the sum\n",
    "of the columns add up to 1.\n",
    "\n",
    "The probability of obtaining our observed data set based on these\n",
    "probabilities is the product of the probabilities associated with each\n",
    "trial.  Multiplying 100 values that are less than 1 produces a\n",
    "ludicrously small number, so we almost always maximize log likelihood\n",
    "instead.  Acutally, we'll use the negative of the log likelihood because\n",
    "optimization routines minmize functions. The negative of the log\n",
    "likelihood of our observed data can be computed by turning the computations\n",
    "above into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_cost_func(p, signal, response):\n",
    "\n",
    "    bounds = np.r_[-np.inf, p[\"criterion\"], np.inf]\n",
    "    noise_ps = np.diff(norm.cdf(bounds, p[\"noise_mean\"], p[\"sd\"]))\n",
    "    signal_ps = np.diff(norm.cdf(bounds, p[\"signal_mean\"], p[\"sd\"]))\n",
    "    lls = np.log(np.where(signal, signal_ps[response], noise_ps[response]))\n",
    "    cost = -lls.sum()\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = roc_cost_func(p, signal, response)\n",
    "print(f\"Cost (negative log-likelihood) = {cost:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the parameters that minimize our cost function we'll use scipy's\n",
    "`minimize` function with `method=\"nelder-mead\"`, which will use the same\n",
    "algorithm as matlab's `fminsearch`.\n",
    "\n",
    "TODO The original matlab tutorial used a custom interface that translated\n",
    "from a structure with named fields to a 1d vector of parameters that both\n",
    "`fminsearch` and `minimize` work with. It also let the user specify which\n",
    "parameters should be held fixed and which should be optimized. I have a\n",
    "similar Python class that could be bundled with the tutorials (and others\n",
    "have implemented similar things that live in libraries you can get from `pip`).\n",
    "But to keep things simple let's just define a slightly less flexible but similar\n",
    "interface function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, fmin\n",
    "def fit(cost_func, p, signal, response):\n",
    "\n",
    "    p_fit = p.copy()\n",
    "    \n",
    "    def cost_func_wrapped(xvec):\n",
    "        signal_mean, *criterion = xvec\n",
    "        p_fit[\"signal_mean\"] = signal_mean\n",
    "        p_fit[\"criterion\"] = criterion\n",
    "        return cost_func(p_fit, signal, response)\n",
    "\n",
    "    x0 = np.r_[p[\"signal_mean\"], p[\"criterion\"]]\n",
    "    res = minimize(cost_func_wrapped, x0, method=\"nelder-mead\")\n",
    "    signal_mean, *criterion = res.x\n",
    "    p_fit[\"signal_mean\"] = signal_mean\n",
    "    p_fit[\"criterion\"] = criterion\n",
    "    \n",
    "    return p_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fit = fit(roc_cost_func, p, signal, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the best-fitting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the resemble the original model parameters?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add resuts of the fitting to the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the full estimated ROC curve\n",
    "p_hits_fit = norm.sf(z, p_fit[\"signal_mean\"], p_fit[\"sd\"])\n",
    "p_fas_fit = norm.sf(z, p_fit[\"noise_mean\"], p_fit[\"sd\"])\n",
    "ax_roc.plot(p_fas_fit, p_hits_fit, c=\"C2\")\n",
    "\n",
    "# Second, the three ROC points\n",
    "p_hit_best = norm.sf(p_fit[\"criterion\"], p_fit[\"signal_mean\"], p_fit[\"sd\"])\n",
    "p_fa_best = norm.sf(p_fit[\"criterion\"], p_fit[\"noise_mean\"], p_fit[\"sd\"])\n",
    "ax_roc.scatter(p_fa_best, p_hit_best, c=\"C2\")\n",
    "\n",
    "display(ax_roc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimate of d-prime is then simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_prime_est = (p_fit[\"signal_mean\"] - p_fit[\"noise_mean\"]) / p_fit[\"sd\"]\n",
    "print(f\"d' estimated from fitting: {d_prime_est:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this over and over to get a feel for the variability of the estimate\n",
    "of d-prime.  Does the rating scale provide a more reliable estimate of\n",
    "d-prime than the standard Yes/No experiment?  You can test this more\n",
    "formally by repeating a bunch of simulations.\n",
    "\n",
    "----\n",
    "\n",
    "## Two-alternative forced choice (2AFC)\n",
    "\n",
    "Another way to avoid the criterion problem is to use a\n",
    "two-alternative-forced-choice paradigm (2AFC) where a trial consists of\n",
    "both a signal and noise draw in either random temporal order or spatial\n",
    "position.  The subject must choose which draw contains the signal.  2AFC\n",
    "is easilly modelled with SDT by drawing once from the signal, and once\n",
    "from the noise distribution.  The subject decides that the signal came\n",
    "from the draw with the larger value.  \n",
    "\n",
    "\n",
    "Here's a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "signal = signal_dist.rvs(n_trials)\n",
    "noise = noise_dist.rvs(n_trials)\n",
    "response = signal > noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response is 1 for correct trials, when the draw from the signal exceeds the noise draw.\n",
    "\n",
    "Overall performance is the mean of the response vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_2afc = response.mean()\n",
    "print(f\"P(Correct) = {pc_2afc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that percent correct is greater than the best percent correct in the\n",
    "yes/no experiment.  \n",
    "\n",
    "Here's something interesting: the expected percent correct in 2AFC should\n",
    "be equal to the area under the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `pc_2afc` is from simulation, so you should run this section several \n",
    "times over to convince yourself that this is true. This means that d-prime\n",
    "can be directly estimated from percent correct for a 2AFC experiment\n",
    "(because d-prime is directly related to area under the ROC curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprime_2afc = np.sqrt(2) * norm.ppf(pc_2afc)\n",
    "print(f\"d' = {dprime_2afc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How reliable is this estimate of d-prime?  An interesting exercise would\n",
    "be to measure the standard deviation of the d-prime estimates for\n",
    "repeated simulations of the standard Yes/No, the Yes/No rating method,\n",
    "and the 2AFC method.  Which wins?\n",
    "\n",
    "----\n",
    "\n",
    "## N-alternative forced choice \n",
    "\n",
    "Forced choice experiments can include more than two options.  For\n",
    "example, suppose a target could appear randomly in one of four spatial\n",
    "positions.  This can be modeled with SDT as taking three samples from the\n",
    "noise distribution and one from the signal distribution.  Like for 2AFC,\n",
    "the subject chooses the location with that generated the largest\n",
    "response. A correct response occurs when the draw from the signal\n",
    "distribution exceeds the maximum of the noise draws.  This is easy to\n",
    "simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alt = 4\n",
    "\n",
    "signal = signal_dist.rvs(n_trials)\n",
    "noise = noise_dist.rvs((n_alt - 1, n_trials))\n",
    "\n",
    "response = signal > noise.max(axis=0)\n",
    "pc_nafc_sim = response.mean()\n",
    "\n",
    "print(f\"P(Correct) = {pc_nafc_sim:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a solution (using numerical integration) for percent correct in an NAFC experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_prime = (p[\"signal_mean\"] - p[\"noise_mean\"]) / p[\"sd\"]\n",
    "z = np.linspace(-5, 5, 501)\n",
    "dz = z[1] - z[0]\n",
    "pc_nafc_num = (norm.pdf(z - d_prime) * norm.cdf(z) ** (n_alt - 1)).sum() * dz\n",
    "\n",
    "f, ax_nafc = plt.subplots(figsize=(3, 4))\n",
    "ax_nafc.bar([\"Simulated\", \"Numerical\"], [pc_nafc_sim, pc_nafc_num])\n",
    "ax_nafc.set(ylabel=\"P(Correct)\")\n",
    "ax_nafc.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divided Attention\n",
    "\n",
    "Let's simulate a real divided attention experiment. Again there are four\n",
    "spatial positions, but this time it's a 2AFC experiment in which a signal\n",
    "(like a grating or something) appears in one of the positions on one of\n",
    "two intervals.  The subject's job is to determine the interval that\n",
    "contained the signal, wherever it was.  Now, consider two conditions, one\n",
    "in which you have no idea which of the four conditions will have the\n",
    "signal and one in which you are cued to the correct location.  It's still\n",
    "a 2AFC task, but one forces you to divide your spatial attention.  Again,\n",
    "it's easy to simulate.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 4\n",
    "\n",
    "noise = noise_dist.rvs((n_pos, n_trials))\n",
    "signal = np.vstack([noise_dist.rvs((n_pos - 1, n_trials)),\n",
    "                    signal_dist.rvs((1, n_trials))])\n",
    "\n",
    "# Decision Rule: choose the signal interval if the max of the\n",
    "# 3 noise + 1 signal draws exceeds the max of the four noise draws\n",
    "\n",
    "response = signal.max(axis=0) > noise.max(axis=0)\n",
    "p_uncued = response.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a cue to the spatial position, then we can assume that the\n",
    "subject ignores the thee uncued locations.  This is just a the same old\n",
    "2AFC experiment. Equivalently we can use the same code with nPositions\n",
    "set to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = noise_dist.rvs(n_trials)\n",
    "signal = signal_dist.rvs(n_trials)\n",
    "response = signal > noise\n",
    "p_cued = response.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax_att = plt.subplots(figsize=(3, 4))\n",
    "ax_att.bar([\"Cued\", \"Uncued\"], [p_cued, p_uncued])\n",
    "ax_att.set(ylabel=\"P(Correct)\")\n",
    "ax_att.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is both obvious and deep. Imagine viewing this graph before\n",
    "going through the tutorial.  Suppose spatial attention leads to enhanced\n",
    "responses in neurons with receptive fields at potential relevant\n",
    "locations.  Suppose also that attention is resource limited so that\n",
    "dividing your attention leads to a weaker attentional gain for each\n",
    "location compared to the cued condition.  Like the spotlight of attention\n",
    "has a fixed amount of stuff to spread around. Finally, assume that a gain\n",
    "change can help performance by increasing the signal-to-noise ratio.\n",
    "It would follow that weaker gain changes for the uncued (divided\n",
    "attention) condition should lead to poorer performance,\n",
    "\n",
    "This argument is all over the attention field. But this simulation shows\n",
    "that you can get strong behavioral effects for attention without any gain\n",
    "changes at all!  This idea was was elegantly described by John Palmer in\n",
    "the 80's, was largely ignored in the 90's and 00's, but has returned\n",
    "recently in light of optical imaging and fMRI results showing that V1\n",
    "responses don't seem to change much with divided attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
