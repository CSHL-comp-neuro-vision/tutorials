
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Psycho_Tutorial_I_SDT</title><meta name="generator" content="MATLAB 8.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2014-07-11"><meta name="DC.source" content="Psycho_Tutorial_I_SDT.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Psychophysics Tutorial I: Signal Detection Theory and 2AFC</a></li><li><a href="#2">Signal Detection Theory</a></li><li><a href="#10">D-prime</a></li><li><a href="#11">Estimating d-prime from Hits and False Alarms</a></li><li><a href="#13">The ROC curve</a></li><li><a href="#16">Area under the ROC curve</a></li><li><a href="#18">The relationship between d-prime and the area under the ROC curve</a></li><li><a href="#20">Simulating a Yes/No experiment</a></li><li><a href="#22">Yes/No with rating scales</a></li><li><a href="#23">Rating scales and the ROC curve</a></li><li><a href="#25">Simulating a Yes/No experiment with rating scales</a></li><li><a href="#27">Maximum likelihood fit of ROC curve</a></li><li><a href="#38">Two-alternative forced choice (2AFC)</a></li><li><a href="#44">N alternative forced choice</a></li><li><a href="#46">Divided Attention</a></li></ul></div><h2>Psychophysics Tutorial I: Signal Detection Theory and 2AFC<a name="1"></a></h2><p>Written June, 2012 by G.M. Boynton</p><p>This is the first Psychophysics tutorial, covering Signal Detection Theory, ROC curves and the 2AFC paradigm.  See also the sdtTutorial which covers some of the same material.</p><p>Written by G.M. Boynton for CSHL 2012</p><pre class="codeinput">addpath(<span class="string">'Psychophysics'</span>);
</pre><h2>Signal Detection Theory<a name="2"></a></h2><p>Suppose you want to determine if a subject can reliably detect a weak stimulus.  The simplest experiment would be to present this stimulus over multiple trials and ask if the subject saw it.  But this won't work because, for example, the subject could simply say 'yes'on each trial. To alleviate this catch trials can be included to keep the subject from cheating.</p><p>Now suppose you introduce catch trials (no stimulus trials) randomly on half of the trials.  The subject's task is to determine if the signal was present on any given trial.  Stimulus present trials are called 'signal' trials, and stimulus absent trials are called 'noise' trials. A subject that guesses, or says 'yes' or 'no' on every trial will be performing at 50%, or chance level.  No more cheating.</p><p>There is a range of stimulus intensities where a subject will perform somewhere between chance and 100% correct performance.  The presence of such a 'soft' threshold is most commonly explained in terms of Signal Detection Theory (SDT).</p><p>SDT assumes that subjects base their decision on an internal response to a stimulus that varies trom trial to trial.  If this internal response exceeds some criterion, the subject reports to have perceived the stimulus.</p><p>This trial-to-trial variability of the internal response could be due to variability in the stimulus itself (as in the case of Poisson noise for very dim lights), or to random neuronal noise at the sensory representation of the stimulus, or due to higher level variability in the attentional or motivational state of the subject.</p><p>Most commonly, this variability is modeled as a norrmal distribution centered around some mean. The simplest implementation has the mean response for the noise trials be zero and signal trials some larger value, with the standard deviations of the signal and noise responses the same.</p><p>Here are some example parameters all in a single structure</p><pre class="codeinput">p.noiseMean = 0;
p.signalMean = 1;
p.sd = 1;
</pre><p>Here is a graph of the probability distribution for the internal responses to signal and noise trials:</p><pre class="codeinput">z = -4:.2:6;  <span class="comment">%response-axis sampling points</span>

noise.y  = normpdf(z,p.noiseMean,p.sd);
signal.y = normpdf(z,p.signalMean,p.sd);

figure(1)
clf
plot(z,noise.y);
hold <span class="string">on</span>
plot(z,signal.y,<span class="string">'r-'</span>);

ylim = get(gca,<span class="string">'YLim'</span>);

text(p.noiseMean,ylim(2)*.9,<span class="string">'Noise'</span>,<span class="string">'VerticalAlignment'</span>,<span class="string">'top'</span>,<span class="string">'HorizontalAlignment'</span>,<span class="string">'center'</span>,<span class="string">'Color'</span>,<span class="string">'b'</span>);
text(p.signalMean,ylim(2)*.9,<span class="string">'Signal'</span>,<span class="string">'VerticalAlignment'</span>,<span class="string">'top'</span>,<span class="string">'HorizontalAlignment'</span>,<span class="string">'center'</span>,<span class="string">'Color'</span>,<span class="string">'r'</span>);
xlabel(<span class="string">'Internal Response'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_01.png" alt=""> <p>We next need to set a criterion value for determining what internal reponses lead to 'Yes' responses.  We'll show it in the figure:</p><pre class="codeinput">p.criterion = 1;
plot(p.criterion*[1,1],ylim,<span class="string">'k:'</span>);

hcrit = text(p.criterion,0,<span class="string">'criterion'</span>,<span class="string">'VerticalAlignment'</span>,<span class="string">'bottom'</span>,<span class="string">'HorizontalAlignment'</span>,<span class="string">'left'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_02.png" alt=""> <p>On any trial, one of four things will happen. Either the signal is present or absent crossed with the subject reporting 'yes' or 'no'. Trial types are labeled this way:</p><pre>         |         Response     |
Signal   |  "Yes"    |  "No"    |
---------------------------------
Present  |    Hit    |  Miss    |
         |           |          |
---------+-----------+----------|
Absent   |  False    | Correct  |
         |  Alarm    | Rejection|
---------------------------------</pre><p>It's easy to see that SDT predicts the probability of each of these four trial types by areas under the normal curve.  The probability of a hit is the probability of drawing a value above the criterion, given that it came from the signal distribution:</p><pre class="codeinput">pHit = 1-normcdf(p.criterion,p.signalMean,p.sd)
</pre><pre class="codeoutput">
pHit =

    0.5000

</pre><p>And the probability of a false alarm is:</p><pre class="codeinput">pFA =  1-normcdf(p.criterion,p.noiseMean,p.sd)
</pre><pre class="codeoutput">
pFA =

    0.1587

</pre><p>The whole table looks like this:</p><pre class="codeinput">disp(<span class="string">' '</span>);
disp(<span class="string">'           |         Response     |'</span>)
disp(<span class="string">'  Signal   |  "Yes"    |  "No"    |'</span>)
disp(<span class="string">'  ---------------------------------'</span>)
fprintf(<span class="string">'  Present  |   %3.1f%%   |   %3.1f   |\n'</span>,100*pHit,100*(1-pHit));
disp(<span class="string">'  ---------+-----------+----------|'</span>);
fprintf(<span class="string">'  Absent   |   %3.1f     |   %3.1f   |\n'</span>,100*pFA,100*(1-pFA));
disp(<span class="string">'  ---------------------------------'</span>);
</pre><pre class="codeoutput"> 
           |         Response     |
  Signal   |  "Yes"    |  "No"    |
  ---------------------------------
  Present  |   50.0%   |   50.0   |
  ---------+-----------+----------|
  Absent   |   15.9     |   84.1   |
  ---------------------------------
</pre><p>Since half the trials are signal trials, the overall performance will be the average of the hit and correction rate:</p><pre class="codeinput">PC = (pHit + (1-pFA))/2;  <span class="comment">%proportion correct</span>
fprintf(<span class="string">'  Percent Correct: %5.2f%%\n'</span>,100*PC);
</pre><pre class="codeoutput">  Percent Correct: 67.07%
</pre><p>Play around with the parameters. See how:</p><p>1) If you shift your criterion very low or high performance will be at chance.</p><p>2) Performance is maximized when it's halfway between the signal and noise means.  This is the criterion an 'ideal observer' should choose.</p><p>3) Performance increases as either the standard deviations decrease or the difference between signal and noise mean increases.</p><p>4) You can offset an increase between signal and noise means by increasing the standard deviation by the same amount.  The model is over-parameterized.</p><h2>D-prime<a name="10"></a></h2><p>The goal of a psychophysicist is to learn something about the 'internal' response to a stimulus based on a behavioral responses.  In SDT terms, we want to know the strength of the internal response to the signal relative to the noise.  Formally, this is defined as the difference between the signal and noise means in standard deviation units and is called d-prime:</p><pre class="codeinput">dPrime = (p.signalMean-p.noiseMean)/p.sd;
fprintf(sprintf(<span class="string">'  dPrime =    %5.2f\n'</span>,dPrime));
</pre><pre class="codeoutput">  dPrime =     1.00
</pre><h2>Estimating d-prime from Hits and False Alarms<a name="11"></a></h2><p>You should see that simply reporting percent correct in a yes/no experiment is a problem because performance varies with criterion: you cannot estimate d-prime from percent correct alone.</p><p>Fortunately we can estimate d-prime by finding the difference in the corresponding z-values from the hit and false alarm rates:</p><pre class="codeinput">zHit = norminv(pHit)
zFA = norminv(pFA)

dPrimeEst = zHit-zFA
</pre><pre class="codeoutput">
zHit =

     0


zFA =

    -1


dPrimeEst =

     1

</pre><p>This is a 'criterion' free estimate of d-prime, and is what is often reported instead of percent correct for a Yes/No experiment.</p><p>Play again with parameters.  See how dPrimeEst stays constant for different criterion values.</p><h2>The ROC curve<a name="13"></a></h2><p>The criterion determines the trade-off between hits and false alarms.  A low (liberal) criterion is sure to get a hit but will lead to lots of false alarms. A high (conservative) criterion will miss a lot of signals, but will also minimize false alarms.  This trade-off is typically visualized in the form of a 'Reciever Operating Characteristic' or ROC curve.  An ROC curve is a plot of hits against false alarms for a range of criterion values:</p><pre class="codeinput">pHits = 1-normcdf(z,p.signalMean,p.sd);
pFAs  = 1-normcdf(z,p.noiseMean,p.sd);
figure(2)
clf
hold <span class="string">on</span>
plot([0,1],[0,1],<span class="string">'k:'</span>);
axis <span class="string">equal</span>
axis <span class="string">tight</span>
xlabel(<span class="string">'pFA'</span>);
ylabel(<span class="string">'pHit'</span>);
plot(pFAs,pHits,<span class="string">'k-'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_03.png" alt=""> <p>We can plot our example hit rate against false rate too:</p><pre class="codeinput">plot(pFA,pHit,<span class="string">'ko'</span>,<span class="string">'MarkerFaceColor'</span>,<span class="string">'w'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_04.png" alt=""> <p>Play around again.  See how:</p><p>1) The point in the ROC curve moves around as you vary the criterion.</p><p>2)The 'bow' of the ROC curve varies with d-prime (either by increasing the signal mean or reducing the standard deviation.</p><h2>Area under the ROC curve<a name="16"></a></h2><p>You hopefully saw that increasing d-prime increases the bow of the ROC curve away from the diagonal.  A measure of this bowing is the area under the ROC curve.  This can be estimated by numerically integrating the sampled curve.  We'll use Matlab's 'trapz' function. (The negative sign is to undo the fact that the ROC curve traces from left-to-right for increasing criterion values).</p><pre class="codeinput">ROCarea = -trapz(pFAs,pHits)
</pre><pre class="codeoutput">
ROCarea =

    0.7595

</pre><p>We'll see later that this area has a special meaning - it's the percent correct that is expected in a two-alternative forced choice (2AFC) experiment.</p><h2>The relationship between d-prime and the area under the ROC curve<a name="18"></a></h2><p>d-prime can be calculated from the area under the ROC curve by:</p><pre class="codeinput">dPrimeFromArea = sqrt(2)*norminv(ROCarea)
</pre><pre class="codeoutput">
dPrimeFromArea =

    0.9965

</pre><p>The calculus behind this is interesting but we'll pass on it.</p><h2>Simulating a Yes/No experiment<a name="20"></a></h2><p>Next we'll use SDT to simulate a subject's response to a series of trials in a Yes/No experiment and estimate the d-prime value that was used in the simulation</p><pre class="codeinput">nTrials = 100;
isSignal = logical(floor(rand(1,nTrials)+.5));  <span class="comment">%coin flip for each trial</span>

<span class="comment">%</span>
<span class="comment">% Generate the internal response for each trial</span>
x = randn(1,nTrials)*p.sd;  <span class="comment">%draws from normal distribution with standard deviation sd.</span>
x(isSignal) = x(isSignal) + p.signalMean;
x(~isSignal) = x(~isSignal) + p.noiseMean;

<span class="comment">% Subject responds '1' if the internal response exceeds the criterion.</span>
response = x&gt;p.criterion;

<span class="comment">% Calculate hits and false alarms</span>
pHitSim = sum(response(isSignal))/sum(isSignal);
pFASim = sum(response(~isSignal))/sum(~isSignal);

<span class="comment">% Show the simulated values in the table</span>
disp(<span class="string">' '</span>);
fprintf(<span class="string">'Simulation of %d trials:\n'</span>,nTrials);
disp(<span class="string">'           |         Response     |'</span>)
disp(<span class="string">'  Signal   |  "Yes"    |  "No"    |'</span>)
disp(<span class="string">'  ---------------------------------'</span>)
fprintf(<span class="string">'  Present  |   %3.1f%%   |   %3.1f   |\n'</span>,100*pHitSim,100*(1-pHitSim));
disp(<span class="string">'  ---------+-----------+----------|'</span>);
fprintf(<span class="string">'  Absent   |   %3.1f    |   %3.1f   |\n'</span>,100*pFASim,100*(1-pFASim));
disp(<span class="string">'  ---------------------------------'</span>);

<span class="comment">% plot it on the ROC curve</span>
figure(2)
hROC = plot(pFASim,pHitSim,<span class="string">'bo'</span>,<span class="string">'MarkerFaceColor'</span>,<span class="string">'w'</span>);

<span class="comment">% calculating d-prime from pHit and pCR</span>
zHitSim = norminv(pHitSim);
zFASim = norminv(pFASim);

dPrimeSim = zHitSim-zFASim;
fprintf(<span class="string">'d-prime from simulation = %5.2f\n'</span>,dPrimeSim);
</pre><pre class="codeoutput"> 
Simulation of 100 trials:
           |         Response     |
  Signal   |  "Yes"    |  "No"    |
  ---------------------------------
  Present  |   58.7%   |   41.3   |
  ---------+-----------+----------|
  Absent   |   16.2    |   83.8   |
  ---------------------------------
d-prime from simulation =  1.21
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_05.png" alt=""> <p>Compare the simulated values to the expected values from the STD model. You can run this section over and over to look at the variability of the d-prime estimate. You can see that:</p><p>1) The estimates of d-prime become more accurate with increasing number of trials</p><p>2) The estiamtes of d-prime become less accurate with criterion values that deviate from the ideal value.  This is important. we typically don't have control over the criterion.  If a lame subject says 'yes' or 'no' almost all the time then there is very litle information for estimatingJ d-prime.</p><p>3) If you're motivated, add a loop to simulate a bunch of simulations to estimate the variability in the estimate for a range of model parameters.</p><p>Simulations like this illustrate an often neglected fact:  A 'perfect' subject that makes decisions according to Signal Detection Theory will still have variability in performance from experimental run to experimental run.  That is, ideal observers will still generate data with finite-sized error bars.  Simulations can give you a feel for how small the erorr bars should be under ideal conditions.</p><h2>Yes/No with rating scales<a name="22"></a></h2><p>One way to get around the criterion problem is to allow subjects more options in their response.  For example, rather than having two buttons, let them have four to indicate their confidence that a signal was present:</p><p>1: Definately no 2: Probably no 3: Probably yes 4: Definately yes</p><p>This effectively allows the subject to have more than one criterion.  To model this with SDT, three criterion values will divide the internal response range into the four response categories:</p><pre class="codeinput">p.criterion = [-.5,.5,1.5];

figure(1)
<span class="keyword">for</span> i=1:length(p.criterion)
    plot(p.criterion(i)*[1,1],ylim,<span class="string">'k-'</span>);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_06.png" alt=""> <h2>Rating scales and the ROC curve<a name="23"></a></h2><p>We can visualize these criterion values on the ROC curve like we did for the single criterion earlier:</p><pre class="codeinput">pHit = 1-normcdf(p.criterion,p.signalMean,p.sd);
pFA = 1-normcdf(p.criterion,p.noiseMean,p.sd);

<span class="comment">% This will clean up the ROC plot and plot the new expected points:</span>
figure(2)
clf
hold <span class="string">on</span>
plot([0,1],[0,1],<span class="string">'k:'</span>);
axis <span class="string">equal</span>
axis <span class="string">tight</span>
xlabel(<span class="string">'pFA'</span>);
ylabel(<span class="string">'pHit'</span>);
plot(pFAs,pHits,<span class="string">'k-'</span>);

plot(pFA,pHit,<span class="string">'ko'</span>,<span class="string">'MarkerFaceColor'</span>,<span class="string">'w'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_07.png" alt=""> <p>For example, pHit for the lowest point on the ROC curve corresponds to the probability that subject will report a 2, 3 or 4 on a signal trial. The next one up is for 3 or 4, and the highest is the probability of a hit if a subject just reponds '4'.</p><h2>Simulating a Yes/No experiment with rating scales<a name="25"></a></h2><pre class="codeinput">nTrials = 100;
isSignal = logical(floor(rand(1,nTrials)+.5));  <span class="comment">%coin flip for each trial</span>

<span class="comment">% Generate the internal response for each trial</span>
x = randn(1,nTrials)*p.sd;  <span class="comment">%draws from normal distribution with standard deviation sd.</span>
x(isSignal) = x(isSignal) + p.signalMean;
x(~isSignal) = x(~isSignal) + p.noiseMean;

critRange = [-inf,p.criterion];
response = zeros(1,nTrials);
<span class="keyword">for</span> i=1:nTrials
    response(i) = find(x(i)&gt;critRange,1,<span class="string">'last'</span>);
<span class="keyword">end</span>

<span class="keyword">for</span> i=1:length(p.criterion)
    pHit(i) = sum(response(isSignal==1)&gt;i)/sum(isSignal==1);
    pFA(i) =  sum(response(isSignal==0)&gt;i)/sum(isSignal==0);
<span class="keyword">end</span>

figure(2)
hold <span class="string">on</span>
plot(pFA,pHit,<span class="string">'bo'</span>,<span class="string">'MarkerFaceColor'</span>,<span class="string">'w'</span>);
title(<span class="string">''</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_08.png" alt=""> <p>Run this section over and over to get an idea of the variability of the simulation with respect to the expected values on the ROC curve.</p><h2>Maximum likelihood fit of ROC curve<a name="27"></a></h2><p>You probably have the (correct) intuition that the rating scale information adds reliability to the estimate of d-prime. But now we have the problem of translating these three points on the ROC curve to a single estimate of d-prime.</p><p>There are a variety of ways of doing this.  One would be to use the cumulative normals like we did before for each of the three points on the ROC curve and average them.  But this doesn't seem right - different points should have different weights due to variability in their reliablity.</p><p>We'll implement a model fitting method.  You should appreciate that every point in the 2D ROC space corresponds to a unique pair of d-prime and criterion values.  That's why there is a direct translation between a single point on the ROC curve and d-prime.  But now we have three points in ROC space that don't necessarily fall on the same ROC curve.  Our goal is to find the single ROC curve that passes closest to the three points. Specifically, we need a four-parmeter fit: what d-prime and three criterion values best fits our observed values?</p><p>To do this we need a cost function that takes in a set of model parameters and data points and returns a value that represents goodness of fit. Then we'll use Matlab's optimization routine to find the model parameters that minmizes this cost function.</p><p>When dealing with proportions, the cost function is always in terms of likelihood (You should never use a least-squares criterion for proportional data!) Our cost function will be the probability of our observed data for a given set of model parameters.</p><p>Suppose the first trial was a noise trial and the subject reported '1' (Definately no).  Looking at figure 1, the probability of this happening is the area under the blue curve to the left of the 1st criterion:</p><pre class="codeinput">fprintf(<span class="string">'Probability of responding ''1'' on a noise trial: %5.4f\n'</span>,normcdf(p.criterion(1),p.noiseMean,p.signalMean));
</pre><pre class="codeoutput">Probability of responding '1' on a noise trial: 0.3085
</pre><p>The probability of responding '2' is the area between the 1st and second criteria and so on.  The whole table of probabilities can be computed like this:</p><pre class="codeinput">critRange = [-inf,p.criterion,inf];

<span class="keyword">for</span> i=1:length(critRange)-1;
    pResp(i,1) = normcdf(critRange(i+1),p.noiseMean,p.sd)-<span class="keyword">...</span>
        normcdf(critRange(i),p.noiseMean,p.sd);
    pResp(i,2) = normcdf(critRange(i+1),p.signalMean,p.sd)-<span class="keyword">...</span>
        normcdf(critRange(i),p.signalMean,p.sd);
<span class="keyword">end</span>

disp(pResp);
</pre><pre class="codeoutput">    0.3085    0.0668
    0.3829    0.2417
    0.2417    0.3829
    0.0668    0.3085

</pre><p>The first column is for noise trials, the second is for signal trials, and each row corresponds to each response ('1'-'4').  Verify that the sum of the columns add up to 1</p><p>The probability of obtaining our observed data set based on these probabilities is the product of the probabilities associated with each trial.  Multiplying 100 values that are less than 1 produces a ludicrously small number, so we almost always maximize log likelihood instead.  Acutally, we'll use the negative of the log likelihood because optimization routines minmize functions. The negative of the log likelihood of our observed data is:</p><pre class="codeinput">cost = 0;

<span class="keyword">for</span> i=1:length(isSignal);
    cost = cost-log(pResp(response(i),isSignal(i)+1));
<span class="keyword">end</span>

fprintf(<span class="string">'Negative of log likelihood: %5.4f ='</span>,cost);
</pre><pre class="codeoutput">Negative of log likelihood: 127.6301 =</pre><p>I've written a function that does this because we'll need it for the optimization routine:</p><pre class="codeinput">disp(logLikelihoodROC(p,isSignal,response))
</pre><pre class="codeoutput">  127.6301

</pre><p>To find the parameters that minimize our cost function we'll use Matlab's 'fminsearch' function.  I find their implementation extremely useful, but inflexible.  Instead, I've written a wrapper function called 'fit' that allows you to choose which parameters to hold fixed and which to set free.  It works like this:</p><pre class="codeinput">pBest = fit(<span class="string">'logLikelihoodROC'</span>,p,{<span class="string">'signalMean'</span>,<span class="string">'criterion'</span>},isSignal,response);
</pre><pre class="codeoutput">Fitting "logLikelihoodROC" with 4 free parameters.
</pre><p>The first argument is the name of the cost function.  The cost function must take in a structure containing the model parameters as its first argument ('p' here).  The second argument into 'fit' is a structure containing the intial parameters for the model fit.  The third is a cell array containing the names of the fields for the parameters to be allowed to vary.  If the field corresponds to an array (like 'criterion' here), all values in the array will be set free.  The remaining fields are the remaining inputs into the cost function.</p><p>Look at the best-fitting parameters:</p><pre class="codeinput">pBest
</pre><pre class="codeoutput">
pBest = 

     noiseMean: 0
    signalMean: 0.8464
            sd: 1
     criterion: [-0.5887 0.2847 1.3461]

</pre><p>Do they resemble the original model parameters:</p><pre class="codeinput">p
</pre><pre class="codeoutput">
p = 

     noiseMean: 0
    signalMean: 1
            sd: 1
     criterion: [-0.5000 0.5000 1.5000]

</pre><p>Let's draw the ROC curve for the best fitting SDT parameters in blue:</p><pre class="codeinput">pHitsBest = 1-normcdf(z,pBest.signalMean,pBest.sd);
pFAsBest = 1-normcdf(z,pBest.noiseMean,pBest.sd);

plot(pFAsBest,pHitsBest,<span class="string">'b-'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_09.png" alt=""> <p>And plot the estimated three ROC points from the model on the blue curve</p><pre class="codeinput">pHitBest = 1-normcdf(pBest.criterion,pBest.signalMean,pBest.sd);
pFABest = 1-normcdf(pBest.criterion,pBest.noiseMean,pBest.sd);
plot(pFABest,pHitBest,<span class="string">'bo'</span>,<span class="string">'MarkerFaceColor'</span>,<span class="string">'b'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_10.png" alt=""> <p>Our estimate of d-prime is then simply:</p><pre class="codeinput">dPrimeEst = (pBest.signalMean-pBest.noiseMean)/pBest.sd;
fprintf(<span class="string">'Estimated d-prime: %5.2f\n'</span>,dPrimeEst);
</pre><pre class="codeoutput">Estimated d-prime:  0.85
</pre><p>Run this over and over to get a feel for the variability of the estimate of d-prime.  Does the rating scale provide a more reliable estimate of d-prime than the standard Yes/No experiment?  You can test this more formally by repeating a bunch of simulations.</p><h2>Two-alternative forced choice (2AFC)<a name="38"></a></h2><p>Another way to avoid the criterion problem is to use a two-alternative-forced-choice paradigm (2AFC) where a trial consists of both a signal and noise draw in either random temporal order or spatial position.  The subject must choose which draw contains the signal.  2AFC is easilly modelled with SDT by drawing once from the signal, and once from the noise distribution.  The subject decides that the signal came from the draw with the larger value.</p><p>Let's get rid of the 'criterion' field:</p><pre class="codeinput"><span class="keyword">if</span> isfield(p,<span class="string">'criterion'</span>)
    p = rmfield(p,<span class="string">'criterion'</span>);
<span class="keyword">end</span>
</pre><p>Here's a simulation: as before...</p><pre class="codeinput">nTrials = 100;

x= randn(2,nTrials);
x(1,:) = x(1,:)*p.sd + p.noiseMean;
x(2,:) = x(2,:)*p.sd + p.signalMean;

response = x(2,:)&gt;x(1,:);
</pre><p>Response is 1 for correct trials, when the draw from the signal exceeds the noise draw.</p><p>Overall performance is the mean of the response vector:</p><pre class="codeinput">pc2AFC = mean(response);
fprintf(<span class="string">'Percent correct: %5.3f\n'</span>,pc2AFC);
</pre><pre class="codeoutput">Percent correct: 0.700
</pre><p>Note that percent correct is greater than the best percent correct in the yes/no experiment.</p><p>Here's something interesting: the expected percent correct in 2AFC should be equal to the area under the ROC curve:</p><pre class="codeinput">fprintf(<span class="string">'Area under ROC:  %5.3f\n'</span>,ROCarea);
</pre><pre class="codeoutput">Area under ROC:  0.759
</pre><p>Run this section several times over to convince yourself that this is true. This means that d-prime can be directly estimated from percent correct for a 2AFC experiment (because d-prime is directly related to area under the ROC curve).</p><pre class="codeinput">dPrimeFrom2AFC = sqrt(2)*norminv(pc2AFC);

fprintf(<span class="string">'d-prime estimated from 2AFC simulation: %5.2f\n'</span>,dPrimeFrom2AFC);
</pre><pre class="codeoutput">d-prime estimated from 2AFC simulation:  0.74
</pre><p>How reliable is this estimate of d-prime?  An interesting exercise would be to measure the standard deviation of the d-prime estimates for repeated simulations of the standard Yes/No, the Yes/No rating method, and the 2AFC method.  Which wins?</p><h2>N alternative forced choice<a name="44"></a></h2><p>Forced choice experiments can include more than two options.  For example, suppose a target could appear randomly in one of four spatial positions.  This can be modeled with SDT as taking three samples from the noise distribution and one from the signal distribution.  Like for 2AFC, the subject chooses the location with that generated the largest response. A correct response occurs when the draw from the signal distribution exceeds the maximum of the noise draws.  This is easy to simulate:</p><pre class="codeinput">nTrials = 100;

p.noiseMean  = 0;
p.signalMean  =1;
p.sd = 1;

nAlternatives = 4;

signal = randn(1,nTrials)*p.sd+p.signalMean;  <span class="comment">%one draw from signal</span>
noise= randn(nAlternatives-1,nTrials)*p.sd+p.noiseMean; <span class="comment">%three from noise</span>

response = signal&gt;max(noise);

PC4AFC = mean(response)
</pre><pre class="codeoutput">
PC4AFC =

    0.6200

</pre><p>There is a solution (using numerical integration) for percent correct for an NAFC experiment:</p><pre class="codeinput">dPrime = (p.signalMean-p.noiseMean)/p.sd;
z = linspace(-5,5,501);
dz = z(2)-z(1);

nAlternatives =4;
PC4AFCnumerical = sum(normpdf(z-dPrime).*normcdf(z).^(nAlternatives-1))*dz;

figure(3)
bar([PC4AFC,PC4AFCnumerical]);
set(gca,<span class="string">'XTickLabel'</span>,{<span class="string">'simulated'</span>,<span class="string">'numerical'</span>});
set(gca,<span class="string">'YLim'</span>,[0,1]);
ylabel(<span class="string">'Proportion correct'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_11.png" alt=""> <h2>Divided Attention<a name="46"></a></h2><p>Let's simulate a real divided attention experiment. Again there are four spatial positions, but this time it's a 2AFC experiment in which a signal (like a grating or something) appears in one of the positions on one of two intervals.  The subject's job is to determine the interval that contained the signal, wherever it was.  Now, consider two conditions, one in which you have no idea which of the four conditions will have the signal and one in which you are cued to the correct location.  It's still a 2AFC task, but one forces you to divide your spatial attention.  Again, it's easy to simulate.</p><pre class="codeinput">nTrials  = 100;

nPositions = 4; <span class="comment">%divided attention (no cue to position)</span>

<span class="comment">%noise interval - 4 random draws</span>
noise = randn(nPositions,nTrials)*p.sd+p.noiseMean;

<span class="comment">%signal interval - 3 random draws, one signal draw</span>
signal = randn(nPositions,nTrials)*p.sd;
signal(1,:) = signal(1,:)+p.signalMean;
signal(2:nPositions,:) = signal(2:nPositions,:) +p.noiseMean;
</pre><p>decision rule: choose the signal interval if the max of the 3 noise + 1 signal draws exceeds the max of the four noise draws</p><pre class="codeinput">response = max(signal,[],1)&gt;max(noise,[],1);

PCuncued = mean(response);
</pre><p>If there is a cue to the spatial position, then we can assume that the subject ignores the thee uncued locations.  This is just a the same old 2AFC experiment. Equivalently we can use the same code with nPositions set to 1:</p><pre class="codeinput">clear <span class="string">signal</span> <span class="string">noise</span>
nPositions = 1;

noise = randn(nPositions,nTrials)*p.sd+p.noiseMean;

signal = randn(1,nTrials)*p.sd;
signal(1,:) = signal(1,:)+p.signalMean;
signal(2:nPositions,:) = signal(2:nPositions,:) +p.noiseMean;
</pre><p>decision rule: choose the signal interval if the max of the 3 noise + 1 signal draws exceeds the max of the four noise draws</p><pre class="codeinput">response = max(signal,[],1)&gt;max(noise,[],1);
PCcued = mean(response);

figure(3)
bar([PCuncued,PCcued]);
set(gca,<span class="string">'XTickLabel'</span>,{<span class="string">'uncued'</span>,<span class="string">'cued'</span>});
set(gca,<span class="string">'YLim'</span>,[.5,1]);
ylabel(<span class="string">'Proportion correct'</span>);
</pre><img vspace="5" hspace="5" src="Psycho_Tutorial_I_SDT_12.png" alt=""> <p>This result is both obvious and deep. Imagine viewing this graph before going through the tutorial.  Suppose spatial attention leads to enhanced responses in neurons with receptive fields at potential relevant locations.  Suppose also that attention is resource limited so that dividing your attention leads to a weaker attentional gain for each location compared to the cued condition.  Like the spotlight of attention has a fixed amount of stuff to spread around. Finally, assume that a gain change can help performance by increasing the signal-to-noise ratio of. It would follow that weaker gain changes for the uncued (divided attention) condition should lead to poorer performance,</p><p>This argument is all over the attention field. But this simulation shows that you can get strong behavioral effects for attention without any gain changes at all!  This idea was was elegantly described by John Palmer in the 80's, was largely ignored in the 90's and 00's, but has returned recently in light of optical imaging and fMRI results showing that V1 responses don't seem to change much with divided attention.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Psychophysics Tutorial I: Signal Detection Theory and 2AFC
%
% Written June, 2012 by G.M. Boynton
%
% This is the first Psychophysics tutorial, covering Signal Detection
% Theory, ROC curves and the 2AFC paradigm.  See also the sdtTutorial which
% covers some of the same material.
%
% Written by G.M. Boynton for CSHL 2012

addpath('Psychophysics');

%% Signal Detection Theory
%
% Suppose you want to determine if a subject can reliably detect a weak
% stimulus.  The simplest experiment would be to present this stimulus over
% multiple trials and ask if the subject saw it.  But this won't work
% because, for example, the subject could simply say 'yes'on each trial.
% To alleviate this catch trials can be included to keep the subject from
% cheating.
%
% Now suppose you introduce catch trials (no stimulus trials) randomly on
% half of the trials.  The subject's task is to determine if the signal was
% present on any given trial.  Stimulus present trials are called 'signal'
% trials, and stimulus absent trials are called 'noise' trials. A subject
% that guesses, or says 'yes' or 'no' on every trial will be performing at
% 50%, or chance level.  No more cheating.
%
% There is a range of stimulus intensities where a subject will perform
% somewhere between chance and 100% correct performance.  The presence of
% such a 'soft' threshold is most commonly explained in terms of Signal
% Detection Theory (SDT).
%
% SDT assumes that subjects base their decision on an internal response to
% a stimulus that varies trom trial to trial.  If this internal response
% exceeds some criterion, the subject reports to have perceived the
% stimulus.
%
% This trial-to-trial variability of the internal response could be due to
% variability in the stimulus itself (as in the case of Poisson noise for
% very dim lights), or to random neuronal noise at the sensory
% representation of the stimulus, or due to higher level variability in the
% attentional or motivational state of the subject.  
%
% Most commonly, this variability is modeled as a norrmal distribution
% centered around some mean. The simplest implementation has the mean
% response for the noise trials be zero and signal trials some larger
% value, with the standard deviations of the signal and noise responses the
% same.  
%
% Here are some example parameters all in a single structure 

p.noiseMean = 0;   
p.signalMean = 1;
p.sd = 1;

%%
% Here is a graph of the probability distribution for the internal
% responses to signal and noise trials:

z = -4:.2:6;  %response-axis sampling points

noise.y  = normpdf(z,p.noiseMean,p.sd);
signal.y = normpdf(z,p.signalMean,p.sd);

figure(1)
clf
plot(z,noise.y);
hold on
plot(z,signal.y,'r-');

ylim = get(gca,'YLim');

text(p.noiseMean,ylim(2)*.9,'Noise','VerticalAlignment','top','HorizontalAlignment','center','Color','b');
text(p.signalMean,ylim(2)*.9,'Signal','VerticalAlignment','top','HorizontalAlignment','center','Color','r');
xlabel('Internal Response');
%%
% We next need to set a criterion value for determining what internal
% reponses lead to 'Yes' responses.  We'll show it in the figure:

p.criterion = 1;
plot(p.criterion*[1,1],ylim,'k:');

hcrit = text(p.criterion,0,'criterion','VerticalAlignment','bottom','HorizontalAlignment','left');

%%
% On any trial, one of four things will happen. Either the signal is
% present or absent crossed with the subject reporting 'yes' or 'no'.
% Trial types are labeled this way:
%
%           |         Response     |
%  Signal   |  "Yes"    |  "No"    |
%  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
%  Present  |    Hit    |  Miss    |
%           |           |          |
%  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-+REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-+REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH|
%  Absent   |  False    | Correct  |
%           |  Alarm    | Rejection|
%  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
%
% It's easy to see that SDT predicts the probability of each of these four
% trial types by areas under the normal curve.  The probability of a hit is
% the probability of drawing a value above the criterion, given that it
% came from the signal distribution:
%
pHit = 1-normcdf(p.criterion,p.signalMean,p.sd)

%%
% And the probability of a false alarm is:
%
pFA =  1-normcdf(p.criterion,p.noiseMean,p.sd)

%% 
% The whole table looks like this:

disp(' ');
disp('           |         Response     |')
disp('  Signal   |  "Yes"    |  "No"    |')
disp('  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-')
fprintf('  Present  |   %3.1f%%   |   %3.1f   |\n',100*pHit,100*(1-pHit));
disp('  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-+REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-+REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH|');
fprintf('  Absent   |   %3.1f     |   %3.1f   |\n',100*pFA,100*(1-pFA));
disp('  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-');

%%
% Since half the trials are signal trials, the overall performance will be
% the average of the hit and correction rate:

PC = (pHit + (1-pFA))/2;  %proportion correct
fprintf('  Percent Correct: %5.2f%%\n',100*PC);

%%
% Play around with the parameters. See how:
%
% 1) If you shift your criterion very low or high performance will be at
% chance.
%
% 2) Performance is maximized when it's halfway between the signal and noise
% means.  This is the criterion an 'ideal observer' should choose. 
%
% 3) Performance increases as either the standard deviations decrease or
% the difference between signal and noise mean increases.
%
% 4) You can offset an increase between signal and noise means by
% increasing the standard deviation by the same amount.  The model is
% over-parameterized.

%% D-prime
%
% The goal of a psychophysicist is to learn something about the 'internal'
% response to a stimulus based on a behavioral responses.  In SDT terms, we
% want to know the strength of the internal response to the signal relative
% to the noise.  Formally, this is defined as the difference between the
% signal and noise means in standard deviation units and is called d-prime:
%
dPrime = (p.signalMean-p.noiseMean)/p.sd;
fprintf(sprintf('  dPrime =    %5.2f\n',dPrime));

%% Estimating d-prime from Hits and False Alarms
%
% You should see that simply reporting percent correct in a yes/no
% experiment is a problem because performance varies with criterion: you
% cannot estimate d-prime from percent correct alone.
%
% Fortunately we can estimate d-prime by finding the difference in the
% corresponding z-values from the hit and false alarm rates:

zHit = norminv(pHit)
zFA = norminv(pFA)

dPrimeEst = zHit-zFA

%%
% This is a 'criterion' free estimate of d-prime, and is what is often
% reported instead of percent correct for a Yes/No experiment.
%
% Play again with parameters.  See how dPrimeEst stays constant for
% different criterion values.


%% The ROC curve
%
% The criterion determines the trade-off between hits and false alarms.  A
% low (liberal) criterion is sure to get a hit but will lead to lots of
% false alarms. A high (conservative) criterion will miss a lot of signals,
% but will also minimize false alarms.  This trade-off is typically
% visualized in the form of a 'Reciever Operating Characteristic' or ROC
% curve.  An ROC curve is a plot of hits against false alarms for a range
% of criterion values:

pHits = 1-normcdf(z,p.signalMean,p.sd);
pFAs  = 1-normcdf(z,p.noiseMean,p.sd);
figure(2)
clf
hold on
plot([0,1],[0,1],'k:');
axis equal
axis tight
xlabel('pFA');
ylabel('pHit');
plot(pFAs,pHits,'k-');

%%
% We can plot our example hit rate against false rate too:
plot(pFA,pHit,'ko','MarkerFaceColor','w');

%%
% Play around again.  See how:
%
% 1) The point in the ROC curve moves around as you vary the criterion.  
%
% 2)The 'bow' of the ROC curve varies with d-prime (either by increasing
% the signal mean or reducing the standard deviation.

%% Area under the ROC curve
%
% You hopefully saw that increasing d-prime increases the bow of the ROC
% curve away from the diagonal.  A measure of this bowing is the area under
% the ROC curve.  This can be estimated by numerically integrating the
% sampled curve.  We'll use Matlab's 'trapz' function. (The negative sign
% is to undo the fact that the ROC curve traces from left-to-right for
% increasing criterion values).

ROCarea = -trapz(pFAs,pHits)


%%
% We'll see later that this area has a special meaning - it's the percent
% correct that is expected in a two-alternative forced choice (2AFC)
% experiment.


%% The relationship between d-prime and the area under the ROC curve
%
% d-prime can be calculated from the area under the ROC curve by:

dPrimeFromArea = sqrt(2)*norminv(ROCarea)

%%
% The calculus behind this is interesting but we'll pass on it. 

%% Simulating a Yes/No experiment
%
% Next we'll use SDT to simulate a subject's response to a series of trials
% in a Yes/No experiment and estimate the d-prime value that was used in
% the simulation

nTrials = 100;
isSignal = logical(floor(rand(1,nTrials)+.5));  %coin flip for each trial

%
% Generate the internal response for each trial
x = randn(1,nTrials)*p.sd;  %draws from normal distribution with standard deviation sd.
x(isSignal) = x(isSignal) + p.signalMean; 
x(~isSignal) = x(~isSignal) + p.noiseMean;

% Subject responds '1' if the internal response exceeds the criterion.
response = x>p.criterion;

% Calculate hits and false alarms
pHitSim = sum(response(isSignal))/sum(isSignal);
pFASim = sum(response(~isSignal))/sum(~isSignal);

% Show the simulated values in the table
disp(' ');
fprintf('Simulation of %d trials:\n',nTrials);
disp('           |         Response     |')
disp('  Signal   |  "Yes"    |  "No"    |')
disp('  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-')
fprintf('  Present  |   %3.1f%%   |   %3.1f   |\n',100*pHitSim,100*(1-pHitSim));
disp('  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-+REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-+REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH|');
fprintf('  Absent   |   %3.1f    |   %3.1f   |\n',100*pFASim,100*(1-pFASim));
disp('  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-');

% plot it on the ROC curve
figure(2)
hROC = plot(pFASim,pHitSim,'bo','MarkerFaceColor','w');

% calculating d-prime from pHit and pCR
zHitSim = norminv(pHitSim);
zFASim = norminv(pFASim);

dPrimeSim = zHitSim-zFASim;
fprintf('d-prime from simulation = %5.2f\n',dPrimeSim);

%%
% Compare the simulated values to the expected values from the STD model.
% You can run this section over and over to look at the variability of the
% d-prime estimate. You can see that:
%
% 1) The estimates of d-prime become more accurate with increasing number
% of trials
%
% 2) The estiamtes of d-prime become less accurate with criterion values
% that deviate from the ideal value.  This is important. we typically don't
% have control over the criterion.  If a lame subject says 'yes' or 'no'
% almost all the time then there is very litle information for estimatingJ
% d-prime.
% 
% 3) If you're motivated, add a loop to simulate a bunch of simulations to
% estimate the variability in the estimate for a range of model parameters.
%
% Simulations like this illustrate an often neglected fact:  A 'perfect'
% subject that makes decisions according to Signal Detection Theory will
% still have variability in performance from experimental run to
% experimental run.  That is, ideal observers will still generate data
% with finite-sized error bars.  Simulations can give you a feel for how
% small the erorr bars should be under ideal conditions.

%% Yes/No with rating scales
%
% One way to get around the criterion problem is to allow subjects more
% options in their response.  For example, rather than having two buttons,
% let them have four to indicate their confidence that a signal was
% present:
%
% 1: Definately no
% 2: Probably no
% 3: Probably yes
% 4: Definately yes
%
% This effectively allows the subject to have more than one criterion.  To
% model this with SDT, three criterion values will divide the internal
% response range into the four response categories:

p.criterion = [-.5,.5,1.5]; 

figure(1)
for i=1:length(p.criterion)
    plot(p.criterion(i)*[1,1],ylim,'k-');
end

%% Rating scales and the ROC curve
%
% We can visualize these criterion values on the ROC curve like we did for
% the single criterion earlier:

pHit = 1-normcdf(p.criterion,p.signalMean,p.sd);
pFA = 1-normcdf(p.criterion,p.noiseMean,p.sd);

% This will clean up the ROC plot and plot the new expected points:
figure(2)
clf
hold on
plot([0,1],[0,1],'k:');
axis equal
axis tight
xlabel('pFA');
ylabel('pHit');
plot(pFAs,pHits,'k-');

plot(pFA,pHit,'ko','MarkerFaceColor','w');

%%
% For example, pHit for the lowest point on the ROC curve corresponds to
% the probability that subject will report a 2, 3 or 4 on a signal trial.
% The next one up is for 3 or 4, and the highest is the probability of
% a hit if a subject just reponds '4'.

%% Simulating a Yes/No experiment with rating scales

nTrials = 100;
isSignal = logical(floor(rand(1,nTrials)+.5));  %coin flip for each trial

% Generate the internal response for each trial
x = randn(1,nTrials)*p.sd;  %draws from normal distribution with standard deviation sd.
x(isSignal) = x(isSignal) + p.signalMean; 
x(~isSignal) = x(~isSignal) + p.noiseMean;

critRange = [-inf,p.criterion];
response = zeros(1,nTrials);
for i=1:nTrials
    response(i) = find(x(i)>critRange,1,'last');
end

for i=1:length(p.criterion)
    pHit(i) = sum(response(isSignal==1)>i)/sum(isSignal==1);
    pFA(i) =  sum(response(isSignal==0)>i)/sum(isSignal==0);
end

figure(2)
hold on
plot(pFA,pHit,'bo','MarkerFaceColor','w');
title('');

%%
% Run this section over and over to get an idea of the variability of the
% simulation with respect to the expected values on the ROC curve.

%% Maximum likelihood fit of ROC curve
%
% You probably have the (correct) intuition that the rating scale
% information adds reliability to the estimate of d-prime. But now we have
% the problem of translating these three points on the ROC curve to a
% single estimate of d-prime.  
%
% There are a variety of ways of doing this.  One would be to use the
% cumulative normals like we did before for each of the three points on the
% ROC curve and average them.  But this doesn't seem right - different
% points should have different weights due to variability in their
% reliablity.  
%
% We'll implement a model fitting method.  You should appreciate that every
% point in the 2D ROC space corresponds to a unique pair of d-prime and
% criterion values.  That's why there is a direct translation between a
% single point on the ROC curve and d-prime.  But now we have three points
% in ROC space that don't necessarily fall on the same ROC curve.  Our goal
% is to find the single ROC curve that passes closest to the three points.
% Specifically, we need a four-parmeter fit: what d-prime and three
% criterion values best fits our observed values?
%
% To do this we need a cost function that takes in a set of model
% parameters and data points and returns a value that represents goodness
% of fit. Then we'll use Matlab's optimization routine to find the model
% parameters that minmizes this cost function.
%
% When dealing with proportions, the cost function is always in terms of
% likelihood (You should never use a least-squares criterion for
% proportional data!) Our cost function will be the probability of our
% observed data for a given set of model parameters.
%
% Suppose the first trial was a noise trial and the subject reported '1'
% (Definately no).  Looking at figure 1, the probability of this happening
% is the area under the blue curve to the left of the 1st criterion:

fprintf('Probability of responding ''1'' on a noise trial: %5.4f\n',normcdf(p.criterion(1),p.noiseMean,p.signalMean));

%%
% The probability of responding '2' is the area between the 1st and second
% criteria and so on.  The whole table of probabilities can be computed
% like this:

critRange = [-inf,p.criterion,inf];

for i=1:length(critRange)-1;
    pResp(i,1) = normcdf(critRange(i+1),p.noiseMean,p.sd)-...
        normcdf(critRange(i),p.noiseMean,p.sd);
    pResp(i,2) = normcdf(critRange(i+1),p.signalMean,p.sd)-...
        normcdf(critRange(i),p.signalMean,p.sd);
end

disp(pResp);

%%
% The first column is for noise trials, the second is for signal trials,
% and each row corresponds to each response ('1'-'4').  Verify that the sum
% of the columns add up to 1
%
% The probability of obtaining our observed data set based on these
% probabilities is the product of the probabilities associated with each
% trial.  Multiplying 100 values that are less than 1 produces a
% ludicrously small number, so we almost always maximize log likelihood
% instead.  Acutally, we'll use the negative of the log likelihood because
% optimization routines minmize functions. The negative of the log
% likelihood of our observed data is:

cost = 0;

for i=1:length(isSignal);
    cost = cost-log(pResp(response(i),isSignal(i)+1));
end

fprintf('Negative of log likelihood: %5.4f =',cost);

%%
% I've written a function that does this because we'll need it for the
% optimization routine:

disp(logLikelihoodROC(p,isSignal,response))

%%
% To find the parameters that minimize our cost function we'll use Matlab's
% 'fminsearch' function.  I find their implementation extremely useful, but
% inflexible.  Instead, I've written a wrapper function called 'fit' that
% allows you to choose which parameters to hold fixed and which to set
% free.  It works like this:

pBest = fit('logLikelihoodROC',p,{'signalMean','criterion'},isSignal,response);

%%
% The first argument is the name of the cost function.  The cost function
% must take in a structure containing the model parameters as its first
% argument ('p' here).  The second argument into 'fit' is a structure
% containing the intial parameters for the model fit.  The third is a cell
% array containing the names of the fields for the parameters to be allowed
% to vary.  If the field corresponds to an array (like 'criterion' here),
% all values in the array will be set free.  The remaining fields are the
% remaining inputs into the cost function.  
%
% Look at the best-fitting parameters:
pBest

%%
% Do they resemble the original model parameters:
p

%%
% Let's draw the ROC curve for the best fitting SDT parameters in blue:

pHitsBest = 1-normcdf(z,pBest.signalMean,pBest.sd);
pFAsBest = 1-normcdf(z,pBest.noiseMean,pBest.sd);

plot(pFAsBest,pHitsBest,'b-');

%%
% And plot the estimated three ROC points from the model on the blue curve
pHitBest = 1-normcdf(pBest.criterion,pBest.signalMean,pBest.sd);
pFABest = 1-normcdf(pBest.criterion,pBest.noiseMean,pBest.sd);
plot(pFABest,pHitBest,'bo','MarkerFaceColor','b');

%%
% Our estimate of d-prime is then simply:
dPrimeEst = (pBest.signalMean-pBest.noiseMean)/pBest.sd;
fprintf('Estimated d-prime: %5.2f\n',dPrimeEst);

%%
% Run this over and over to get a feel for the variability of the estimate
% of d-prime.  Does the rating scale provide a more reliable estimate of
% d-prime than the standard Yes/No experiment?  You can test this more
% formally by repeating a bunch of simulations.
    
%% Two-alternative forced choice (2AFC)
%
% Another way to avoid the criterion problem is to use a
% two-alternative-forced-choice paradigm (2AFC) where a trial consists of
% both a signal and noise draw in either random temporal order or spatial
% position.  The subject must choose which draw contains the signal.  2AFC
% is easilly modelled with SDT by drawing once from the signal, and once
% from the noise distribution.  The subject decides that the signal came
% from the draw with the larger value.  
%
% Let's get rid of the 'criterion' field:

if isfield(p,'criterion')
    p = rmfield(p,'criterion');
end

%%
% Here's a simulation:
% as before...
nTrials = 100;

x= randn(2,nTrials);
x(1,:) = x(1,:)*p.sd + p.noiseMean;
x(2,:) = x(2,:)*p.sd + p.signalMean;

response = x(2,:)>x(1,:);

%%
% Response is 1 for correct trials, when the draw from the signal exceeds
% the noise draw.
%
% Overall performance is the mean of the response vector:
pc2AFC = mean(response);
fprintf('Percent correct: %5.3f\n',pc2AFC);

%%
% Note that percent correct is greater than the best percent correct in the
% yes/no experiment.  
%
% Here's something interesting: the expected percent correct in 2AFC should
% be equal to the area under the ROC curve:

fprintf('Area under ROC:  %5.3f\n',ROCarea);

%%
% Run this section several times over to convince yourself that this is
% true. This means that d-prime can be directly estimated from percent
% correct for a 2AFC experiment (because d-prime is directly related to
% area under the ROC curve).

dPrimeFrom2AFC = sqrt(2)*norminv(pc2AFC);

fprintf('d-prime estimated from 2AFC simulation: %5.2f\n',dPrimeFrom2AFC);

%%
% How reliable is this estimate of d-prime?  An interesting exercise would
% be to measure the standard deviation of the d-prime estimates for
% repeated simulations of the standard Yes/No, the Yes/No rating method,
% and the 2AFC method.  Which wins?

%% N alternative forced choice 
%
% Forced choice experiments can include more than two options.  For
% example, suppose a target could appear randomly in one of four spatial
% positions.  This can be modeled with SDT as taking three samples from the
% noise distribution and one from the signal distribution.  Like for 2AFC,
% the subject chooses the location with that generated the largest
% response. A correct response occurs when the draw from the signal
% distribution exceeds the maximum of the noise draws.  This is easy to
% simulate:

nTrials = 100;

p.noiseMean  = 0;
p.signalMean  =1;
p.sd = 1;

nAlternatives = 4;

signal = randn(1,nTrials)*p.sd+p.signalMean;  %one draw from signal
noise= randn(nAlternatives-1,nTrials)*p.sd+p.noiseMean; %three from noise

response = signal>max(noise);  

PC4AFC = mean(response)

%%
% There is a solution (using numerical integration) for percent correct for
% an NAFC experiment:

dPrime = (p.signalMean-p.noiseMean)/p.sd;
z = linspace(-5,5,501);
dz = z(2)-z(1);

nAlternatives =4;
PC4AFCnumerical = sum(normpdf(z-dPrime).*normcdf(z).^(nAlternatives-1))*dz;

figure(3)
bar([PC4AFC,PC4AFCnumerical]);
set(gca,'XTickLabel',{'simulated','numerical'});
set(gca,'YLim',[0,1]);
ylabel('Proportion correct');

%% Divided Attention
% Let's simulate a real divided attention experiment. Again there are four
% spatial positions, but this time it's a 2AFC experiment in which a signal
% (like a grating or something) appears in one of the positions on one of
% two intervals.  The subject's job is to determine the interval that
% contained the signal, wherever it was.  Now, consider two conditions, one
% in which you have no idea which of the four conditions will have the
% signal and one in which you are cued to the correct location.  It's still
% a 2AFC task, but one forces you to divide your spatial attention.  Again,
% it's easy to simulate.  

nTrials  = 100;

nPositions = 4; %divided attention (no cue to position)

%noise interval - 4 random draws
noise = randn(nPositions,nTrials)*p.sd+p.noiseMean;

%signal interval - 3 random draws, one signal draw
signal = randn(nPositions,nTrials)*p.sd;
signal(1,:) = signal(1,:)+p.signalMean;
signal(2:nPositions,:) = signal(2:nPositions,:) +p.noiseMean;

%%
% decision rule: choose the signal interval if the max of the 3 noise + 1
% signal draws exceeds the max of the four noise draws
response = max(signal,[],1)>max(noise,[],1);

PCuncued = mean(response);

%%
% If there is a cue to the spatial position, then we can assume that the
% subject ignores the thee uncued locations.  This is just a the same old
% 2AFC experiment. Equivalently we can use the same code with nPositions
% set to 1:
clear signal noise
nPositions = 1;

noise = randn(nPositions,nTrials)*p.sd+p.noiseMean;

signal = randn(1,nTrials)*p.sd;
signal(1,:) = signal(1,:)+p.signalMean;
signal(2:nPositions,:) = signal(2:nPositions,:) +p.noiseMean;

%%
% decision rule: choose the signal interval if the max of the 3 noise + 1
% signal draws exceeds the max of the four noise draws
response = max(signal,[],1)>max(noise,[],1);
PCcued = mean(response);

figure(3)
bar([PCuncued,PCcued]);
set(gca,'XTickLabel',{'uncued','cued'});
set(gca,'YLim',[.5,1]);
ylabel('Proportion correct');

%%
% This result is both obvious and deep. Imagine viewing this graph before
% going through the tutorial.  Suppose spatial attention leads to enhanced
% responses in neurons with receptive fields at potential relevant
% locations.  Suppose also that attention is resource limited so that
% dividing your attention leads to a weaker attentional gain for each
% location compared to the cued condition.  Like the spotlight of attention
% has a fixed amount of stuff to spread around. Finally, assume that a gain
% change can help performance by increasing the signal-to-noise ratio of.
% It would follow that weaker gain changes for the uncued (divided
% attention) condition should lead to poorer performance,
% 
% This argument is all over the attention field. But this simulation shows
% that you can get strong behavioral effects for attention without any gain
% changes at all!  This idea was was elegantly described by John Palmer in
% the 80's, was largely ignored in the 90's and 00's, but has returned
% recently in light of optical imaging and fMRI results showing that V1
% responses don't seem to change much with divided attention.


##### SOURCE END #####
--></body></html>