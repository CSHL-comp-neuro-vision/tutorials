%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  File: pcasvdTutorial.m%%%  Original Lisp Code written by E.J. Chichilnisky, summer 1992 %%%  Converted to Matlab by D.J. Heeger, summer 1996%%%  supplemented by Anne Churchland on 5/30/02%%%  updated by Lea Duncker on 03/07/22%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% the first part of this tutorial is designed to provide an introduction to % principle components analysis and the singular value decomposition% it would behoove the reader to follow up this first part with the% second part of the tutorial which explains how the singular value % decomposition accomplishes what it does.  For a detailed% introduction, consult a linear algebra text.  Linear Algebra and its% Applications by Gilbert Strang is excellent.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%clear all; close all;%% Chapter 1: Principal components analysis% One way to think about principle components analysis that it can reveal whether% it is reasonable to decrease the dimensionality of a data set.% Here is an example to illustrate this principle:% suppose we visit 7 popular North American cities and rate various aspects of % life there.names = strvcat('New York','San Francisco','Winnipeg','Miami','Los Angeles','Philadelphia','Bozeman')categories=strvcat('housing cost','cafe culture','recreational opportunities','# of clubs')ratings=[10.0000 2.1000 2.2000 10.0000;          9.0000 1.9000 3.20000 10.0000;          5.5000 2.000 4.0000 10.0000;          3.0000 1.8000 9.0000 4.0000;          7.0000 1.7000 2.0000 5.8000;          6.2000 2.2000 4.1000 5.4000;          2.2000 2.0000 9.8000 1.1000]      % each row of the "ratings" matrix will correspond to one city.  Each% column in the row represents how that category was rated. % The matrix is kind of cumbersome.  Each city has many dimensions% along which it can vary: housing cost, cafe culture, etc.  It would% be nice if we could describe a city more simply.% let's see what this data looks like% This plot will show the means and standard deviations% for each of the categories (or dimensions) averaged% across cities.figuresubplot(1,2,1)boxplot(ratings,0,'+',0);set(gca,'Yticklabel',categories);% The boxplot doesn't tell us much about how the dimensions% might be related to each other.  Let's look at some of those:subplot(3,3,3)plot(ratings(:,1),ratings(:,4),'*');xlabel('Housing cost');ylabel('# of clubs');axis([0 11 0 11]);hold on;subplot(3,3,6)plot(ratings(:,1),ratings(:,3),'*');xlabel('Housing cost');ylabel('Rec. opportun');axis([0 11 0 11]);hold on;subplot(3,3,9)plot(ratings(:,1),ratings(:,2),'*');xlabel('cafe culture');ylabel('# of clubs');axis([0 11 0 11]);hold on;% These two plots indicate that some of the ratings might be related% to each other.  That is, having good recreational opportunities means% that housing costs tend to be low.  Having lots of nightclubs% means that housing costs tend to be high. % From this plot, it looks as if we could might be able to tell something% about one variable just be having information about another variable.%% ***** Looking at a pair of variables% to find out just how much we would know, we perform a% principle components analysis.  This analysis allows us to% ask whether 2 (or some greater number) of our dimensions might% be similar enough so that we could represent them as only one dimension.% In this example, we will find out whether our two categories,% "# of clubs" and "housing cost" can, in fact, be represented% as one axis% let's start off just looking at two of the dimensions at a% time. mini_ratings = [ratings(:,1) ratings(:,4)];% we saw above that housing cost and # of clubs were correlated.  The% function princomp will tell us how correlated they are, and will define% axes for a new space where the data will be uncorrelated.  To learn more% about principal components analysis, check out the excellent tutorial on% the subject.[pcs,newdata,weights,tsq]=pca(mini_ratings); % the first output, "pcs" tells us the principle components. % These are the axes for the new space and will reveal how % correlated your dimensions are with% one another.  If they are highly correlated, they might be% redundant and you  might be able to represent the data with% a smaller number of dimensions.% Let's look at the first principal componentpcs(:,1)% Note that these two values are very similar to one another % This indicates that the two values are very correlated.  That% they have the same sign indicates that this is a positive correlation.% The first principal component also defines the angle of greatest % variance of the data.  This is easier to see when plotted. %first, we'll re-plot the data from beforefigureplot(ratings(:,1),ratings(:,4),'*');xlabel('Housing cost');ylabel('# of clubs');axis([0 11 0 11]);hold on;% now, we'll draw on the first principle component% note that the "princomp" function subtracts off the mean% values of the data so we have to add them back on. means=[mean(ratings(:,1)) mean(ratings(:,4))];e1=pcs(:,1);new_point = [weights(1)*e1(1) + means(1)  weights(1)*e1(2) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'c');hold on;% Indeed, the cyan trace lies along the line of greatest variance % of the data.  Notice that it looks like the least-squares regression% line.  It actually is the least-squares regression line if we're% minimizing the sum of the squared distances *orthogonal* to the line.% Normally in least-squares regression we minimize the *vertical* sum of% squares.  We'll go into this more in Chapter 3: Total least squares.% We also have a second principle component, in fact, we will always% have as many principle components as we have dimension. The second% principle component tells you about the next largest axis of% variance of your data. means=[mean(ratings(:,1)) mean(ratings(:,4))];e2=pcs(:,2);new_point = [weights(2)*e2(1) + means(1)  weights(2)*e2(2) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'r')% In this dataset, only one axis of variance is readily apparent% We will see a later example where the 2nd principal component% makes more sense.  Notice that in both cases the vector was% scaled by its corresponding value in "weights".  These values, which% are the 3rd outputs of the princomp function correspond to the% spread of the data in that dimension. % So the "pcs" have told us the axes of the new space and the% weights have scaled then. Next, % "newdata" puts the data in this new coordinate system defined % by the principal componentsclfplot(newdata(:,1),newdata(:,2),'+')xlabel('1st Principal Component');ylabel('2nd Principal Component');% this next line will let you click on points to see% what they represent.  Hit return when you have had% enough. title('Hit <Return> to exit'); gname(names);% As you can see, we are plotting the same cities as before. The% difference is that now, in the new space, the data are not % correlated like they were before%% **** Looking at all four variables% Now let's do the same thing again, but this time with all 4% of our dimensions: housing cost, cafe culture, recreational% opportunities and # of clubs. [pcs,newdata,weights,tsq]=pca(ratings); % again, let's look at the first principal component:pcs(:,1)% Now the 1st principal component has 4 values since% the new space for the data will have 4 dimensions.% The first value corresponds to the first dimension% etc. % Note that the 1st and 4th values are similar.% The 1st and 4th values correspond to "housing cost" and % "# of clubs", dimensions we knew to be correlated.% Let's plot this principal component on the dataclose;% the only remaining figure should be the original one with% the box plot and the 3 scatter plots.subplot(3,3,3),hold on;means=[mean(ratings(:,1)) mean(ratings(:,4))];e1=pcs(:,1);new_point = [6*e1(1) + means(1)  6*e1(4) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'c');% The principal component runs along the axis of greatest% variance of the two dimensions.% let's look again:pcs(:,1)% Notice that the 1st and 3rd values of the principle component have a similar% magnitude but are opposite in sign.  Recall that we knew these% dimensions were inversely correlated.  Let's plot this principle% component on the data. subplot(3,3,6),hold on;means=[mean(ratings(:,1)) mean(ratings(:,3))];e1=pcs(:,1);new_point = [6*e1(1) + means(1)  6*e1(3) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'c');% lastly, notice that the second value of the principal component is% quite small.  Again, this makes sense given that we knew this% dimension wasn't really correlated with the others.  Apparently,% Cafe culture doesn't tell us much else about a city. subplot(3,3,9),hold on;means=[mean(ratings(:,1)) mean(ratings(:,2))];e1=pcs(:,1);new_point = [6*e1(1) + means(1)  6*e1(2) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'c');% and again, let's plot the data in the new space to make% sure it is decorrelatedfigureplot(newdata(:,1),newdata(:,2),'+')gname(names)%% **** Looking at the percent of variance that each PC explains% The goal of this endeavor was to see whether we could% reduce the number of dimensions of the data.  Each principal% component represents a new dimension.  Let's see if we can% get away with fewer than we had before. % we can tell just hown much of the variance is explained by % each principal component by looking at a pareto plot.% A pareto plot is a barplot (in the order of largest to smallest)% and a superimposed curve showing the cumulative counts.clfsubplot(2,1,1)percent_explained=100*weights/sum(weights);pareto(percent_explained);% The blue bars tell us the percent of variability explained% by each principle component.  Almost all of the variability% can be explained by the first principle component in this case.% This means that we can capture almost everything about the data % a small number of dimension. The blue lines tell us how much% of the cumulative variance has been explained. % Note that whenever you can explain all the variability without using % all the principal components, there will be fewer blue bars than % there are principal components. % When would we only need ONE dimension to describe all the data?simple = [10.0000   10.0100   10.0110   10.0120    9.0000    9.0100    9.0110    9.0120    5.5000    5.5100    5.5110    5.5120    3.0000    3.0100    3.0110    3.0120    7.0000    7.0100    7.0110    7.0120    6.2000    6.2100    6.2110    6.2120    2.2000    2.2100    2.2110    2.2120][new_pcs,newdata,weights,tsq]=pca(simple); subplot(2,1,2)percent_explained=100*weights/sum(weights);pareto(percent_explained);% here, all the dimensions are correlated so we can represent% the entire matrix using only one dimension.  All the variability% is explained by only one principle component.% You can change around the ratings matrix to see how the% principal components and the pareto plot change as % correlations change.  What will the pareto plot look% like when the dimensions are totally uncorrelated?% Plotted in the new space, the correlations bewteen% dimensions are reduced.  This could be useful in data% analysis:  for instance, if many properties are being% recorded from a neuron (eg orientation preference,% contrast sensitivity, rf size), a principal components% analysis might reveal that certain properties tend to% go together. There is another, rather different use for% this approach: we might be interested not just in the fact % that the data can be decorrelated, but what exactly needs% to be done to the data in order to decorrelate it. In other% words, we might want to ask what kind of filter we would need% to pass the data through in order to decorrelate it. % The SVD function is another matlab function which accomplishes% pretty much the same goal.  However, the outputs are in a different% form and can allow us to see the lienar algebra that makes all% this possible.   This second part will explain some of the linear% algebra in detail and then will return to principle component analysis% at the end. %% Chapter 2: SVD (Singular Value Decomposition):% The SVD decomposes a matrix into the product of the three% components:%     A = U S V^t% where ^t means transpose.  A is the original NxM matrix, U is% an NxN orthonormal matrix, V is an MxM orthonormal matrix,% and S is an NxM matrix with non-zero elements only along% the diagonal.%% This is closely related to the Principal Components Analysis (PCA)% in PCA we decompose a covariance matrix into the product of three% matrices: A = V S V^t.  In this decomposition, V is a matrix of% eigenvectors and S is a diagonal matrix of eigenvalues.% Let's try it on a randomly filled 10x5 matrix:A = rand([10,5]);[U,S,V] = svd(A);%The principlal components that we discussed above ( we called them pcs)% are in the U matrix: e1=U(:,1)e2=U(:,2)% And once again,  these vectors will define a new coordinate system% for our (hopefully now uncorrelated) data.% Look at S:S% Check that the decomposition worked:newA = U * S * V';mean((newA - A).^2,2)% Check that U and V are orthonormal matrices.  All of these% should be identity matrices:id=U'*U;mean((id-eye(size(id))).^2,2)id=U*U';mean((id-eye(size(id))).^2,2)id=V'*V;mean((id-eye(size(id))).^2,2)id=V*V';mean((id-eye(size(id))).^2,2)%% **** Four fundamental subspaces% Consider a matrix A as a linear transform, y=Ax, that% transforms N-dimension vectors, x, into M-dimensional vectors,% y.% If the matrix A is singular then there is a subspace of N-space% that is mapped to zero by A, (i.e., a set of vectors x such% that Ax=0).  This is called the "row-nullspace" of A, since% vectors in this space are "nulled" by the row-vectors of A.% There is a subspace of M-space that can be REACHED by A (i.e.,% for y in this space, there exists a v in N-space such that% Av=y).  This is called the "column-space" of A.  The% dimensionality of the column-space is called the "rank" of A.% The SVD explicitly constructs orthonormal bases for the% row-nullspace and column-space of A.  The columns of U, whose% same-numbered elements in S are non-zero, are an orthonormal% set of basis vectors that span the column-space of A.  The% remaining colums of U span the row-nullspace of A^t (also% called the column-nullspace of A).% The columns of V (rows of V^t), whose same-numbered elements in% S are zero, are an orthonormal set of vectors that span the% row-nullspace of A.  The remaining columns of V span the% column-space of A^t (also called the row-space of A).% Matlab provides functions "orth" and "null" to A=[1 0 0;   0 1 0]% Column space:orth(A)% Row space:orth(A')% Row nullspace:null(A)% Column nullspace (for our matrix there is no column nullspace):null(A')% Here's another example in which one of the columns is a linear combination% of the others.A =[1 -1 0;     0 1 -1;     1 0 -1]% It is easy to see that A has rank 2 by noting that you can get% the third column of A by summing the first 2 columns and then% multiplying by -1.  In fact, the vector (1,1,-1) is in the% column nullspace of A.  So is any scalar multiple of (1,1,-1).% It is called the column nullspace because it takes the columns% to zero: (1,1,-1) A = 0.colnull=null(A')colnull' * A% The vector (1,1,1) is in the row nullspace of A.  So is any% scalar multiple of (1,1,1).  It's called the row nullspace% because it takes the rows to zero: A (1,1,1)^t = 0.rownull=null(A)A * rownull% And the other two spaces:orth(A)orth(A')% Row space and row nullspace are orthogonal:orth(A')' * null(A)% Col space and col nullspace are orthogonal:orth(A)' * null(A')%% **** Solving a system of linear equationsA =[1 -1 0;     0 1 -1;     1 0 -1]% The product Ax is always a combination of the columns of A:%         (1 -1  0) (x0)      (1)      (-1)      ( 0)%    Ax = (0  1 -1) (x1) = x0 (0) + x1 ( 1) + x2 (-1)%         (1  0 -1) (x2)      (1)      ( 0)      (-1)% To solve Ax=b is to find a combination of the columns that% gives b.  We consider all possible combinations Ax, coming from% all choices of x.  Those products form the column space of A.% In the example, the columns lie in 3-dimensional space, but% their combinations fill out a plane (the matrix has rank 2).% The plane goes through the origin since one of the combinations% has weights x0=x1=x2=0.  Some vectors b do not lie on the% plane, so for them Ax=b can not be solved exactly.  The system% Ax=b has an exact solution only when the right side b is in the% column space of A.% The vector b=(2,3,4) is not in the column space of A so Ax=b% has no solution.  The vector b=(2,3,5) does lie in the plane% (spanned by the columns of A) so there is a solution,% x=(5,3,0).A * [5 3 0]'% However, there are other solutions as well.  x=(6,4,1) will work:A * [6 4 1]'% In fact, we can take x=(5,3,0) and add any scalar multiple of% y=(1,1,1) since (1,1,1) is in the null space.  We can write it% this way:%    A (x + c y) = Ax + A (cy) = Ax + c Ay = Ax + c 0 = Ax% If an NxN matrix A has linearly independent columns then%  (1) the row nullspace contains only the point 0%  (2) the solution to Ax=b (if there is one) is unique%  (3) the rank of A is N% In general any two solutions to Ax=b differ by a vector in the% row nullspace.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% **** Least squares regression:% Regression: When b is not in the column space of A, we can% still find a vector x that comes the closest to solving% Ax=b.% We motivate this by thinking about fitting empirical data.  We% change notation a bit (hopefully without too much confusion)% and try to solve Ap=y  instead of Ax=b.% Let's simulate collecting some data in a certain experiment.% For each of a number of test conditions, x, we measure an% outcome, y.  And let's assume that the relationship between y's% and x's is given by a polynomial:%       y = 3 x^2 + x - 4.% It should be obvious that this was an arbitrary choice.  Let's% use this formula to simulate some data.  First, we make a% vector of a bunch of x  values (evenly spaced between 0 and 1).% Then we compute y for each of those x values:x=[0:0.1:1]';y = 3*x.^2 + x -4;% Look at the data:plot(x,y,'o')% Now let's pretend we don't know the exact relationship between% y and x.  In particular, we have the data (xvals and yvals) and% we know that y is a second-order polynomial of x:%    p0 + p1 x + p2 X^2% but we don't know the parameter values p=(p0,p1,p2)% How might we solve for those parameter values?  We build a% matrix A whose columns depend on the x values.  In particular,% the first column of A has a bunch of x^2 values, the second% column of A has a bunch of x values and the third column has a% bunch of 1's.N=length(x);A=zeros(N,3);for index=1:N  xi=x(index);  A(index,1)=xi^2;  A(index,2)=xi;  A(index,3)=1;end% Look at the A matrix we've just constructed:A% Then we solve: y = A p, where we know y and we just constructed% A.  If A were a square (full rank) matrix, this would be easy;% we'd just invert A to get: p = inv(A) y.  In practice, it is% seldom necessary to form the explicit inverse of a matrix.% Matlab provides the matrix division operator, p = A\b.  This% produces the solution using Gaussian elimination, without% forming the inverse.p=A\y% This worked perfectly because there was no noise in the data.% Let's do it again with (simulated) noisy data.noisyY = y + 0.1*randn(size(y));plot(x,noisyY,'o')p=A\noisyY% With more data points it will work better:x=[0:5e-3:1]';y = 3*x.^2 + x -4;noisyY = y + 0.1*randn(size(y));N=length(x);A=zeros(N,3);for index=1:N  xi=x(index);  A(index,1)=xi^2;  A(index,2)=xi;  A(index,3)=1;endp=A\noisyY% The x and y values are vectors; according to our (second-order% polynomial) model, each element of y is a quadratic function of% the corresponding element of xvals.  A is an Nx3 matrix, and% the col-space of A is a 3-dimensional subspace of N space.  In% particular, the columns of A are a basis for all possible% second-order polynomials.  The product, Ap, is a particular% linear combination of those basis vectors, hence, it is a% particular second order polynomial.  If y=Ap (for some/any p),% then we say that y is in the column space of A.  That's what we% had in the noiseless case.  The y vector was exactly equal to% Ap for the right choice of p.  After adding noise, however, y% was no longer in the column space of A.  There was no choice of% p such that y=Ap exactly.  The regression solution found a p% such that Ap came as close as possible to y.  In other words,% it found y-est = A p-est, where the distance between noisyY and% y-est was minimized given that y-est was forced to be in the% column space of A.% All of this generalizes to many other situations, e.g., to% models other than second-order polynials).  In particular,% standard linear regression is a special case in which A only% has 2 columns (x's and 1's).  We can do it for other hairier% (non-polynomial) functions too.  Try simulating noisy data for:% y = 10 log(x) + 20.  Plot the data.  Then estimate the% parameters (10,20) from the noisy data.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 3: Total least squares% The regression operation we've been discussing produces a% particular kind of "best" solution.  Let's reconsider the% simple case of fitting a line through the origin to some (x,y)% data points.  The regression solution finds a vector s that% minimizes the function:%%      E(s) = | xs - y |^2,%% where s is the slope of the fitted line.  This error function% penalizes VERTICAL displacements of the data points from the% line.  But there are other ways to specify error functions.% What if we believe that there is noise present in BOTH the x% and y measurements?  We might then desire a penalty for the% perpendicular distance of the data points from the line.  Such% a function can easily be written as:%%      E'(u) = | M . u |^2 / | u |^2,%% where u is a 2-vector, and M is a 2-column matrix with columns% formed from the data vectors x and y.  Since we are dividing by% the squared norm of u, scaling u does not change the value of% the function.  The vector that minimizes this function is% typically scaled to be a unit vector.  It points in a direction% perpendicular to the best-fitting line.% This solution may be found by computing the SVD of M, and% choosing the column of V (a 2-vector) corresponding to the% largest singular value.  Let's try this on a data set with noise% added to both x and y:x=[-1:0.05:1]';y=2*x;noisyX = x + 0.2*randn(size(x));noisyY = y + 0.2*randn(size(y));plot(noisyX,noisyY,'o')hold on;% Standard (y-distance) regression:standard_slope = noisyX\noisyYplot(noisyX,noisyX*standard_slope,'g')% TLS regression: (most of the time, but not always, this gives a% better estimate)M=[noisyX';noisyY']';[U,S,V]=svd(M);svd_slope = V(2,1)/V(1,1)plot(noisyX,noisyX*svd_slope,'r')legend({'data','standard slope','svd slope'});%% Chapter 4: Covariance % This section of the tutorial deals with multi-dimensional data% sets.  Each data point represents one test condition, e.g., a% data point might be the height and weight of a person% represented as a vector (height,weight).  Or each data point% might be the intensity values of an image represented as a very% long vector (p1,p2,...,pN), where p_i is the intensity at the% i_th pixel.% The function multiRandn generates multi-dimensional random% vector, given a particular mean and covariance matrix.  The% following code uses multiRandn to generate a simulated data% set, with M data points.M=100;ActualMean=[10 2]';ActualCov=[1 0.8           0.8 1];	data=zeros(length(ActualMean),M); for index=1:M  data(:,index)=mvnrnd(ActualMean,ActualCov);endclf;plot(data(1,:),data(2,:),'o')% Data is an 2xM array.  The cols of data are the data points% that we plotted in the scatter plot.  The xvalues are in the% first row of data, and the yvalues are in the second row.  The% correlation coefficient in this example is 0.8, so the scatter% plot slopes up and to the right at a 45 degree angle.% Next let's compute the mean of the data set.  It will be close% (but not exactly equal) to the mean that we specified at the% outset.EstMean=mean(data,2)% We compute the covariance of the data in two steps.  First we% subtract EstMean from each of the columns to give us a new% matrix D.  Then we compute D D^t, and divided by the number of% data points, M, minus 1.D=data-EstMean*ones(1,M);EstCov=D*D'/(M-1)% The built-in matlab function "cov" does the same thing (but you% have to have the data in rows instead of columns):cov(data')% Obviously, the empirical covariance would be closer if we used% a bigger data set (larger M).  Try it.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 5: Principal components analysis (reprise)% Compute the svd of D.  The cols of U span the col-space of D.% These are also the eigenvectors of the covariance, D D^t.[U,S,V]=svd(D);e1=U(:,1)e2=U(:,2)%%%%%%the first output of princomp is equivalent to U (although you have to take princomp of D')% Like before, we can plot these new axes in the old space and they% will lie along the axes of greatest variability. % Once again, we will scale them, this time by the eigenvalues in S. means=[mean(data(1,:)) mean(data(2,:))];hold on;axis([6 14 -2 6])new_point = [S(1,1)*e1(1) + means(1)  S(1,1)*e1(2) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'c');hold on;% This time, the data clearly varies in a second dimension,% one that is perpendicular to the first.  The second principle% component goes along this axis.means=[mean(data(1,:)) mean(data(2,:))];hold on;axis([6 14 -2 6])new_point = [S(2,2)*e2(1) + means(1)  S(2,2)*e2(2) + means(2)];axis_of_variance = [means ;new_point];plot(axis_of_variance(:,1),axis_of_variance(:,2),'r');hold on;% Show that these are eigenvectors of D D^t.  For a vector v to% be an eigenvector of a matrix A means that: Av=lv for some% scalar l.  In other words, passing v through the matrix only% affects its length, not its direction.DDt = D*D';DDt_e1 = (DDt * e1) ./ e1DDt_e2 = (DDt * e2) ./ e2%%%%%in princomp, the eigenvalue are the third output. % The eigenvector corresponding to the largest eigenvalue is% called the first principle component.  The svd returns the% eigenvectors in order, so e1 is the eigenvector with the% largest eigenvalue.  The first principle component is the unit% vector with the largest projection onto the data set.  The e1% that you computed should be very close to (.707,.707): Note% that you might also have gotten (-.707,-.707).  That's just as% good.  With either sign, it points in the elongated direction% of the scatter plot, hence it has the largest projection onto% the data set.  The other eigenvector, e2, points in the% perpendicular direction.  The e2 that you computed should be% very close to (-.707,.707) or (.707,-.707):% Again, the first principle component, e1, is the unit vector% with the largest projection onto the data set.  The projection% of e1 onto the data is:norm(e1'*D)% It's helpful to draw the shape of the matrices on a piece of% paper: D is a (2xM) matrix, e1 is (2x1) column vector, so e1'*D% is a (1xM) row vector .  Each element of this row vector is the% length of the projection of a single data point onto the unit% vector e1.  The vector-length of this row vector gives the sum% of the projection lengths.% The projection onto the other eigenvector is smaller:norm(e2'*D)% The point is that you know a lot about a data point by knowing% only the projection of that data point onto e1.  Let's compute% the vector-distance between a data point and its projection% onto e1:datapoint = D(:,1)distance = norm(datapoint - e1*(e1'*datapoint))% This distance is pretty small because you know a lot just by% knowing the projection onto e1.  And now, the average of the% vector-distances (for all the data points):perps=D-(e1*(e1'*D));mean(sqrt(sum(perps.^2)))% Compare this number to what you would get by using some other% arbitrary unit vector.  Evaluate the following a bunch of% times.  It picks a random unit vector, prints that vector, then% projects the data set onto that vector.  The numbers you get% will always be bigger than or equal to what you got using e1.% When the random unit vector is close to e1, you get a smaller% number.  When it is far from e1, you get a larger number.foo=rand(2,1);e=foo/norm(foo)perps=D-(e*(e'*D));mean(sqrt(sum(perps.^2)))% Try this a bunch of times to see what you get for a bunch of% different random unit vectors.  Each data point is a vector of% 2 numbers.  If you had to summarize each data point with one% number, what number would you choose?% Now do it all again for a higher dimensional data set.  Using% this covariance matrix, what do you expect the principle% components to be?M=100;ActualMean=[0 0 0]';ActualCov=[4 0 0           0 2 0           0 0 1];% Generate the data:data=zeros(length(ActualMean),M); for index=1:M  data(:,index)=mvnrnd(ActualMean,ActualCov);end% Estimate mean and covariance:EstMean=mean(data,2)EstCov=cov(data')% Compute the principle components.D=data-EstMean*ones(1,M);[U,S,V]=svd(D);e1=U(:,1)e2=U(:,2)e3=U(:,3)